{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-source Model Enablement\n",
    "\n",
    "To implement the usage of LiteLLM and enable AutoGen to use open-sourced models while considering the context provided, we will address the points raised and create a refactoring plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import autogen\n",
    "import litellm\n",
    "from litellm import completion\n",
    "print(autogen.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='.env',\n",
    "    model_api_key_map={\n",
    "        # \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        \"mistralai/Mistral-7B-v0.1\": \"HUGGINGFACE_HUB\",\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            # \"gpt-4\",\n",
    "            \"mistralai/Mistral-7B-v0.1\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'api_key': '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',\n",
       "  'model': 'gpt-4'},\n",
       " {'api_key': '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',\n",
       "  'model': 'mistralai/Mistral-7B-v0.1'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Names vs. Config Names Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "{'gpt-4-32k-0613', 'gpt-3.5-turbo-0613', 'gpt-35-turbo', 'gpt-4-0314', 'gpt-3.5-turbo-16k', 'gpt-4-0613', 'gpt-4-32k-0314', 'gpt-3.5-turbo-0301', 'gpt-4-32k', 'gpt-3.5-turbo-16k-0613', 'gpt-3.5-turbo', 'gpt-4'}\n"
     ]
    }
   ],
   "source": [
    "print(type(autogen.Completion.chat_models))\n",
    "print(autogen.Completion.chat_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'text-ada-001': 0.0004, 'text-babbage-001': 0.0005, 'text-curie-001': 0.002, 'code-cushman-001': 0.024, 'code-davinci-002': 0.1, 'text-davinci-002': 0.02, 'text-davinci-003': 0.02, 'gpt-3.5-turbo': (0.0015, 0.002), 'gpt-3.5-turbo-0301': (0.0015, 0.002), 'gpt-3.5-turbo-0613': (0.0015, 0.002), 'gpt-3.5-turbo-16k': (0.003, 0.004), 'gpt-3.5-turbo-16k-0613': (0.003, 0.004), 'gpt-35-turbo': 0.002, 'gpt-4': (0.03, 0.06), 'gpt-4-32k': (0.06, 0.12), 'gpt-4-0314': (0.03, 0.06), 'gpt-4-32k-0314': (0.06, 0.12), 'gpt-4-0613': (0.03, 0.06), 'gpt-4-32k-0613': (0.06, 0.12)}\n"
     ]
    }
   ],
   "source": [
    "print(type(autogen.Completion.price1K))\n",
    "print(autogen.Completion.price1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'gpt-4': {'max_tokens': 8192, 'input_cost_per_token': 3e-05, 'output_cost_per_token': 6e-05, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-4-0314': {'max_tokens': 8192, 'input_cost_per_token': 3e-05, 'output_cost_per_token': 6e-05, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-4-0613': {'max_tokens': 8192, 'input_cost_per_token': 3e-05, 'output_cost_per_token': 6e-05, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-4-32k': {'max_tokens': 32768, 'input_cost_per_token': 6e-05, 'output_cost_per_token': 0.00012, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-4-32k-0314': {'max_tokens': 32768, 'input_cost_per_token': 6e-05, 'output_cost_per_token': 0.00012, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-4-32k-0613': {'max_tokens': 32768, 'input_cost_per_token': 6e-05, 'output_cost_per_token': 0.00012, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-3.5-turbo': {'max_tokens': 4097, 'input_cost_per_token': 1.5e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-3.5-turbo-0301': {'max_tokens': 4097, 'input_cost_per_token': 1.5e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-3.5-turbo-0613': {'max_tokens': 4097, 'input_cost_per_token': 1.5e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-3.5-turbo-16k': {'max_tokens': 16385, 'input_cost_per_token': 3e-06, 'output_cost_per_token': 4e-06, 'litellm_provider': 'openai', 'mode': 'chat'}, 'gpt-3.5-turbo-16k-0613': {'max_tokens': 16385, 'input_cost_per_token': 3e-06, 'output_cost_per_token': 4e-06, 'litellm_provider': 'openai', 'mode': 'chat'}, 'text-davinci-003': {'max_tokens': 4097, 'input_cost_per_token': 2e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'text-completion-openai', 'mode': 'completion'}, 'text-curie-001': {'max_tokens': 2049, 'input_cost_per_token': 2e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'text-completion-openai', 'mode': 'completion'}, 'text-babbage-001': {'max_tokens': 2049, 'input_cost_per_token': 4e-07, 'output_cost_per_token': 4e-07, 'litellm_provider': 'text-completion-openai', 'mode': 'completion'}, 'text-ada-001': {'max_tokens': 2049, 'input_cost_per_token': 4e-07, 'output_cost_per_token': 4e-07, 'litellm_provider': 'text-completion-openai', 'mode': 'completion'}, 'babbage-002': {'max_tokens': 16384, 'input_cost_per_token': 4e-07, 'output_cost_per_token': 4e-07, 'litellm_provider': 'text-completion-openai', 'mode': 'completion'}, 'davinci-002': {'max_tokens': 16384, 'input_cost_per_token': 2e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'text-completion-openai', 'mode': 'completion'}, 'gpt-3.5-turbo-instruct': {'max_tokens': 8192, 'input_cost_per_token': 1.5e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'text-completion-openai', 'mode': 'completion'}, 'claude-instant-1': {'max_tokens': 100000, 'input_cost_per_token': 1.63e-06, 'output_cost_per_token': 5.51e-06, 'litellm_provider': 'anthropic', 'mode': 'chat'}, 'claude-instant-1.2': {'max_tokens': 100000, 'input_cost_per_token': 1.63e-06, 'output_cost_per_token': 5.51e-06, 'litellm_provider': 'anthropic', 'mode': 'chat'}, 'claude-2': {'max_tokens': 100000, 'input_cost_per_token': 1.102e-05, 'output_cost_per_token': 3.268e-05, 'litellm_provider': 'anthropic', 'mode': 'chat'}, 'text-bison': {'max_tokens': 8192, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-text-models', 'mode': 'completion'}, 'text-bison@001': {'max_tokens': 8192, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-text-models', 'mode': 'completion'}, 'chat-bison': {'max_tokens': 4096, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-chat-models', 'mode': 'chat'}, 'chat-bison@001': {'max_tokens': 4096, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-chat-models', 'mode': 'chat'}, 'chat-bison-32k': {'max_tokens': 32000, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-chat-models', 'mode': 'chat'}, 'code-bison': {'max_tokens': 6144, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-code-text-models', 'mode': 'chat'}, 'code-bison@001': {'max_tokens': 6144, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-code-text-models', 'mode': 'completion'}, 'code-gecko@001': {'max_tokens': 2048, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-chat-models', 'mode': 'completion'}, 'code-gecko@latest': {'max_tokens': 2048, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-chat-models', 'mode': 'completion'}, 'codechat-bison': {'max_tokens': 6144, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-code-chat-models', 'mode': 'chat'}, 'codechat-bison@001': {'max_tokens': 6144, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-code-chat-models', 'mode': 'chat'}, 'codechat-bison-32k': {'max_tokens': 32000, 'input_cost_per_token': 1.25e-07, 'output_cost_per_token': 1.25e-07, 'litellm_provider': 'vertex_ai-chat-models', 'mode': 'chat'}, 'command-nightly': {'max_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'output_cost_per_token': 1.5e-05, 'litellm_provider': 'cohere', 'mode': 'completion'}, 'command': {'max_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'output_cost_per_token': 1.5e-05, 'litellm_provider': 'cohere', 'mode': 'completion'}, 'command-light': {'max_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'output_cost_per_token': 1.5e-05, 'litellm_provider': 'cohere', 'mode': 'completion'}, 'command-medium-beta': {'max_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'output_cost_per_token': 1.5e-05, 'litellm_provider': 'cohere', 'mode': 'completion'}, 'command-xlarge-beta': {'max_tokens': 4096, 'input_cost_per_token': 1.5e-05, 'output_cost_per_token': 1.5e-05, 'litellm_provider': 'cohere', 'mode': 'completion'}, 'replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1': {'max_tokens': 4096, 'litellm_provider': 'replicate', 'mode': 'chat'}, 'openrouter/openai/gpt-3.5-turbo': {'max_tokens': 4095, 'input_cost_per_token': 1.5e-06, 'output_cost_per_token': 2e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/openai/gpt-3.5-turbo-16k': {'max_tokens': 16383, 'input_cost_per_token': 3e-06, 'output_cost_per_token': 4e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/openai/gpt-4': {'max_tokens': 8192, 'input_cost_per_token': 3e-05, 'output_cost_per_token': 6e-05, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/anthropic/claude-instant-v1': {'max_tokens': 100000, 'input_cost_per_token': 1.63e-06, 'output_cost_per_token': 5.51e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/anthropic/claude-2': {'max_tokens': 100000, 'input_cost_per_token': 1.102e-05, 'output_cost_per_token': 3.268e-05, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/google/palm-2-chat-bison': {'max_tokens': 8000, 'input_cost_per_token': 5e-07, 'output_cost_per_token': 5e-07, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/google/palm-2-codechat-bison': {'max_tokens': 8000, 'input_cost_per_token': 5e-07, 'output_cost_per_token': 5e-07, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/meta-llama/llama-2-13b-chat': {'max_tokens': 4096, 'input_cost_per_token': 2e-07, 'output_cost_per_token': 2e-07, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/meta-llama/llama-2-70b-chat': {'max_tokens': 4096, 'input_cost_per_token': 1.5e-06, 'output_cost_per_token': 1.5e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/meta-llama/codellama-34b-instruct': {'max_tokens': 8096, 'input_cost_per_token': 5e-07, 'output_cost_per_token': 5e-07, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/nousresearch/nous-hermes-llama2-13b': {'max_tokens': 4096, 'input_cost_per_token': 2e-07, 'output_cost_per_token': 2e-07, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/mancer/weaver': {'max_tokens': 8000, 'input_cost_per_token': 5.625e-06, 'output_cost_per_token': 5.625e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/gryphe/mythomax-l2-13b': {'max_tokens': 8192, 'input_cost_per_token': 1.875e-06, 'output_cost_per_token': 1.875e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/jondurbin/airoboros-l2-70b-2.1': {'max_tokens': 4096, 'input_cost_per_token': 1.3875e-05, 'output_cost_per_token': 1.3875e-05, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/undi95/remm-slerp-l2-13b': {'max_tokens': 6144, 'input_cost_per_token': 1.875e-06, 'output_cost_per_token': 1.875e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/pygmalionai/mythalion-13b': {'max_tokens': 4096, 'input_cost_per_token': 1.875e-06, 'output_cost_per_token': 1.875e-06, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'openrouter/mistralai/mistral-7b-instruct': {'max_tokens': 4096, 'input_cost_per_token': 0.0, 'output_cost_per_token': 0.0, 'litellm_provider': 'openrouter', 'mode': 'chat'}, 'j2-ultra': {'max_tokens': 8192, 'input_cost_per_token': 1.5e-05, 'output_cost_per_token': 1.5e-05, 'litellm_provider': 'ai21', 'mode': 'completion'}, 'j2-mid': {'max_tokens': 8192, 'input_cost_per_token': 1e-05, 'output_cost_per_token': 1e-05, 'litellm_provider': 'ai21', 'mode': 'completion'}, 'j2-light': {'max_tokens': 8192, 'input_cost_per_token': 3e-06, 'output_cost_per_token': 3e-06, 'litellm_provider': 'ai21', 'mode': 'completion'}, 'dolphin': {'max_tokens': 4096, 'input_cost_per_token': 2e-05, 'output_cost_per_token': 2e-05, 'litellm_provider': 'nlp_cloud', 'mode': 'completion'}, 'chatdolphin': {'max_tokens': 4096, 'input_cost_per_token': 2e-05, 'output_cost_per_token': 2e-05, 'litellm_provider': 'nlp_cloud', 'mode': 'chat'}, 'luminous-base': {'max_tokens': 2048, 'input_cost_per_token': 3e-05, 'output_cost_per_token': 3.3e-05, 'litellm_provider': 'aleph_alpha', 'mode': 'completion'}, 'luminous-base-control': {'max_tokens': 2048, 'input_cost_per_token': 3.75e-05, 'output_cost_per_token': 4.125e-05, 'litellm_provider': 'aleph_alpha', 'mode': 'chat'}, 'luminous-extended': {'max_tokens': 2048, 'input_cost_per_token': 4.5e-05, 'output_cost_per_token': 4.95e-05, 'litellm_provider': 'aleph_alpha', 'mode': 'completion'}, 'luminous-extended-control': {'max_tokens': 2048, 'input_cost_per_token': 5.625e-05, 'output_cost_per_token': 6.1875e-05, 'litellm_provider': 'aleph_alpha', 'mode': 'chat'}, 'luminous-supreme': {'max_tokens': 2048, 'input_cost_per_token': 0.000175, 'output_cost_per_token': 0.0001925, 'litellm_provider': 'aleph_alpha', 'mode': 'completion'}, 'luminous-supreme-control': {'max_tokens': 2048, 'input_cost_per_token': 0.00021875, 'output_cost_per_token': 0.000240625, 'litellm_provider': 'aleph_alpha', 'mode': 'chat'}, 'together-ai-up-to-3b': {'input_cost_per_token': 1e-07, 'output_cost_per_token': 1e-07}, 'together-ai-3.1b-7b': {'input_cost_per_token': 2e-07, 'output_cost_per_token': 2e-07}, 'together-ai-7.1b-20b': {'max_tokens': 1000, 'input_cost_per_token': 4e-07, 'output_cost_per_token': 4e-07}, 'together-ai-20.1b-40b': {'input_cost_per_token': 1e-06, 'output_cost_per_token': 1e-06}, 'together-ai-40.1b-70b': {'input_cost_per_token': 3e-06, 'output_cost_per_token': 3e-06}}\n"
     ]
    }
   ],
   "source": [
    "print(type(litellm.get_model_cost_map()))\n",
    "print(litellm.get_model_cost_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "from autogen.oai import Completion\n",
    "\n",
    "def map_oai_litellm_models():\n",
    "    \"\"\"\n",
    "    The purpose of this function is to find the corresponding\n",
    "    mappings of oai models as their names are different in litellm,\n",
    "    which we need to enable building agents with open sources models.\n",
    "\n",
    "    # Mapping\n",
    "    # For incommon models, a direct mapping can be created as they are named the same in both systems.\n",
    "    # For unique models, manual mapping might be needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetching model data\n",
    "    litellm_models = set(litellm.get_model_cost_map().keys())  # Assuming it's a dict\n",
    "    autogen_models = set(Completion.price1K)  # Assuming it's an iterable of model names\n",
    "    \n",
    "    # Identifying common and unique models\n",
    "    common_models = autogen_models.intersection(litellm_models)\n",
    "    unique_autogen_models = autogen_models.difference(litellm_models)\n",
    "    unique_litellm_models = litellm_models.difference(autogen_models)\n",
    "    \n",
    "    # Mapping for common models\n",
    "    common_mapping = {model: model for model in common_models}\n",
    "    \n",
    "    # Mapping for unique models\n",
    "    unique_mapping = {}\n",
    "    for auto_model in unique_autogen_models:\n",
    "        closest_match = difflib.get_close_matches(auto_model, unique_litellm_models, n=2, cutoff=0.1)\n",
    "        if closest_match:\n",
    "            unique_mapping[auto_model] = closest_match[0]\n",
    "        else:\n",
    "            # Handle unmatched models. Potentially log them for manual mapping\n",
    "            unique_mapping[auto_model] = None\n",
    "            print(f\"No match found for {auto_model}. Consider manually mapping this model.\")\n",
    "    \n",
    "    # Merge mappings\n",
    "    model_mapping = {**common_mapping, **unique_mapping}\n",
    "    \n",
    "    return model_mapping\n",
    "\n",
    "model_mapping = map_oai_litellm_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code-davinci-002': 'davinci-002',\n",
      " 'gpt-3.5-turbo': 'gpt-3.5-turbo',\n",
      " 'gpt-3.5-turbo-0301': 'gpt-3.5-turbo-0301',\n",
      " 'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613',\n",
      " 'gpt-3.5-turbo-16k': 'gpt-3.5-turbo-16k',\n",
      " 'gpt-3.5-turbo-16k-0613': 'gpt-3.5-turbo-16k-0613',\n",
      " 'gpt-35-turbo': 'gpt-3.5-turbo-instruct',\n",
      " 'gpt-4': 'gpt-4',\n",
      " 'gpt-4-0314': 'gpt-4-0314',\n",
      " 'gpt-4-0613': 'gpt-4-0613',\n",
      " 'gpt-4-32k': 'gpt-4-32k',\n",
      " 'gpt-4-32k-0314': 'gpt-4-32k-0314',\n",
      " 'gpt-4-32k-0613': 'gpt-4-32k-0613',\n",
      " 'text-ada-001': 'text-ada-001',\n",
      " 'text-babbage-001': 'text-babbage-001',\n",
      " 'text-curie-001': 'text-curie-001',\n",
      " 'text-davinci-002': 'davinci-002',\n",
      " 'text-davinci-003': 'text-davinci-003'}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "model_mapping.pop('code-cushman-001', None)\n",
    "pp.pprint(model_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.model_alias_map = {\n",
    "    'code-davinci-002': 'davinci-002',\n",
    "    'gpt-3.5-turbo': 'gpt-3.5-turbo',\n",
    "    'gpt-3.5-turbo-0301': 'gpt-3.5-turbo-0301',\n",
    "    'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613',\n",
    "    'gpt-3.5-turbo-16k': 'gpt-3.5-turbo-16k',\n",
    "    'gpt-3.5-turbo-16k-0613': 'gpt-3.5-turbo-16k-0613',\n",
    "    'gpt-35-turbo': 'gpt-3.5-turbo-instruct',\n",
    "    'gpt-4': 'gpt-4',\n",
    "    'gpt-4-0314': 'gpt-4-0314',\n",
    "    'gpt-4-0613': 'gpt-4-0613',\n",
    "    'gpt-4-32k': 'gpt-4-32k',\n",
    "    'gpt-4-32k-0314': 'gpt-4-32k-0314',\n",
    "    'gpt-4-32k-0613': 'gpt-4-32k-0613',\n",
    "    'text-ada-001': 'text-ada-001',\n",
    "    'text-babbage-001': 'text-babbage-001',\n",
    "    'text-curie-001': 'text-curie-001',\n",
    "    'text-davinci-002': 'davinci-002',\n",
    "    'text-davinci-003': 'text-davinci-003'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitution with LiteLLM: \n",
    "\n",
    "Given that the direct substitution with `litellm.completion` might not be universally sufficient, there needs to be a mechanism that checks conditions under which to use LiteLLM and when to default to OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
