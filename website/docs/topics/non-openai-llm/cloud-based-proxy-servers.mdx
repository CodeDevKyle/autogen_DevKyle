# Cloud-based Proxy Servers
When cloud-based proxy servers provide an OpenAI-compatible API, using them in AutoGen
is straightforward. With [LLM Configuration](/docs/topics/llm_configuration) done in the same way
as when using OpenAI's models, the primary difference is typically the authentication which is
usually handled through an API key.

Examples of some cloud-based proxy server providers that have OpenAI-compatible API are provided
below.

## together.ai



````mdx-code-block
:::tip
Not all proxy servers support Function Calling with their OpenAI-compatible API. Check their
documentation.

See Non-OpenAI LLM examples and guides in the [notebooks section](/docs/notebooks) for examples of
proxy servers that support Function Calling.
:::
````

