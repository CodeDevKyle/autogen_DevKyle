# Cloud-based Proxy Servers
When cloud-based proxy servers provide an OpenAI-compatible API, using them in AutoGen
is straightforward. With [LLM Configuration](/docs/topics/llm_configuration) done in the same way
as when using OpenAI's models, the primary difference is typically the authentication which is
usually handled through an API key.

Examples of some cloud-based proxy server providers that have OpenAI-compatible API are provided
below.

## Together AI
![together.ai](images/togetherai250.png)

The first example for using together.ai is a group chat, based on [this notebook](https://github.com/microsoft/FLAML/blob/4ea686af5c3e8ff24d9076a7a626c8b28ab5b1d7/notebook/autogen_multiagent_roleplay_chat.ipynb).

Start by [installing AutoGen](/docs/installation/) and getting your [together.ai API key](https://api.together.xyz/settings/profile).

Put your togther.ai API key in an environment variable.

Linux / Mac OSX:

```python
$ export TOGETHER_API_KEY=YourTogetherAIKeyHere
```

Windows (command prompt):

```python
set TOGETHER_API_KEY=YourTogetherAIKeyHere
```

Create your LLM configuration

```python
import autogen
import os

llm_config={
    "config_list": [
        {
            # Available together.ai model strings:
            # https://docs.together.ai/docs/inference-models
            "model": "mistralai/Mistral-7B-Instruct-v0.1",
            "api_key": os.environ['TOGETHER_API_KEY'],
            "base_url": "https://api.together.xyz/v1"
        }
    ],
    "cache_seed": 42
}
```

## Construct Agents


## Start Chat

```python
user_proxy.initiate_chat(
    manager, message="Find a latest paper about gpt-4 on arxiv and find its potential applications in software."
)
# type exit to terminate the chat
```


````mdx-code-block
:::tip
Not all proxy servers support Function Calling with their OpenAI-compatible API. Check their
documentation.

See Non-OpenAI LLM examples and guides in the [notebooks section](/docs/notebooks) for examples of
proxy servers that support Function Calling.
:::
````

