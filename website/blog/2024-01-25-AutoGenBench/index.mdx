---
title: "AutoGenBench -- A Tool for Measuring and Evaluating AutoGen Agents"
authors:
  - afourney
  - qingyunwu
tags: [AutoGen]
---


![AutoGenBench](img/teaser.jpg)


## TLDR
Today we are releasing AutoGenBench – a tool for evaluating AutoGen agents and workflows on established LLM and agentic benchmarks.

AutoGenBench is a standalone command line tool, installable from PyPI, which handles downloading, configuring, running, and reporting supported benchmarks. AutoGenBench works best when run alongside Docker, since it uses Docker to isolate tests from one another.

* See the [AutoGenBench README](https://github.com/microsoft/autogen/blob/main/samples/tools/testbed/README.md) for information on installation and running benchmarks.
* See the [AutoGenBench CONTRIBUTING guide](https://github.com/microsoft/autogen/blob/main/samples/tools/testbed/CONTRIBUTING.md) for information on developing or contributing benchmark datasets.


### Quick Start
Get started quickly by running the following commands in a bash terminal.

*Note:* You may need to adjust the path to the `OAI_CONFIG_LIST`, as appropriate.
```sh
export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)
pip install autogenbench
autogenbench clone HumanEval
cd HumanEval
autogenbench run --subsample --repeat 3 0.1 Tasks/human_eval.json
autogenbench tabulate Results/human_eval
```

## Introduction
Measurement and evaluation are core components of every major AI or ML research project, and so the same is true for AutoGen. To this end, today we are releasing AutoGenBench, a standalone command line tool that we have been using to guide development of AutoGen. Conveniently, AutoGenBench handles: downloading, configuring, running, and reporting results of agents on various public benchmark datasets. In addition to reporting top-line numbers, each AutoGenBench run produces a comprehensive set of logs and telemetry that can be used for debugging, profiling, computing custom metrics, and as input to [AgentEval](https://microsoft.github.io/autogen/blog/2023/11/20/AgentEval). In the remainder of this blog post, we outline core design principles for AutoGenBench (key to understanding its operation); present a guide to installing and running AutoGenBench; outline a roadmap for evaluatio; and end with an open call to contributors.

## Design Principles
AutoGenBench is designed around three core design principles. Understanding these principle will help with the understanding of the tool, its operation and its output. These three principles are:
-  **Repetition:** LLMs are stochastic, and in many cases, so too is the code they write to solve problems. For example, a Python script might call an external search engine, and the results may vary run-to-run. This can lead to variance in agent performance. Repetition is key to measuring and understanding this variance. To this end, AutoGenBench is built from the ground up with an understanding that tasks may be run multiple times, and that variance is a metric we often want to measure.

 -  **Isolation:** Agents interact with the worlds in both subtle and overt ways. For example an agent may install a python library or write a file to disk. This can lead to order effects that can impact measurement. Consider, for example, comparing two agents on a common benchmark. One agent may appear more efficient than the other simply because it ran second, and benefitted from the hard work the first agent did in installing and debugging necessary Python libraries. To address this, AutoGenBench isolates each task in Docker container. This ensures that all runs start with the same initial conditions (in addition to just being a much safer way to run agent-produced code)

-  **Instrumentation:** While top-line metrics are great for comparing agents or models, we often want much more information about how the argents are performing, where they are getting stuck, and how they can be improved. We may also have new research questions in the future that require computing a different set of metrics to answer. To this end, AutoGenBench is designed to log everything, and to compute metrics from those logs. This ensures that one can always go back to the logs to answer questions about what happened, run profiling software, or feed the logs into tools like [AgentEval](https://microsoft.github.io/autogen/blog/2023/11/20/AgentEval).

## Installing and Running AutoGenBench
As noted above, isolation is a key design principle, and so AutoGenBench must be run in en environment where Docker is available (Desktop or Engine). **It will not run in GitHub codespaces**, unless you opt for native execution (with is strongly discouraged). To install Docker Desktop see [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/).
Once Docker is installed, AutoGenBench can then be installed as a standalone tool from PyPI. With `pip`, installation can be achieved as follows:

```sh
pip install autogenbench
```
After installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter described later.

If you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:

```sh
export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)
```
## A Typical Session
Once AutoGenBench and necessary keys are installed, a typical session will look as follows:

```
autogenbench clone HumanEval
cd HumanEval
cat README.txt
autogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonl
autogenbench tabulate results/r_human_eval_two_agents
```

Where:
- `autogenbench clone HumanEval` downloads and expands the HumanEval benchmark scenario.
- `cd HumanEval; cat README.txt` navigates to the benchmark directory, and prints the README (which you should always read!)
- autogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonl`
 runs a 10% subsample of the tasks defined in `Tasks/human_eval_two_agents.jsonl. Each task is run 3 times.
- `autogenbench tabulate results/human_eval_two_agents` tabulates the results of the run.

Each of these commands has extensive in-line help via:

- `autogenbench --help`
- `autogenbench clone --help`
- `autogenbench run --help`
- `autogenbench tabulate --help`

For more detailed instructions about these commands, and other details related to installation and execution, please see the [AutoGenBench README](https://github.com/microsoft/autogen/blob/main/samples/tools/testbed/README.md).

## Roadmap
While we are writing about AutoGenBench for the first time today, we note that it is very much an evolving project in its own right. Over the next few weeks and months we hope to:
- Onboard many additional benchmarks beyond those shipping today
- Greatly improve logging and telemetry
- Introduce new core metrics including total costs, task completion time, conversation turns, etc.
- Provide tighter integration with AgentEval and AutoGen Studio
For an up to date tracking of our work items on this project, please see [AutoGenBench Work Items]( https://github.com/microsoft/autogen/issues/973)

## Call for Participation
Finally, we want to end this blog post with an open call for contributions. New benchmarks are constantly being published. Everyone may have their own distinct set of metrics that they care most about optimizing. AutoGenBench, as a tool, is still nascent and has much opportunity for improvement. To this end, we welcome any and all contributions to this corner of the AutoGen project. If contributing is something that interests you, please see the [contributor’s guide](https://github.com/microsoft/autogen/blob/main/samples/tools/testbed/CONTRIBUTING.md).
