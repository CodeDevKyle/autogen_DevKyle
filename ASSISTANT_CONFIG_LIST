{   
    "gpt-4": {
        "base": "GPT",
        "type": ["chat", "instruct", "code"],
        "language": ["en", "cn", "de", "es", "fr", "jp", "kr"],
        "params": "online",
        "context": "8192",
        "release": "2023",
        "benchmark": {},
        "description": "A set of models that improve on GPT-3.5 and can understand as well as generate natural language or code."
    },
    "gpt-3.5-turbo": {
        "base": "GPT",
        "type": ["chat", "instruct", "code"],
        "language": ["en", "cn", "de", "es", "fr", "jp", "kr"],
        "params": "online",
        "context": "4096",
        "release": "2023",
        "benchmark": {},
        "description": "A set of models that improve on GPT-3 and can understand as well as generate natural language or code."
    },
    "meta-llama/Llama-2-7b-chat-hf": {
        "base": "LLAMA",
        "type": ["chat", "instruct", "code"],
        "language": ["en"],
        "params": "7b",
        "context": "4096",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {
            "ARC": 52.9,
            "HellaSwag": 78.55,
            "MMLU": 48.32,
            "TruthfulQA": 45.57,
            "avg": 56.34
        },
        "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the description for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format."
    },
    "meta-llama/Llama-2-13b-chat-hf": {
        "base": "LLAMA",
        "type": ["chat", "instruct", "code"],
        "language": ["en"],
        "params": "13b",
        "context": "4096",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {
            "ARC": 59.04,
            "HellaSwag": 81.94,
            "MMLU": 54.64,
            "TruthfulQA": 44.12,
            "avg": 59.93
        },
        "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the description for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format."
    },
    "meta-llama/Llama-2-70b-chat-hf": {
        "model_name": "LLAMA",
        "type": ["chat", "instruct", "code"],
        "language": ["en"],
        "params": "70b",
        "context": "4096",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {
            "ARC": 64.59,
            "HellaSwag": 85.88,
            "MMLU": 63.91,
            "TruthfulQA": 52.8,
            "avg": 66.8
        },
        "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the description for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format."
    },
    "tiiuae/falcon-7b-instruct": {
        "base": "Falcon",
        "type": ["chat", "instruct", "code"],
        "language": ["en", "fr"],
        "params": "7b",
        "context": "2048",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {},
        "description": "Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets."
    },
    "tiiuae/falcon-40b-instruct": {
        "base": "Falcon",
        "type": ["chat", "instruct", "code"],
        "language": ["en", "fr"],
        "params": "40b",
        "context": "2048",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {
            "ARC": 61.6,
            "HellaSwag": 84.31,
            "MMLU": 55.45,
            "TruthfulQA": 52.52,
            "avg": 63.47
        },
        "description": "Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize."
    },
    "tiiuae/falcon-180B-chat": {
        "base": "Falcon",
        "type": ["chat", "instruct", "code"],
        "language": ["en", "de", "es", "fr"],
        "params": "180b",
        "context": "2048",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {},
        "description": "Falcon-180B-Chat is a 180B parameters causal decoder-only model built by TII based on Falcon-180B and finetuned on a mixture of Ultrachat, Platypus and Airoboros."
    },
    "FlagAlpha/Llama2-Chinese-7b-Chat": {
        "base": "LLAMA",
        "type": ["chat", "instruct", "code"],
        "language": ["cn", "en"],
        "params": "7b",
        "context": "4096",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {
            "ARC": 52.39,
            "HellaSwag": 77.52,
            "MMLU": 47.72,
            "TruthfulQA": 46.87,
            "avg": 56.13
        },
        "description": "Since Llama2 itself has weak Chinese alignment, we use the Chinese instruction set and fine-tune for meta-llama/Llama-2-7b-chat-hf to make it capable of strong Chinese dialog."
    },
    "FlagAlpha/Llama2-Chinese-13b-Chat": {
        "base": "LLAMA",
        "type": ["chat", "instruct", "code"],
        "language": ["cn", "en"],
        "params": "13b",
        "context": "4096",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {
            "ARC": 55.97,
            "HellaSwag": 82.05,
            "MMLU": 54.74,
            "TruthfulQA": 48.9,
            "avg": 60.41
        },
        "description": "Since Llama2 itself has weak Chinese alignment, we use the Chinese instruction set and fine-tune for meta-llama/Llama-2-13b-chat-hf to make it capable of strong Chinese dialog."
    },
    "baichuan-inc/Baichuan2-7B-Chat": {
        "base": "Baichuan",
        "type": ["chat", "instruct", "code"],
        "language": ["cn", "en"],
        "params": "7b",
        "context": "4096",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Baichuan 2 (7b) is the new generation of large-scale open-source language models launched by Baichuan Intelligence inc.. It is trained on a high-quality corpus with 2.6 trillion tokens and has achieved the promising performance in authoritative Chinese and English benchmarks of the same size."
    },
    "baichuan-inc/Baichuan2-13B-Chat": {
        "base": "Baichuan",
        "type": ["chat", "instruct", "code"],
        "language": ["cn", "en"],
        "params": "13b",
        "context": "4096",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Baichuan 2 (13b) is the new generation of large-scale open-source language models launched by Baichuan Intelligence inc.. It is trained on a high-quality corpus with 2.6 trillion tokens and has achieved the promising performance in authoritative Chinese and English benchmarks of the same size."
    },
    "elyza/ELYZA-japanese-Llama-2-7b-instruct": {
        "base": "LLAMA",
        "type": ["instruct", "code"],
        "language": ["jp", "en"],
        "params": "7b",
        "context": "4096",
        "precision": "fp16",
        "release": "2023",
        "benchmark": {
            "ARC": 53.16,
            "HellaSwag": 78.25,
            "MMLU": 47.07,
            "TruthfulQA": 39.08,
            "avg": 54.39
        },
        "description": "ELYZA-japanese-Llama-2-7b is a model based on Llama2 with additional find-tuning to extend Japanese language capabilities."
    },
    "codellama/CodeLlama-7b-Instruct-hf": {
        "base": "LLAMA",
        "type": ["code", "instruct"],
        "language": ["en"],
        "params": "34b",
        "context": "16384",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Code Llama (7b) is designed for general code synthesis and understanding."
    },
    "codellama/CodeLlama-13b-Instruct-hf": {
        "base": "LLAMA",
        "type": ["code", "instruct"],
        "language": ["en"],
        "params": "34b",
        "context": "16384",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Code Llama (13b) is designed for general code synthesis and understanding."
    },
    "codellama/CodeLlama-34b-Instruct-hf": {
        "base": "LLAMA",
        "type": ["code", "instruct"],
        "language": ["en"],
        "params": "34b",
        "context": "16384",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Code Llama (34b) is designed for general code synthesis and understanding."
    },
    "codellama/CodeLlama-7b-Python-hf": {
        "base": "LLAMA",
        "type": ["code", "instruct"],
        "language": ["en"],
        "params": "7b",
        "context": "16384",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Code Llama (7b, python) is designed specifically for generating python code."
    },
    "codellama/CodeLlama-13b-Python-hf": {
        "base": "LLAMA",
        "type": ["code", "instruct"],
        "language": ["en"],
        "params": "13b",
        "context": "16384",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Code Llama (13b, python) is designed specifically for generating python code."
    },
    "codellama/CodeLlama-34b-Python-hf": {
        "base": "LLAMA",
        "type": ["code", "instruct"],
        "language": ["en"],
        "params": "34b",
        "context": "16384",
        "precision": "bfp16",
        "release": "2023",
        "benchmark": {},
        "description": "Code Llama (34b, python) is designed specifically for generating python code."
    }
}