{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 42,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0,  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=20,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
    "    },\n",
    ")\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"\n",
    "    \n",
    "            You started doing the \"original task\" (below) but we had to stop due to rate limits, please can you try and contiue from where we got to? \n",
    "            \n",
    "            original task:\n",
    "            Create a Django project called 'photos' and create a web app with the following features:\n",
    "            1. an elegant and simple designed homepage\n",
    "            2. the ability for a user to login, register or recover their login\n",
    "            3. once the user has logged in they should be able to upload a photo and view past uploaded photos\n",
    "            \n",
    "        please ensure you test all aspects of the code to ensure it runs and functions correctly \n",
    "        \n",
    "        \n",
    "        \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_proxy (to chat_manager):\n",
      "\n",
      "\n",
      "    tell me a joke\n",
      "\n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The agent 'user_proxy' has an empty system_message, and may not work well with GroupChat.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant (to chat_manager):\n",
      "\n",
      "```python\n",
      "import random\n",
      "jokes = [\"Why was the math book sad? Because it had too many problems.\",\n",
      "         \"Why did the tomato turn red? Because it saw the salad dressing!\",\n",
      "         \"What do you call a fake noodle? An impasta!\"]\n",
      "print(random.choice(jokes))\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The agent 'user_proxy' has an empty system_message, and may not work well with GroupChat.\n",
      "GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned:\n",
      "\n",
      "The current role being played is Coder, so the next role to play would be Assistant.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
      "user_proxy (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The agent 'user_proxy' has an empty system_message, and may not work well with GroupChat.\n",
      "GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned:\n",
      "User\\_proxy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coder (to chat_manager):\n",
      "\n",
      "\n",
      "This code uses the `random` module in Python to select a random joke from a list of jokes. The joke is then printed to the console using the `print()` function.\n",
      "\n",
      "Here's a breakdown of the code:\n",
      "\n",
      "* `import random`: This line imports the `random` module, which is used to generate random numbers and to select elements randomly from a list.\n",
      "* `jokes = [\"Why was the math book sad? Because it had too many problems.\", \"Why did the tomato turn red? Because it saw the salad dressing!\", \"What do you call a fake noodle? An impasta!\"]`: This line defines a list of jokes. Each joke is a string that contains a pun or a play on words.\n",
      "* `print(random.choice(jokes))`: This line uses the `random.choice()` function to select a random element from the `jokes` list, and then prints it to the console using the `print()` function.\n",
      "\n",
      "The output of this code is:\n",
      "\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n",
      "\n",
      "This joke is a play on words, with \"saw\" having multiple meanings (seeing and red color).\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The agent 'user_proxy' has an empty system_message, and may not work well with GroupChat.\n",
      "GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned:\n",
      "User\\_Proxy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant (to chat_manager):\n",
      "\n",
      "```python\n",
      "import random\n",
      "jokes = [\"Why was the math book sad? Because it had too many problems.\",\n",
      "         \"Why did the tomato turn red? Because it saw the salad dressing!\",\n",
      "         \"What do you call a fake noodle? An impasta!\"]\n",
      "print(random.choice(jokes))\n",
      "```\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The agent 'user_proxy' has an empty system_message, and may not work well with GroupChat.\n",
      "GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned:\n",
      "User\\_Proxy\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_mistral = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"mistral\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "config_list_codellama = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"codellama\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config_mistral = {\"config_list\": config_list_mistral, \"temperature\": 0}\n",
    "llm_config_codellama = {\"config_list\": config_list_codellama, \"temperature\": 0}\n",
    "\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name = \"Assistant\",\n",
    "    llm_config = llm_config_mistral\n",
    ")\n",
    "\n",
    "coder = autogen.AssistantAgent(\n",
    "    name = \"Coder\",\n",
    "    llm_config = llm_config_codellama\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=20,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "task = \"\"\"\n",
    "    tell me a joke\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "groupchat = autogen.GroupChat(agents = [user_proxy, coder, assistant], messages = [], max_round=20)\n",
    "manager = autogen.GroupChatManager(groupchat= groupchat, llm_config = llm_config_mistral)\n",
    "user_proxy.initiate_chat(manager, message=task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
