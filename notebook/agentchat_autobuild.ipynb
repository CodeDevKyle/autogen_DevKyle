{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1004af6a7fbfcd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AutoBuild\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we introduce a new class, `AgentBuilder`, to help user build an automatic task solving process powered by multi-agent system. Specifically, our main building pipeline include `build()` and `start()`. In `build()`, we prompt a LLM to create multiple participant agent and initialize a group chat, and specify whether this task need programming to solve. After that, user can call `start()` at a proper time to complete the task. AutoBuilder also support open-source LLMs by [vLLM](https://docs.vllm.ai/en/latest/index.html) and [Fastchat](https://github.com/lm-sys/FastChat). Check the supported model list [here](https://docs.vllm.ai/en/latest/models/supported_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78dda8e3826d8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Requirement\n",
    "\n",
    "AutoBuild need the latest version of AutoGen.\n",
    "You can install AutoGen by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9ae50658be975",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install pyautogen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e63ab3604bdb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: prepare configuration\n",
    "Prepare a `config_path` for assistant agent to limit the choice of LLM you want to use in this task. This config can be a path of json file or a name of environment variable. A `default_llm_config` is also required for initialize the specific config of LLMs like seed, temperature, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2505f029423b21ab",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:18.987795800Z",
     "start_time": "2023-11-28T09:23:18.977219Z"
    }
   },
   "outputs": [],
   "source": [
    "config_path = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST'  # modify path\n",
    "default_llm_config = {\n",
    "    'temperature': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6586c68fa425b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: create a AgentBuilder\n",
    "Create a `AgentBuilder` with the specified `config_path`. AgentBuilder will use GPT-4 in default to complete the whole process, you can also change the `builder_model` to other OpenAI model if you want. You can also specify a OpenAI or open-source LLM as agent backbone, see blog for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa67c771a0fed37",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:19.902186400Z",
     "start_time": "2023-11-28T09:23:18.979228600Z"
    }
   },
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "builder = AgentBuilder(config_path=config_path, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a655fb6618324",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: specify a building task\n",
    "\n",
    "Specify a building task with a general description. Building task will help build manager (a LLM) decide what agents should be build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68315f6ec912c58a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:21.419250500Z",
     "start_time": "2023-11-28T09:23:21.413220200Z"
    }
   },
   "outputs": [],
   "source": [
    "building_task = \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782dd5ecb6c217a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: build group chat agents\n",
    "Use `build()` to let build manager (the specified `builder_model`) complete the group chat agents generation. If you think coding is necessary in your task, you can use `coding=True` to add a user proxy (an automatic code interpreter) into the agent list, like: \n",
    "```python\n",
    "builder.build(building_task, default_llm_config, coding=True)\n",
    "```\n",
    "If `coding` is not specified, AutoBuilder will determine on its own whether the user proxy should be added or not according to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab490fdbe46c0473",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:23.564327400Z",
     "start_time": "2023-11-28T09:23:23.488457300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating agents...\n",
      "Data_scientist,Domain_expert,Software_engineer are generated.\n",
      "Preparing configuration for Data_scientist...\n",
      "Preparing configuration for Domain_expert...\n",
      "Preparing configuration for Software_engineer...\n",
      "Creating agent Data_scientist with backbone gpt-4-1106-preview...\n",
      "Creating agent Domain_expert with backbone gpt-4-1106-preview...\n",
      "Creating agent Software_engineer with backbone gpt-4-1106-preview...\n",
      "Adding user console proxy...\n"
     ]
    }
   ],
   "source": [
    "builder.build(building_task, default_llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dd99880a4bf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5: execute task\n",
    "Let agents generated in `build()` to complete the task collaboratively in a group chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d52e3d9a1bf91cb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:27.331284800Z",
     "start_time": "2023-11-28T09:23:25.264410200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mSoftware_engineer\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to search for papers that match the query. We'll write a Python script that uses the `requests` library to make a GET request to the arXiv API with the appropriate query parameters. The script will parse the response to find the most recent paper related to GPT-4 and then print out the title, authors, summary, and potential applications in software.\n",
      "\n",
      "First, ensure you have the `requests` library installed. If not, you can install it using pip:\n",
      "\n",
      "```bash\n",
      "pip install requests\n",
      "```\n",
      "\n",
      "Now, here's the Python script to find the latest paper about GPT-4 on arXiv:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from datetime import datetime\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "# Function to search arXiv for papers related to GPT-4\n",
      "def search_arxiv(query):\n",
      "    # Define the URL and parameters for the arXiv API request\n",
      "    url = 'http://export.arxiv.org/api/query'\n",
      "    params = {\n",
      "        'search_query': query,\n",
      "        'sortBy': 'submittedDate',\n",
      "        'sortOrder': 'descending',\n",
      "        'max_results': 1\n",
      "    }\n",
      "    \n",
      "    # Make the GET request to the arXiv API\n",
      "    response = requests.get(url, params=params)\n",
      "    \n",
      "    # Check if the request was successful\n",
      "    if response.status_code == 200:\n",
      "        return response.text\n",
      "    else:\n",
      "        print(f\"Error: {response.status_code}\")\n",
      "        return None\n",
      "\n",
      "# Function to parse the XML response and extract paper details\n",
      "def parse_arxiv_response(xml_response):\n",
      "    # Parse the XML response\n",
      "    root = ET.fromstring(xml_response)\n",
      "    \n",
      "    # Find the entry element, which contains the paper information\n",
      "    entry = root.find('{http://www.w3.org/2005/Atom}entry')\n",
      "    \n",
      "    # Extract the title, authors, summary, and published date\n",
      "    title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
      "    authors = [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')]\n",
      "    summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
      "    published_date = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
      "    \n",
      "    return {\n",
      "        'title': title,\n",
      "        'authors': authors,\n",
      "        'summary': summary,\n",
      "        'published_date': published_date\n",
      "    }\n",
      "\n",
      "# Main function to execute the script\n",
      "def main():\n",
      "    # Search for the latest paper on GPT-4\n",
      "    query = 'all:gpt-4'\n",
      "    xml_response = search_arxiv(query)\n",
      "    \n",
      "    if xml_response:\n",
      "        # Parse the response and extract paper details\n",
      "        paper_details = parse_arxiv_response(xml_response)\n",
      "        \n",
      "        # Print the paper details\n",
      "        print(f\"Title: {paper_details['title']}\")\n",
      "        print(f\"Authors: {', '.join(paper_details['authors'])}\")\n",
      "        print(f\"Summary: {paper_details['summary']}\")\n",
      "        print(f\"Published Date: {paper_details['published_date']}\")\n",
      "        \n",
      "        # Analyze potential applications in software based on the summary\n",
      "        # This part requires human analysis and cannot be automated\n",
      "        print(\"\\nPotential applications in software:\")\n",
      "        print(\"The potential applications in software based on the summary of the paper would require a detailed analysis and understanding of the content. This step should be performed by a domain expert who can interpret the findings and suggest practical applications.\")\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "```\n",
      "\n",
      "This script will print the details of the latest paper about GPT-4 on arXiv. After running the script, you should manually analyze the summary to evaluate the potential applications in software. This analysis requires domain expertise and cannot be fully automated.\n",
      "\n",
      "Please run the script, and I will assist you with the analysis of the potential applications in software based on the information retrieved.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is bash)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Requirement already satisfied: requests in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests) (2023.7.22)\n",
      "\n",
      "Title: MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\n",
      "Authors: Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, Antoine Bosselut\n",
      "Summary: Large language models (LLMs) can potentially democratize access to medical\n",
      "knowledge. While many efforts have been made to harness and improve LLMs'\n",
      "medical knowledge and reasoning capacities, the resulting models are either\n",
      "closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters),\n",
      "which restricts their abilities. In this work, we improve access to large-scale\n",
      "medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B\n",
      "parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through\n",
      "our adaptation of Nvidia's Megatron-LM distributed trainer), and extends\n",
      "pretraining on a comprehensively curated medical corpus, including selected\n",
      "PubMed articles, abstracts, and internationally-recognized medical guidelines.\n",
      "Evaluations using four major medical benchmarks show significant performance\n",
      "gains over several state-of-the-art baselines before and after task-specific\n",
      "finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the\n",
      "best public baseline in its parameter class and 3% over the strongest baseline\n",
      "we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B\n",
      "outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of\n",
      "Med-PaLM-2. We release our code for curating the medical pretraining corpus and\n",
      "the MEDITRON model weights to drive open-source development of more capable\n",
      "medical LLMs.\n",
      "Published Date: 2023-11-27T18:49:43Z\n",
      "\n",
      "Potential applications in software:\n",
      "The potential applications in software based on the summary of the paper would require a detailed analysis and understanding of the content. This step should be performed by a domain expert who can interpret the findings and suggest practical applications.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mDomain_expert\u001B[0m (to chat_manager):\n",
      "\n",
      "Based on the summary of the paper titled \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models,\" we can infer several potential applications in software, particularly within the medical domain:\n",
      "\n",
      "1. **Medical Knowledge Access**: MEDITRON-70B, as an open-source large language model (LLM) adapted to the medical domain, can be integrated into software platforms to provide healthcare professionals and researchers with easier access to medical knowledge. This could be in the form of intelligent search engines or decision support systems that can understand and process complex medical queries.\n",
      "\n",
      "2. **Clinical Decision Support**: The model's ability to understand and process medical literature can be leveraged to develop clinical decision support tools that assist healthcare providers in diagnosing and treating patients. By integrating MEDITRON-70B into electronic health record (EHR) systems, it could provide real-time, evidence-based recommendations.\n",
      "\n",
      "3. **Medical Education**: Educational software can utilize MEDITRON-70B to create interactive learning experiences for medical students and professionals. The model can generate quizzes, explain complex medical concepts, and simulate patient interactions for training purposes.\n",
      "\n",
      "4. **Research Acceleration**: Researchers can use MEDITRON-70B to quickly sift through vast amounts of medical literature to find relevant studies, thus accelerating the pace of medical research. This could be particularly useful in systematic reviews and meta-analyses.\n",
      "\n",
      "5. **Telemedicine**: In telemedicine applications, MEDITRON-70B could be used to pre-assess patient symptoms and provide preliminary advice, helping to triage cases and manage healthcare resources more effectively.\n",
      "\n",
      "6. **Language Translation and Localization**: Given the model's large-scale pretraining on medical texts, it could be used to translate medical documents and guidelines into different languages, making medical knowledge more accessible globally.\n",
      "\n",
      "7. **Personalized Medicine**: Software applications could use MEDITRON-70B to analyze patient data and generate personalized treatment plans based on the latest medical research and guidelines.\n",
      "\n",
      "8. **Healthcare Chatbots**: The model could power chatbots that provide medical information to patients, answer frequently asked questions, and guide them through administrative processes like appointment scheduling.\n",
      "\n",
      "9. **Drug Discovery and Development**: MEDITRON-70B could assist in the drug discovery process by analyzing scientific literature to identify potential drug targets and predict the effects of drug compounds.\n",
      "\n",
      "10. **Public Health Monitoring**: The model could be used to monitor public health trends by analyzing social media, news articles, and other text sources for mentions of disease outbreaks or health-related events.\n",
      "\n",
      "It's important to note that while these applications are promising, they also require careful consideration of ethical implications, data privacy, and the need for rigorous validation to ensure patient safety and the accuracy of medical information provided by the software.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "builder.start(task=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a30e4b4297edd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6 (Optional): clear all agents and prepare for the next task\n",
    "You can clear all agents generated in this task by the following code if your task is completed or the next task is largely different from the current task. If the agent's backbone is an open-source LLM, this process will also shutdown the endpoint server. If necessary, you can use `recycle_endpoint=False` to retain the previous open-source LLMs' endpoint server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb0bfff01dd1330",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:28.687772900Z",
     "start_time": "2023-11-28T09:23:28.681728600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "builder.clear_all_agents(recycle_endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb098638a086898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save & load configs\n",
    "\n",
    "You can save all necessary information of the built group chat agents. Here is a case for those agents generated in the above task:\n",
    "```json\n",
    "{\n",
    "    \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    \"agent_configs\": [\n",
    "        {\n",
    "            \"name\": \"Data_scientist\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Data Scientist, you are tasked with automating the retrieval and analysis of academic papers from arXiv. Utilize your Python programming acumen to develop scripts for gathering necessary information such as searching for relevant papers, downloading them, and processing their contents. Apply your analytical and language skills to interpret the data and deduce the applications of the research within specific domains.\\n\\n1. To compile information, write and implement Python scripts that search and interact with online resources, download and read files, extract content from documents, and perform other information-gathering tasks. Use the printed output as the foundation for your subsequent analysis.\\n\\n2. Execute tasks programmatically with Python scripts when possible, ensuring results are directly displayed. Approach each task with efficiency and strategic thinking.\\n\\nProgress through tasks systematically. In instances where a strategy is not provided, outline your plan before executing. Clearly distinguish between tasks handled via code and those utilizing your analytical expertise.\\n\\nWhen providing code, include only Python scripts meant to be run without user alterations. Users should execute your script as is, without modifications:\\n\\n```python\\n# filename: <filename>\\n# Python script\\nprint(\\\"Your output\\\")\\n```\\n\\nUsers should not perform any actions other than running the scripts you provide. Avoid presenting partial or incomplete scripts that require user adjustments. Refrain from requesting users to copy-paste results; instead, use the 'print' function when suitable to display outputs. Monitor the execution results they share.\\n\\nIf an error surfaces, supply corrected scripts for a re-run. If the strategy fails to resolve the issue, reassess your assumptions, gather additional details as needed, and explore alternative approaches.\\n\\nUpon successful completion of a task and verification of the results, confirm the achievement of the stated objective. Ensuring accuracy and validity of the findings is paramount. Evidence supporting your conclusions should be provided when feasible.\\n\\nUpon satisfying the user's needs and ensuring all tasks are finalized, conclude your assistance with \\\"TERMINATE\\\".\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Research_analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Research Analyst, you are expected to be a proficient AI assistant possessing a strong grasp of programming, specifically in Python, and robust analytical capabilities. Your primary responsibilities will include:\\n\\n1. Conducting comprehensive searches and retrieving information autonomously through Python scripts, such as querying databases, accessing web services (like arXiv), downloading and reading files, and retrieving system information.\\n2. Analyzing the content of the retrieved documents, particularly academic papers, and extracting insights regarding their application in specific domains, such as the potential uses of GPT-4 in software development.\\n3. Presenting your findings in a clear, detailed manner, explaining the implications of the research and its relevance to the assigned task.\\n4. Employing your programming skills to automate tasks where possible, ensuring the output is delivered through Python code with clear, executable instructions. Your code will be designed for the user to execute without amendment or additional input.\\n5. Verifying the results of information gathering and analysis to ensure accuracy and completeness, providing evidence to support your conclusions when available.\\n6. Communicating the completion of each task and confirming that the user's needs have been satisfied through a clear and conclusive statement, followed by the word \\\"TERMINATE\\\" to signal the end of the interaction.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Software_developer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a dedicated AI assistant for a software developer, your role involves employing your Python programming prowess and proficiency in natural language processing to facilitate the discovery and analysis of scholarly articles on arXiv. Your tasks include crafting Python scripts to automatically search, retrieve, and present information regarding the latest research, with a focus on applicable advancements in technology such as GPT-4 and its potential impact on the domain of software development.\\n\\n1. Utilize Python to programmatically seek out and extract pertinent data, for example, navigating or probing the web, downloading/ingesting documents, or showcasing content from web pages or files. When enough information has been accumulated to proceed, you will then analyze and interpret the findings.\\n\\n2. When there's a need to perform an operation programmatically, your Python code should accomplish the task and manifest the outcome. Progress through the task incrementally and systematically.\\n\\nProvide a clear plan outlining each stage of the task, specifying which components will be executed through Python coding and which through your linguistic capabilities. When proposing Python code, remember to:\\n\\n- Label the script type within the code block\\n- Avoid suggesting code that the user would need to alter\\n- Refrain from including more than one code block in your response\\n- Circumvent requesting the user to manually transcribe any results; utilize 'print' statements where applicable\\n- Examine the user's reported execution outcomes\\n\\nIf an error arises, your responsibility is to rectify the issue and submit the corrected script. Should an error remain unresolvable, or if the task remains incomplete post successful code execution, re-evaluate the scenario, gather any further required information, and formulate an alternative approach.\\n\\nUpon confirming that the task has been satisfactorily accomplished and the user's requirements have been met, indicate closure of the procedure with a concluding statement.\"\n",
    "        }\n",
    "    ],\n",
    "    \"manager_system_message\": \"Group chat manager.\",\n",
    "    \"coding\": true,\n",
    "    \"default_llm_config\": {\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "These information will be saved in JSON format. You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with a generated filename 'save_config_TASK_MD5.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b88a5d482ceba4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:29.950261800Z",
     "start_time": "2023-11-28T09:23:29.944238100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building config saved to ./save_config_eb1be857faa608aeb4c5af11fe4ab245.json\n"
     ]
    }
   ],
   "source": [
    "saved_path = builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35620c10ee42be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, you can load the saved config and skip the building process. AutoBuilder will create agents with those information without prompting the builder manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34addd498e5ab174",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:23:39.363905700Z",
     "start_time": "2023-11-28T09:23:31.493375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loding config from ./save_config_eb1be857faa608aeb4c5af11fe4ab245.json\n",
      "Creating agent Data_scientist with backbone gpt-4-1106-preview...\n",
      "Creating agent Domain_expert with backbone gpt-4-1106-preview...\n",
      "Creating agent Software_engineer with backbone gpt-4-1106-preview...\n",
      "Adding user console proxy...\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "Find a latest paper about Llava on arxiv and find its potential applications in multi-modal model.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mSoftware_engineer\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about \"Llava\" on arXiv and evaluate its potential applications in multi-modal models, we will break down the task into the following steps:\n",
      "\n",
      "1. Search for the latest papers on arXiv with the keyword \"Llava\".\n",
      "2. Download and read the most recent paper found.\n",
      "3. Analyze the content to determine its potential applications in multi-modal models.\n",
      "\n",
      "Let's start with step 1. We will use the `arxiv` Python library to search for papers. If you don't have the `arxiv` library installed, you can install it using pip:\n",
      "\n",
      "```bash\n",
      "pip install arxiv\n",
      "```\n",
      "\n",
      "Now, let's write a Python script to search for the latest papers on arXiv with the keyword \"Llava\":\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "import datetime\n",
      "\n",
      "# Define the search query and parameters\n",
      "search_query = 'all:Llava'\n",
      "sort_by = arxiv.SortCriterion.SubmittedDate\n",
      "sort_order = arxiv.SortOrder.Descending\n",
      "\n",
      "# Search for papers on arXiv\n",
      "search = arxiv.Search(\n",
      "    query=search_query,\n",
      "    max_results=1,\n",
      "    sort_by=sort_by,\n",
      "    sort_order=sort_order\n",
      ")\n",
      "\n",
      "# Fetch the result\n",
      "try:\n",
      "    paper = next(search.results())\n",
      "    print(f\"Title: {paper.title}\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in paper.authors)}\")\n",
      "    print(f\"Abstract: {paper.summary}\")\n",
      "    print(f\"Published: {paper.published}\")\n",
      "    print(f\"URL: {paper.entry_id}\")\n",
      "except StopIteration:\n",
      "    print(\"No papers found with the keyword 'Llava'.\")\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "```\n",
      "\n",
      "This script will search for the most recent papers on arXiv with the keyword \"Llava\" and print out the title, authors, abstract, publication date, and URL of the latest paper.\n",
      "\n",
      "Please run this script in your Python environment to find the latest paper. Once we have the paper, we can proceed to step 2 and 3 to download, read, and analyze its content for potential applications in multi-modal models.\n",
      "\n",
      "After running the script, please provide the title and abstract of the paper so that we can discuss its potential applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is bash)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Requirement already satisfied: arxiv in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2023.7.22)\n",
      "\n",
      "Title: PG-Video-LLaVA: Pixel Grounding Large Video-Language Models\n",
      "Authors: Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, Fahad Khan\n",
      "Abstract: Extending image-based Large Multimodal Models (LMM) to videos is challenging\n",
      "due to the inherent complexity of video data. The recent approaches extending\n",
      "image-based LMM to videos either lack the grounding capabilities (e.g.,\n",
      "VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for\n",
      "better video understanding (e.g., Video-ChatGPT). Addressing these gaps, we\n",
      "propose Video-LLaVA, the first LMM with pixel-level grounding capability,\n",
      "integrating audio cues by transcribing them into text to enrich video-context\n",
      "understanding. Our framework uses an off-the-shelf tracker and a novel\n",
      "grounding module, enabling it to spatially and temporally localize objects in\n",
      "videos following user instructions. We evaluate Video-LLaVA using video-based\n",
      "generative and question-answering benchmarks and introduce new benchmarks\n",
      "specifically designed to measure prompt-based object grounding performance in\n",
      "videos. Further, we propose the use of Vicuna over GPT-3.5, as utilized in\n",
      "Video-ChatGPT, for video-based conversation benchmarking, ensuring\n",
      "reproducibility of results which is a concern with the proprietary nature of\n",
      "GPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its\n",
      "advantages to the video domain, delivering promising gains on video-based\n",
      "conversation and grounding tasks. Project Page:\n",
      "https://github.com/mbzuai-oryx/Video-LLaVA\n",
      "Published: 2023-11-22 14:48:30+00:00\n",
      "URL: http://arxiv.org/abs/2311.13435v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mDomain_expert\u001B[0m (to chat_manager):\n",
      "\n",
      "Based on the information provided, the paper titled \"PG-Video-LLaVA: Pixel Grounding Large Video-Language Models\" presents a Large Multimodal Model (LMM) that extends image-based LMMs to videos by incorporating pixel-level grounding capabilities and integrating audio cues through text transcription to enhance video-context understanding.\n",
      "\n",
      "The potential applications of Video-LLaVA in multi-modal models are significant:\n",
      "\n",
      "1. **Enhanced Video Understanding**: Video-LLaVA's ability to spatially and temporally localize objects in videos can be applied to improve video analysis systems, such as those used in surveillance, sports analytics, and content moderation.\n",
      "\n",
      "2. **Interactive Video Systems**: The grounding module allows for interactive systems where users can query specific objects or events within a video, which could be useful in educational platforms, interactive storytelling, or customer service bots that interact with video content.\n",
      "\n",
      "3. **Accessibility**: By transcribing audio signals and grounding visual content, Video-LLaVA can aid in creating more accessible content for individuals with hearing or visual impairments, providing richer descriptions of video content.\n",
      "\n",
      "4. **Content Creation and Editing**: The model's grounding capabilities can be leveraged in video editing software to allow creators to manipulate video content through natural language commands, streamlining the editing process.\n",
      "\n",
      "5. **Video-Based Conversational Agents**: The paper mentions video-based conversation benchmarking, which implies that Video-LLaVA could be used to develop conversational agents that understand and respond to video content, enhancing customer support or entertainment applications.\n",
      "\n",
      "6. **Research and Education**: In academic settings, Video-LLaVA could be used to analyze educational videos, providing summaries, answering questions, and helping students locate information within video lectures.\n",
      "\n",
      "7. **Marketing and Advertising**: Companies could use Video-LLaVA to analyze consumer videos for product placement, sentiment analysis, and to understand consumer behavior through video content.\n",
      "\n",
      "8. **Integration with Robotics**: Robots equipped with video processing capabilities could use Video-LLaVA to better understand their environment and interact with it based on visual and audio cues.\n",
      "\n",
      "The paper's introduction of new benchmarks for prompt-based object grounding performance in videos also suggests that Video-LLaVA could push the boundaries of what is possible in video understanding and interaction, leading to more sophisticated multi-modal applications.\n",
      "\n",
      "For further analysis, one would need to delve into the technical details of the paper, experiment with the model, and possibly contribute to the project, which is available on the provided GitHub page.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path).load(saved_path)\n",
    "new_builder.start(task=\"Find a latest paper about Llava on arxiv and find its potential applications in multi-modal model.\")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0cf8f09eef5cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use GPTs\n",
    "\n",
    "[GPTs](https://openai.com/blog/introducing-gpts) allow user to create an assistant with a simple instruction of the task. It has plugin support that can let ChatGPT complete some complex instructions, and can optionally update the assistant's instruction to let it adapted to new task or improve on the current task.\n",
    "AutoBuild also support GPTs api by adding `use_gpts=True` to the `build()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4051c25b2cd1918c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:26:01.069144900Z",
     "start_time": "2023-11-28T09:23:39.355869200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating agents...\n",
      "Data_scientist,Domain_expert,Software_engineer are generated.\n",
      "Preparing configuration for Data_scientist...\n",
      "Preparing configuration for Domain_expert...\n",
      "Preparing configuration for Software_engineer...\n",
      "Creating agent Data_scientist with backbone gpt-4-1106-preview...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple assistants with name Data_scientist found. Using the first assistant in the list. Please specify the assistant ID in llm_config to use a specific assistant.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent Domain_expert with backbone gpt-4-1106-preview...\n",
      "Creating agent Software_engineer with backbone gpt-4-1106-preview...\n",
      "Adding user console proxy...\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "Find a latest paper about XAI on arxiv and find its potential applications in medical.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mSoftware_engineer\u001B[0m (to chat_manager):\n",
      "\n",
      "To accomplish the task of finding the latest paper about Explainable Artificial Intelligence (XAI) on arXiv and evaluating its potential applications in the medical field, we will break down the task into several steps:\n",
      "\n",
      "1. **Search arXiv for the latest papers on XAI**: Use the arXiv API to search for papers that match the topic \"Explainable Artificial Intelligence\".\n",
      "2. **Download and read the paper's summary**: Extract and present the content of the paper's summary to gain insight into its subject matter.\n",
      "3. **Evaluate real-world applications in medicine**: Analyze the content for potential applications within the medical field.\n",
      "\n",
      "Let's start with the first step:\n",
      "\n",
      "### Step 1 - Searching for the latest XAI papers on arXiv\n",
      "\n",
      "We will use the `arxiv` Python library to search for papers. If you don't have it installed, you can install it using `pip install arxiv`.\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "import datetime\n",
      "\n",
      "# Define the query parameters\n",
      "query = \"cat:cs.AI AND ti:XAI\"\n",
      "sort_by = arxiv.SortCriterion.SubmittedDate  # Sort by most recent submissions\n",
      "\n",
      "# Search for papers on arXiv that match the query\n",
      "search = arxiv.Search(\n",
      "    query=query,\n",
      "    sort_by=sort_by,\n",
      "    sort_order=arxiv.SortOrder.Descending,\n",
      "    max_results=1\n",
      ")\n",
      "\n",
      "# Fetch the latest paper\n",
      "for paper in search.results():\n",
      "    print(f\"Title: {paper.title}\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in paper.authors)}\")\n",
      "    print(f\"Submitted Date: {paper.published}\")\n",
      "    print(f\"Summary: {paper.summary.replace('\\n', ' ')}\")\n",
      "    print(f\"URL: {paper.entry_id}\")\n",
      "    break  # We only want the latest paper\n",
      "```\n",
      "\n",
      "Once you run this script, it should output the title, authors, submission date, and summary of the latest paper about Explainable Artificial Intelligence on arXiv, as well as provide a URL to the paper.\n",
      "\n",
      "### Step 2 - Reading the paper's summary\n",
      "\n",
      "The script above already takes care of downloading and presenting the summary of the latest paper. This will provide us with an overview of the paper's content and potential applications.\n",
      "\n",
      "### Step 3 - Evaluating potential medical applications\n",
      "\n",
      "For evaluating real-world applications in medicine, we would need to read and understand the content of the paper's summary. Especially, we would look for:\n",
      "- Any mention of medical datasets or case studies within the paper summary.\n",
      "- Generalizable findings or methods that could be applied to medical scenarios, such as diagnostic tools, drug discovery, patient data analysis, etc.\n",
      "\n",
      "After you have executed the Python script and obtained the latest paper's summary information, review the content to determine if there are potential applications in the medical field. I can guide you through this analysis by asking you to consider the following questions:\n",
      "\n",
      "- Does the paper discuss applications or case studies related to medicine or healthcare?\n",
      "- Are the methods or approaches described in the paper suitable for tasks in the medical domain, such as interpreting medical images, analyzing genomic data, or personalizing treatment plans?\n",
      "\n",
      "If you share the summary with me, I can help in conceptualizing how the findings in the paper might be applied to the medical field. However, I cannot analyse the content myself, you will have to apply this analysis based on my guidance.\n",
      "\n",
      "Please run the code provided to find the latest paper on XAI from arXiv and share the summary with me for further guidance on potential medical applications.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "  File \"\", line 21\n",
      "    print(f\"Summary: {paper.summary.replace('\\n', ' ')}\")\n",
      "                                                        ^\n",
      "SyntaxError: f-string expression part cannot include a backslash\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mSoftware_engineer\u001B[0m (to chat_manager):\n",
      "\n",
      "It appears there was an error due to the use of a backslash within an f-string expression. Let's correct the script to properly print the summary without causing a syntax error. We can use double quotes for the string inside the `replace` method to avoid this issue.\n",
      "\n",
      "Here is the corrected Python script:\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "import datetime\n",
      "\n",
      "# Define the query parameters\n",
      "query = \"cat:cs.AI AND ti:XAI\"\n",
      "sort_by = arxiv.SortCriterion.SubmittedDate  # Sort by most recent submissions\n",
      "\n",
      "# Search for papers on arXiv that match the query\n",
      "search = arxiv.Search(\n",
      "    query=query,\n",
      "    sort_by=sort_by,\n",
      "    sort_order=arxiv.SortOrder.Descending,\n",
      "    max_results=1\n",
      ")\n",
      "\n",
      "# Fetch the latest paper\n",
      "for paper in search.results():\n",
      "    print(f\"Title: {paper.title}\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in paper.authors)}\")\n",
      "    print(f\"Submitted Date: {paper.published}\")\n",
      "    summary = paper.summary.replace(\"\\n\", \" \")\n",
      "    print(f\"Summary: {summary}\")\n",
      "    print(f\"URL: {paper.entry_id}\")\n",
      "    break  # We only want the latest paper\n",
      "```\n",
      "\n",
      "Please run the corrected script to get the details of the latest paper regarding Explainable Artificial Intelligence (XAI) on arXiv. Once you obtain the summary, we can discuss how to conceptualize its real-world applications in the medical field.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Forms of Understanding of XAI-Explanations\n",
      "Authors: Hendrik Buschmeier, Heike M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, André Groß, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, Britta Wrede\n",
      "Submitted Date: 2023-11-15 08:06:51+00:00\n",
      "Summary: Explainability has become an important topic in computer science and artificial intelligence, leading to a subfield called Explainable Artificial Intelligence (XAI). The goal of providing or seeking explanations is to achieve (better) 'understanding' on the part of the explainee. However, what it means to 'understand' is still not clearly defined, and the concept itself is rarely the subject of scientific investigation. This conceptual article aims to present a model of forms of understanding in the context of XAI and beyond. From an interdisciplinary perspective bringing together computer science, linguistics, sociology, and psychology, a definition of understanding and its forms, assessment, and dynamics during the process of giving everyday explanations are explored. Two types of understanding are considered as possible outcomes of explanations, namely enabledness, 'knowing how' to do or decide something, and comprehension, 'knowing that' -- both in different degrees (from shallow to deep). Explanations regularly start with shallow understanding in a specific domain and can lead to deep comprehension and enabledness of the explanandum, which we see as a prerequisite for human users to gain agency. In this process, the increase of comprehension and enabledness are highly interdependent. Against the background of this systematization, special challenges of understanding in XAI are discussed.\n",
      "URL: http://arxiv.org/abs/2311.08760v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mDomain_expert\u001B[0m (to chat_manager):\n",
      "\n",
      "Based on the summary of the paper titled \"Forms of Understanding of XAI-Explanations,\" we can glean that this paper is conceptual in nature, aiming to define and explore the concept of 'understanding' in the context of Explainable Artificial Intelligence (XAI). The interdisciplinary perspective adopted by the authors examines the dynamics of understanding during the process of providing explanations, particularly focusing on two types of understanding: enabledness (knowing how) and comprehension (knowing that), ranging from shallow to deep levels.\n",
      "\n",
      "While the paper does not explicitly target the medical field, we can conceptualize its potential applications in medicine by considering how these concepts could enhance AI's role in healthcare:\n",
      "\n",
      "1. **Patient Education**: Utilizing the concept of 'understanding' to explain AI-driven medical diagnoses or treatment plans to patients, enabling them to comprehend their condition and the suggested medical interventions. By converting complex medical data into understandable explanations, patients can make more informed decisions about their health.\n",
      "\n",
      "2. **Clinical Decision Support**: Improvement in clinician's adoption of AI tools for decision-making by employing explanations that focus on enabledness (knowing how) to use AI recommendations and comprehension (knowing that) to understand the underlying reasons for AI's advice.\n",
      "\n",
      "3. **Medical Study and Research**: Enabling medical researchers to leverage AI tools that can explain their outputs in a way that provides deep understanding, thereby increasing trust in AI's ability to identify patterns and offer insights in complex datasets like genomic information or epidemiological studies.\n",
      "\n",
      "4. **Training Healthcare Professionals**: Enhancing the educational tools for healthcare professionals by incorporating XAI systems that provide varying degrees of understanding, from basic conceptual learning to advanced clinical applications.\n",
      "\n",
      "5. **Ethics and Legal Compliance**: The ability to provide understandable explanations for AI's actions could facilitate compliance with ethical guidelines and legal requirements, especially in sensitive areas such as patient data handling or algorithmic decisions affecting patient care.\n",
      "\n",
      "In summary, while the paper presents a conceptual model of understanding applicable to XAI systems, its principles could potentially be adapted and used to develop AI tools that are more transparent, trusted, and user-friendly in the medical domain. Applications could span patient interaction, clinical support, medical research, educational initiatives, and compliance requirements, all of which benefit from a higher degree of explainability and understanding provided by AI systems.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path)\n",
    "new_builder.build(building_task, default_llm_config, use_gpts=True)  # Transfer to GPTs API.\n",
    "new_builder.start(task=\"Find a latest paper about XAI on arxiv and find its potential applications in medical.\")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57194293da4ae4e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
