{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1004af6a7fbfcd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AutoBuild\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we introduce a new class, `AgentBuilder`, to help user build an automatic task solving process powered by multi-agent system. Specifically, our main building pipeline include `build()` and `start()`. In `build()`, we prompt a LLM to create multiple participant agent and initialize a group chat, and specify whether this task need programming to solve. After that, user can call `start()` at a proper time to complete the task. AutoBuilder also support open-source LLMs by [vLLM](https://docs.vllm.ai/en/latest/index.html) and [Fastchat](https://github.com/lm-sys/FastChat). Check the supported model list [here](https://docs.vllm.ai/en/latest/models/supported_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78dda8e3826d8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Requirement\n",
    "\n",
    "AutoBuild need the latest version of AutoGen.\n",
    "You can install AutoGen by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9ae50658be975",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install pyautogen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e63ab3604bdb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: prepare configuration\n",
    "Prepare a `config_path` for assistant agent to limit the choice of LLM you want to use in this task. This config can be a path of json file or a name of environment variable. A `default_llm_config` is also required for initialize the specific config of LLMs like seed, temperature, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2505f029423b21ab",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T05:14:16.863997100Z",
     "start_time": "2023-11-27T05:14:16.846959100Z"
    }
   },
   "outputs": [],
   "source": [
    "config_path = 'YOUR CONFIG PATH'  # modify path\n",
    "default_llm_config = {\n",
    "    'temperature': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6586c68fa425b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: create a AgentBuilder\n",
    "Create a `AgentBuilder` with the specified `config_path`. AgentBuilder will use GPT-4 in default to complete the whole process, you can also change the `builder_model` to other OpenAI model if you want. You can also specify a OpenAI or open-source LLM as agent backbone, see blog for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa67c771a0fed37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T06:42:24.270587200Z",
     "start_time": "2023-11-26T06:42:22.943534500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "builder = AgentBuilder(config_path=config_path, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a655fb6618324",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: specify a building task\n",
    "\n",
    "Specify a building task with a general description. Building task will help build manager (a LLM) decide what agents should be build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68315f6ec912c58a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-26T06:42:20.859520700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building_task = \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782dd5ecb6c217a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: build group chat agents\n",
    "Use `build()` to let build manager (the specified `builder_model`) complete the group chat agents generation. If you think coding is necessary in your task, you can use `coding=True` to add a user proxy (an automatic code interpreter) into the agent list, like: \n",
    "```python\n",
    "builder.build(building_task, default_llm_config, coding=True)\n",
    "```\n",
    "If `coding` is not specified, AutoBuilder will determine on its own whether the user proxy should be added or not according to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab490fdbe46c0473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T06:42:25.995470100Z",
     "start_time": "2023-11-26T06:42:25.917253700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating agents...\n",
      "Data_scientist,Research_analyst,Software_developer are generated.\n",
      "Preparing configuration for Data_scientist...\n",
      "Preparing configuration for Research_analyst...\n",
      "Preparing configuration for Software_developer...\n",
      "Creating agent Data_scientist with backbone gpt-4-1106-preview...\n",
      "Creating agent Research_analyst with backbone gpt-4-1106-preview...\n",
      "Creating agent Software_developer with backbone gpt-4-1106-preview...\n",
      "Adding user console proxy...\n"
     ]
    }
   ],
   "source": [
    "builder.build(building_task, default_llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dd99880a4bf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5: execute task\n",
    "Use agents generated in `build()` to complete the task collaboratively in a group chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d52e3d9a1bf91cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:27:43.776416600Z",
     "start_time": "2023-11-25T19:27:40.439581500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mSoftware_developer\u001B[0m (to chat_manager):\n",
      "\n",
      "To accomplish this task, we will follow these steps:\n",
      "\n",
      "1. **Search arXiv for the latest papers on GPT-4**: We will use the `arxiv` Python library to search for the most recent papers related to GPT-4. If the `arxiv` library is not installed, you will need to install it using `pip install arxiv`.\n",
      "\n",
      "2. **Extract Information**: From the search results, we will extract the title, authors, summary, and publication date of the latest paper.\n",
      "\n",
      "3. **Analyze for Potential Applications in Software Development**: We will read through the summary to identify mentions of potential applications in software development.\n",
      "\n",
      "4. **Present Findings**: We will print out the relevant information.\n",
      "\n",
      "Let's start with step 1. Here is the Python code to search for the latest papers on GPT-4 on arXiv:\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "\n",
      "# Define the search query and parameters\n",
      "search_query = 'all:\"GPT-4\"'\n",
      "sort_by = arxiv.SortCriterion.SubmittedDate\n",
      "sort_order = arxiv.SortOrder.Descending\n",
      "\n",
      "# Perform the search\n",
      "search = arxiv.Search(\n",
      "  query=search_query,\n",
      "  max_results=1,\n",
      "  sort_by=sort_by,\n",
      "  sort_order=sort_order\n",
      ")\n",
      "\n",
      "# Fetch the result\n",
      "for result in search.results():\n",
      "    print(\"Title:\", result.title)\n",
      "    print(\"Authors:\", \", \".join(author.name for author in result.authors))\n",
      "    print(\"Abstract:\", result.summary)\n",
      "    print(\"Publication Date:\", result.published)\n",
      "    print(\"URL:\", result.entry_id)\n",
      "    break  # Since we only want the latest paper, we break after the first result\n",
      "```\n",
      "\n",
      "Please run this code in your Python environment to retrieve the latest paper on GPT-4 from arXiv. Once you have the paper, we can proceed to analyze its content for potential applications in software development.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs\n",
      "Authors: Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li\n",
      "Abstract: In the field of document understanding, significant advances have been made\n",
      "in the fine-tuning of Multimodal Large Language Models (MLLMs) with\n",
      "instruction-following data. Nevertheless, the potential of text-grounding\n",
      "capability within text-rich scenarios remains underexplored. In this paper, we\n",
      "present a text-grounding document understanding model, termed TGDoc, which\n",
      "addresses this deficiency by enhancing MLLMs with the ability to discern the\n",
      "spatial positioning of text within images. Empirical evidence suggests that\n",
      "text-grounding improves the model's interpretation of textual content, thereby\n",
      "elevating its proficiency in comprehending text-rich images. Specifically, we\n",
      "compile a dataset containing 99K PowerPoint presentations sourced from the\n",
      "internet. We formulate instruction tuning tasks including text detection,\n",
      "recognition, and spotting to facilitate the cohesive alignment between the\n",
      "visual encoder and large language model. Moreover, we curate a collection of\n",
      "text-rich images and prompt the text-only GPT-4 to generate 12K high-quality\n",
      "conversations, featuring textual locations within text-rich scenarios. By\n",
      "integrating text location data into the instructions, TGDoc is adept at\n",
      "discerning text locations during the visual question process. Extensive\n",
      "experiments demonstrate that our method achieves state-of-the-art performance\n",
      "across multiple text-rich benchmarks, validating the effectiveness of our\n",
      "method.\n",
      "Publication Date: 2023-11-22 06:46:37+00:00\n",
      "URL: http://arxiv.org/abs/2311.13194v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mResearch_analyst\u001B[0m (to chat_manager):\n",
      "\n",
      "Based on the information retrieved from arXiv, the latest paper related to GPT-4 is titled \"Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs\" by Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and Houqiang Li. The paper was published on November 22, 2023.\n",
      "\n",
      "**Abstract Summary and Potential Applications in Software Development:**\n",
      "The paper discusses the enhancement of Multimodal Large Language Models (MLLMs) with text-grounding capabilities to improve document understanding. The authors present a model called TGDoc, which is designed to discern the spatial positioning of text within images. This is particularly useful for understanding text-rich images, such as PowerPoint presentations.\n",
      "\n",
      "The potential applications in software development could include:\n",
      "\n",
      "1. **Document Management Systems**: The TGDoc model could be integrated into document management systems to improve the searchability and categorization of documents by understanding the spatial layout of text within images.\n",
      "\n",
      "2. **Content Creation Tools**: Software tools for creating presentations or other visual content could benefit from TGDoc by providing suggestions or automations based on the textual content within the images.\n",
      "\n",
      "3. **Accessibility Features**: The ability to understand text within images could enhance accessibility features in software, allowing for better text-to-speech conversion for visually impaired users.\n",
      "\n",
      "4. **Data Extraction and Analysis**: TGDoc could be used in data extraction tools to pull out relevant information from text-rich images for analysis or reporting purposes.\n",
      "\n",
      "5. **Enhanced User Interfaces**: User interfaces that can interpret the spatial layout of text could provide more intuitive interactions, especially in applications that deal with document editing or creation.\n",
      "\n",
      "6. **Automated Customer Support**: By understanding text within images, customer support bots could provide more accurate responses to user inquiries that involve screenshots or other visual aids.\n",
      "\n",
      "The paper's approach to integrating text location data into instructions for better comprehension of text-rich images could be a significant step forward in the development of more intelligent and context-aware software applications.\n",
      "\n",
      "**Publication Details:**\n",
      "- **Title**: Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs\n",
      "- **Authors**: Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li\n",
      "- **Abstract URL**: [http://arxiv.org/abs/2311.13194v1](http://arxiv.org/abs/2311.13194v1)\n",
      "- **Publication Date**: November 22, 2023\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "builder.start(task=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a30e4b4297edd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6 (Optional): clear all agents and prepare for the next task\n",
    "You can clear all agents generated in this task by the following code if your task is completed or the next task is largely different from the current task. If the agent's backbone is an open-source LLM, this process will also shutdown the endpoint server. If necessary, you can use `recycle_endpoint=False` to retain the previous open-source LLMs' endpoint server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb0bfff01dd1330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:29:39.150365Z",
     "start_time": "2023-11-25T19:29:39.147791300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "builder.clear_all_agents(recycle_endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb098638a086898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save & load configs\n",
    "\n",
    "You can save all necessary information of the built group chat agents. Here is a case for those agents generated in the above task:\n",
    "```json\n",
    "{\n",
    "    \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    \"agent_configs\": [\n",
    "        {\n",
    "            \"name\": \"Data_scientist\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Data Scientist, you are tasked with automating the retrieval and analysis of academic papers from arXiv. Utilize your Python programming acumen to develop scripts for gathering necessary information such as searching for relevant papers, downloading them, and processing their contents. Apply your analytical and language skills to interpret the data and deduce the applications of the research within specific domains.\\n\\n1. To compile information, write and implement Python scripts that search and interact with online resources, download and read files, extract content from documents, and perform other information-gathering tasks. Use the printed output as the foundation for your subsequent analysis.\\n\\n2. Execute tasks programmatically with Python scripts when possible, ensuring results are directly displayed. Approach each task with efficiency and strategic thinking.\\n\\nProgress through tasks systematically. In instances where a strategy is not provided, outline your plan before executing. Clearly distinguish between tasks handled via code and those utilizing your analytical expertise.\\n\\nWhen providing code, include only Python scripts meant to be run without user alterations. Users should execute your script as is, without modifications:\\n\\n```python\\n# filename: <filename>\\n# Python script\\nprint(\\\"Your output\\\")\\n```\\n\\nUsers should not perform any actions other than running the scripts you provide. Avoid presenting partial or incomplete scripts that require user adjustments. Refrain from requesting users to copy-paste results; instead, use the 'print' function when suitable to display outputs. Monitor the execution results they share.\\n\\nIf an error surfaces, supply corrected scripts for a re-run. If the strategy fails to resolve the issue, reassess your assumptions, gather additional details as needed, and explore alternative approaches.\\n\\nUpon successful completion of a task and verification of the results, confirm the achievement of the stated objective. Ensuring accuracy and validity of the findings is paramount. Evidence supporting your conclusions should be provided when feasible.\\n\\nUpon satisfying the user's needs and ensuring all tasks are finalized, conclude your assistance with \\\"TERMINATE\\\".\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Research_analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Research Analyst, you are expected to be a proficient AI assistant possessing a strong grasp of programming, specifically in Python, and robust analytical capabilities. Your primary responsibilities will include:\\n\\n1. Conducting comprehensive searches and retrieving information autonomously through Python scripts, such as querying databases, accessing web services (like arXiv), downloading and reading files, and retrieving system information.\\n2. Analyzing the content of the retrieved documents, particularly academic papers, and extracting insights regarding their application in specific domains, such as the potential uses of GPT-4 in software development.\\n3. Presenting your findings in a clear, detailed manner, explaining the implications of the research and its relevance to the assigned task.\\n4. Employing your programming skills to automate tasks where possible, ensuring the output is delivered through Python code with clear, executable instructions. Your code will be designed for the user to execute without amendment or additional input.\\n5. Verifying the results of information gathering and analysis to ensure accuracy and completeness, providing evidence to support your conclusions when available.\\n6. Communicating the completion of each task and confirming that the user's needs have been satisfied through a clear and conclusive statement, followed by the word \\\"TERMINATE\\\" to signal the end of the interaction.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Software_developer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a dedicated AI assistant for a software developer, your role involves employing your Python programming prowess and proficiency in natural language processing to facilitate the discovery and analysis of scholarly articles on arXiv. Your tasks include crafting Python scripts to automatically search, retrieve, and present information regarding the latest research, with a focus on applicable advancements in technology such as GPT-4 and its potential impact on the domain of software development.\\n\\n1. Utilize Python to programmatically seek out and extract pertinent data, for example, navigating or probing the web, downloading/ingesting documents, or showcasing content from web pages or files. When enough information has been accumulated to proceed, you will then analyze and interpret the findings.\\n\\n2. When there's a need to perform an operation programmatically, your Python code should accomplish the task and manifest the outcome. Progress through the task incrementally and systematically.\\n\\nProvide a clear plan outlining each stage of the task, specifying which components will be executed through Python coding and which through your linguistic capabilities. When proposing Python code, remember to:\\n\\n- Label the script type within the code block\\n- Avoid suggesting code that the user would need to alter\\n- Refrain from including more than one code block in your response\\n- Circumvent requesting the user to manually transcribe any results; utilize 'print' statements where applicable\\n- Examine the user's reported execution outcomes\\n\\nIf an error arises, your responsibility is to rectify the issue and submit the corrected script. Should an error remain unresolvable, or if the task remains incomplete post successful code execution, re-evaluate the scenario, gather any further required information, and formulate an alternative approach.\\n\\nUpon confirming that the task has been satisfactorily accomplished and the user's requirements have been met, indicate closure of the procedure with a concluding statement.\"\n",
    "        }\n",
    "    ],\n",
    "    \"manager_system_message\": \"Group chat manager.\",\n",
    "    \"coding\": true,\n",
    "    \"default_llm_config\": {\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "These information will be saved in JSON format. You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with a generated filename 'save_config_TASK_MD5.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b88a5d482ceba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:41:44.198099300Z",
     "start_time": "2023-11-25T19:41:44.194085500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building config saved to ./save_config_eb1be857faa608aeb4c5af11fe4ab245.json\n"
     ]
    }
   ],
   "source": [
    "saved_path = builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35620c10ee42be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, you can load the saved config and skip the building process. AutoBuilder will create agents with those information without prompting the builder manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34addd498e5ab174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:41:48.044519600Z",
     "start_time": "2023-11-25T19:41:45.661666200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mSoftware_developer\u001B[0m (to chat_manager):\n",
      "\n",
      "To accomplish this task, we will follow these steps:\n",
      "\n",
      "1. **Search arXiv for the latest papers on GPT-4**: We will use the `arxiv` Python library to search for the most recent papers related to GPT-4. If the `arxiv` library is not installed, you will need to install it using `pip install arxiv`.\n",
      "\n",
      "2. **Extract Information**: From the search results, we will extract the title, authors, summary, and publication date of the latest paper.\n",
      "\n",
      "3. **Analyze for Potential Applications in Software Development**: We will read through the summary to identify mentions of potential applications in software development.\n",
      "\n",
      "4. **Present Findings**: We will print out the relevant information.\n",
      "\n",
      "Let's start with step 1. Here is the Python code to search for the latest papers on GPT-4 on arXiv:\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "\n",
      "# Define the search query and parameters\n",
      "search_query = 'all:\"GPT-4\"'\n",
      "sort_by = arxiv.SortCriterion.SubmittedDate\n",
      "sort_order = arxiv.SortOrder.Descending\n",
      "\n",
      "# Perform the search\n",
      "search = arxiv.Search(\n",
      "  query=search_query,\n",
      "  max_results=1,\n",
      "  sort_by=sort_by,\n",
      "  sort_order=sort_order\n",
      ")\n",
      "\n",
      "# Fetch the result\n",
      "for result in search.results():\n",
      "    print(\"Title:\", result.title)\n",
      "    print(\"Authors:\", \", \".join(author.name for author in result.authors))\n",
      "    print(\"Abstract:\", result.summary)\n",
      "    print(\"Publication Date:\", result.published)\n",
      "    print(\"URL:\", result.entry_id)\n",
      "    break  # Since we only want the latest paper, we break after the first result\n",
      "```\n",
      "\n",
      "Please run this code in your Python environment to retrieve the latest paper on GPT-4 from arXiv. Once you have the paper, we can proceed to analyze its content for potential applications in software development.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs\n",
      "Authors: Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li\n",
      "Abstract: In the field of document understanding, significant advances have been made\n",
      "in the fine-tuning of Multimodal Large Language Models (MLLMs) with\n",
      "instruction-following data. Nevertheless, the potential of text-grounding\n",
      "capability within text-rich scenarios remains underexplored. In this paper, we\n",
      "present a text-grounding document understanding model, termed TGDoc, which\n",
      "addresses this deficiency by enhancing MLLMs with the ability to discern the\n",
      "spatial positioning of text within images. Empirical evidence suggests that\n",
      "text-grounding improves the model's interpretation of textual content, thereby\n",
      "elevating its proficiency in comprehending text-rich images. Specifically, we\n",
      "compile a dataset containing 99K PowerPoint presentations sourced from the\n",
      "internet. We formulate instruction tuning tasks including text detection,\n",
      "recognition, and spotting to facilitate the cohesive alignment between the\n",
      "visual encoder and large language model. Moreover, we curate a collection of\n",
      "text-rich images and prompt the text-only GPT-4 to generate 12K high-quality\n",
      "conversations, featuring textual locations within text-rich scenarios. By\n",
      "integrating text location data into the instructions, TGDoc is adept at\n",
      "discerning text locations during the visual question process. Extensive\n",
      "experiments demonstrate that our method achieves state-of-the-art performance\n",
      "across multiple text-rich benchmarks, validating the effectiveness of our\n",
      "method.\n",
      "Publication Date: 2023-11-22 06:46:37+00:00\n",
      "URL: http://arxiv.org/abs/2311.13194v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mResearch_analyst\u001B[0m (to chat_manager):\n",
      "\n",
      "Based on the information retrieved from arXiv, the latest paper related to GPT-4 is titled \"Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs\" by Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and Houqiang Li. The paper was published on November 22, 2023.\n",
      "\n",
      "**Abstract Summary and Potential Applications in Software Development:**\n",
      "The paper discusses the enhancement of Multimodal Large Language Models (MLLMs) with text-grounding capabilities to improve document understanding. The authors present a model called TGDoc, which is designed to discern the spatial positioning of text within images. This is particularly useful for understanding text-rich images, such as PowerPoint presentations.\n",
      "\n",
      "The potential applications in software development could include:\n",
      "\n",
      "1. **Document Management Systems**: The TGDoc model could be integrated into document management systems to improve the searchability and categorization of documents by understanding the spatial layout of text within images.\n",
      "\n",
      "2. **Content Creation Tools**: Software tools for creating presentations or other visual content could benefit from TGDoc by providing suggestions or automations based on the textual content within the images.\n",
      "\n",
      "3. **Accessibility Features**: The ability to understand text within images could enhance accessibility features in software, allowing for better text-to-speech conversion for visually impaired users.\n",
      "\n",
      "4. **Data Extraction and Analysis**: TGDoc could be used in data extraction tools to pull out relevant information from text-rich images for analysis or reporting purposes.\n",
      "\n",
      "5. **Enhanced User Interfaces**: User interfaces that can interpret the spatial layout of text could provide more intuitive interactions, especially in applications that deal with document editing or creation.\n",
      "\n",
      "6. **Automated Customer Support**: By understanding text within images, customer support bots could provide more accurate responses to user inquiries that involve screenshots or other visual aids.\n",
      "\n",
      "The paper's approach to integrating text location data into instructions for better comprehension of text-rich images could be a significant step forward in the development of more intelligent and context-aware software applications.\n",
      "\n",
      "**Publication Details:**\n",
      "- **Title**: Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs\n",
      "- **Authors**: Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li\n",
      "- **Abstract URL**: [http://arxiv.org/abs/2311.13194v1](http://arxiv.org/abs/2311.13194v1)\n",
      "- **Publication Date**: November 22, 2023\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mResearch_analyst\u001B[0m (to chat_manager):\n",
      "\n",
      "It seems there might be some confusion or an issue with the user console. If you have any further questions or need assistance with another task, please let me know, and I'll be happy to help. If you're done, no further action is needed. Thank you!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path).load(saved_path)\n",
    "new_builder.start(task=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0cf8f09eef5cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use GPTs\n",
    "\n",
    "[GPTs](https://openai.com/blog/introducing-gpts) allow user to create an assistant with a simple instruction of the task. It has plugin support that can let ChatGPT complete some complex instructions, and can optionally update the assistant's instruction to let it adapted to new task or improve on the current task.\n",
    "AutoBuild also support GPTs api by adding `use_gpts=True` to the `build()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4051c25b2cd1918c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:43:09.367458800Z",
     "start_time": "2023-11-25T19:42:15.653608Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "assistant_id was None, creating a new assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating agents...\n",
      "Data_scientist,Research_analyst,Software_developer are generated.\n",
      "Preparing configuration for Data_scientist...\n",
      "Preparing configuration for Research_analyst...\n",
      "Preparing configuration for Software_developer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "assistant_id was None, creating a new assistant\n",
      "assistant_id was None, creating a new assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mSoftware_developer\u001B[0m (to chat_manager):\n",
      "\n",
      "To accomplish the task of finding the latest paper about GPT-4 on arXiv and identifying its potential applications in software development, we will break down the process into the following steps:\n",
      "\n",
      "1. **Search for papers on arXiv**: We'll use the arXiv API to search for recent papers mentioning GPT-4. The arXiv API allows us to programmatically query their database.\n",
      "\n",
      "2. **Retrieve and analyze the paper**: Once we have identified relevant papers, we will download the metadata of the latest paper. We will inspect the abstract and other available metadata for mentions of potential applications in software development.\n",
      "\n",
      "3. **Present findings**: Finally, we will present a summary of our findings.\n",
      "\n",
      "Here is a Python script that uses the arXiv API to search for the latest paper mentioning GPT-4:\n",
      "\n",
      "```python\n",
      "import urllib\n",
      "import urllib.request\n",
      "import feedparser\n",
      "\n",
      "# Define a function to search the arXiv API\n",
      "def search_arxiv(query):\n",
      "    # Base URL for the arXiv API\n",
      "    base_url = 'http://export.arxiv.org/api/query?'\n",
      "    \n",
      "    # Parameters for the API query\n",
      "    params = {\n",
      "        'search_query': query,\n",
      "        'sortBy': 'submittedDate',\n",
      "        'sortOrder': 'descending'\n",
      "    }\n",
      "    \n",
      "    # Encode the parameters and make a GET request to the arXiv API\n",
      "    query_string = urllib.parse.urlencode(params)\n",
      "    with urllib.request.urlopen(base_url + query_string) as response:\n",
      "        response_text = response.read()\n",
      "    \n",
      "    # Parse the response using feedparser\n",
      "    feed = feedparser.parse(response_text)\n",
      "    return feed.entries\n",
      "\n",
      "# Search for papers on GPT-4\n",
      "papers = search_arxiv('all:\"GPT-4\"')\n",
      "\n",
      "if papers:\n",
      "    # Get the latest paper\n",
      "    latest_paper = papers[0]\n",
      "    \n",
      "    # Check for discussions of software development applications in the paper's summary\n",
      "    summary_lower = latest_paper.summary.lower()\n",
      "    software_related_terms = ['software', 'developing', 'programming', 'coding']\n",
      "    software_applications = any(term in summary_lower for term in software_related_terms)\n",
      "    \n",
      "    # Print out the details of the latest paper\n",
      "    print(f\"Title: {latest_paper.title}\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in latest_paper.authors)}\")\n",
      "    print(f\"Abstract: {latest_paper.summary}\")\n",
      "    print(f\"Published: {latest_paper.published}\")\n",
      "    print(f\"Link: {latest_paper.link}\")\n",
      "    \n",
      "    # Print out whether software applications were mentioned\n",
      "    if software_applications:\n",
      "        print(\"\\nThe paper discusses potential applications in software development.\")\n",
      "    else:\n",
      "        print(\"\\nThe paper does not specifically discuss software development applications.\")\n",
      "else:\n",
      "    print(\"No papers found on GPT-4.\")\n",
      "\n",
      "```\n",
      "\n",
      "Please run this script to search for and analyze the latest papers on GPT-4. The script will print out information about the most recent paper on this topic and if it finds mention of applications in software development in the abstract. If any errors occur or if the script does not produce the expected outcome, please let me know, and I will provide assistance.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Towards Improving Document Understanding: An Exploration on\n",
      "  Text-Grounding via MLLMs\n",
      "Authors: Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li\n",
      "Abstract: In the field of document understanding, significant advances have been made\n",
      "in the fine-tuning of Multimodal Large Language Models (MLLMs) with\n",
      "instruction-following data. Nevertheless, the potential of text-grounding\n",
      "capability within text-rich scenarios remains underexplored. In this paper, we\n",
      "present a text-grounding document understanding model, termed TGDoc, which\n",
      "addresses this deficiency by enhancing MLLMs with the ability to discern the\n",
      "spatial positioning of text within images. Empirical evidence suggests that\n",
      "text-grounding improves the model's interpretation of textual content, thereby\n",
      "elevating its proficiency in comprehending text-rich images. Specifically, we\n",
      "compile a dataset containing 99K PowerPoint presentations sourced from the\n",
      "internet. We formulate instruction tuning tasks including text detection,\n",
      "recognition, and spotting to facilitate the cohesive alignment between the\n",
      "visual encoder and large language model. Moreover, we curate a collection of\n",
      "text-rich images and prompt the text-only GPT-4 to generate 12K high-quality\n",
      "conversations, featuring textual locations within text-rich scenarios. By\n",
      "integrating text location data into the instructions, TGDoc is adept at\n",
      "discerning text locations during the visual question process. Extensive\n",
      "experiments demonstrate that our method achieves state-of-the-art performance\n",
      "across multiple text-rich benchmarks, validating the effectiveness of our\n",
      "method.\n",
      "Published: 2023-11-22T06:46:37Z\n",
      "Link: http://arxiv.org/abs/2311.13194v1\n",
      "\n",
      "The paper does not specifically discuss software development applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mResearch_analyst\u001B[0m (to chat_manager):\n",
      "\n",
      "It appears that the script successfully retrieved information about the latest paper related to GPT-4:\n",
      "\n",
      "- **Title**: Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs\n",
      "- **Authors**: Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li\n",
      "- **Abstract**: The paper presents a text-grounding document understanding model, termed TGDoc, which enhances Multimodal Large Language Models (MLLMs) with the ability to discern the spatial positioning of text within images. While the paper involves text-only GPT-4 to generate high-quality conversations within text-rich scenarios, it is focused on document understanding and text-grounding, not directly on software development applications.\n",
      "- **Published Date**: 2023-11-22\n",
      "- **Link**: [Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs](http://arxiv.org/abs/2311.13194v1)\n",
      "  \n",
      "Based on the abstract, the paper does not specifically discuss applications in software development. However, there might be indirect applications or implications that are not detailed in the abstract, and further reading of the full paper would be necessary to uncover those.\n",
      "\n",
      "Since the provided abstract does not directly mention applications in software development, I am unable to extract insights regarding such applications without further analysis of the full document, which goes beyond the scope of this interaction.\n",
      "\n",
      "If needed, I can assist with the next steps or provide additional services within the constraints given.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mUser_console_and_Python_code_interpreter\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path)\n",
    "new_builder.build(building_task, default_llm_config, use_gpts=True)  # Transfer to GPTs API.\n",
    "new_builder.start(task=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57194293da4ae4e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
