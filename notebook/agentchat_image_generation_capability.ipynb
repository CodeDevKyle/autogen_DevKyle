{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bbe66b-3784-4da2-a5ca-5406a4d0c843",
   "metadata": {},
   "source": [
    "# Adding Different LLM Modalities to Exisiting Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1742c7a-2db4-49a0-bc6a-0500a4fa6af3",
   "metadata": {},
   "source": [
    "### This notebook showcases how to add image generation modality as a conversable agent capability "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017e450-b4da-4bd2-b4ff-91bccd664284",
   "metadata": {},
   "source": [
    "First lets import all the required modules to run this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3951ffc-ab4e-4bf6-bc36-f0d7f84c5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL.Image import Image\n",
    "\n",
    "import autogen\n",
    "from autogen.agentchat import groupchat\n",
    "from autogen.agentchat.contrib import img_utils\n",
    "from autogen.agentchat.contrib.capabilities import generate_images\n",
    "from autogen.oai import openai_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296c6bb-25c5-4619-beaa-ced8bc3f2378",
   "metadata": {},
   "source": [
    "Lets define our config list with the models we want to experiment with. For this example, we wil be using Dalle image generator to interact with the GPT4V agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7725a82-5fbe-47e0-9f10-54030b90f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "OAI_CONFIG_LIST = [\n",
    "    {\"model\": \"dall-e-3\", \"api_key\": os.environ.get(\"OAI_API_KEY\")},\n",
    "    {\"model\": \"gpt-4-vision-preview\", \"api_key\": os.environ.get(\"OAI_API_KEY\")},\n",
    "    {\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ.get(\"OAI_API_KEY\")},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c6db3-fb95-4ff4-803b-81c105a095e2",
   "metadata": {},
   "source": [
    "Lets define our LLM configs (you can experiment with different params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "104db7e8-e30a-4012-916d-5f1622e5da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_config() -> Dict:\n",
    "    filtered_configs = openai_utils.filter_config(OAI_CONFIG_LIST, filter_dict={\"model\": [\"gpt-3.5-turbo\"]})\n",
    "\n",
    "    return {\"config_list\": filtered_configs, \"timeout\": 120, \"temperature\": 0.7}\n",
    "\n",
    "\n",
    "def gpt_v_config() -> Dict:\n",
    "    filtered_configs = openai_utils.filter_config(OAI_CONFIG_LIST, filter_dict={\"model\": [\"gpt-4-vision-preview\"]})\n",
    "\n",
    "    return {\"config_list\": filtered_configs, \"timeout\": 120, \"temperature\": 0.7, \"max_tokens\": 1000}\n",
    "\n",
    "\n",
    "def dalle_config() -> Dict:\n",
    "    filtered_configs = openai_utils.filter_config(OAI_CONFIG_LIST, filter_dict={\"model\": [\"dall-e-3\"]})\n",
    "\n",
    "    return {\"config_list\": filtered_configs, \"timeout\": 120, \"temperature\": 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cec727-c237-4205-8681-e8a792f0a242",
   "metadata": {},
   "source": [
    "Our system will consist of 3 main agents:\n",
    "1. User proxy\n",
    "2. Image generator agent\n",
    "3. Critic\n",
    "\n",
    "We will use the user proxy to define the initial prompt. The image generator agent will have to generate the images based on the prompts given to it. The critic is there to improve the output of the image generator.\n",
    "\n",
    "The image generator agent and the critic will communicate with each other through a groupchat, acting like an art studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "132e57ea-a041-4c4d-99ca-b0616505b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITIC_SYSTEM_MESSAGE = \"\"\"You need to improve the prompt of the figures you saw.\n",
    "How to create a figure that is better in terms of color, shape, text (clarity), and other things.\n",
    "Reply with the following format:\n",
    "\n",
    "CRITICS: the image needs to improve...\n",
    "PROMPT: here is the updated prompt!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427233a0-190b-4acd-80a3-c5951609b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator_agent() -> autogen.ConversableAgent:\n",
    "    agent = autogen.ConversableAgent(name=\"dalle\", llm_config=gpt_config())\n",
    "    dalle_gen = generate_images.DalleImageGenerator(llm_config=dalle_config(), cache_settings={\"directory\": \".cache/\"})\n",
    "    image_gen_capability = generate_images.ImageGeneration(image_generator=dalle_gen)\n",
    "\n",
    "    image_gen_capability.add_to_agent(agent)\n",
    "    return agent\n",
    "\n",
    "def critic_agent() -> autogen.ConversableAgent:\n",
    "    return autogen.ConversableAgent(name=\"critic\", llm_config=gpt_v_config(), system_message=CRITIC_SYSTEM_MESSAGE)\n",
    "\n",
    "def art_studio() -> groupchat.GroupChatManager:\n",
    "    gc = groupchat.GroupChat(\n",
    "        admin_name=None,\n",
    "        agents=[image_generator_agent(), critic_agent()],\n",
    "        messages=[],\n",
    "        speaker_selection_method=\"round_robin\",\n",
    "        max_round=4,\n",
    "    )\n",
    "    return groupchat.GroupChatManager(groupchat=gc, llm_config=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f65910-b811-43f9-b6a0-986b52c83d94",
   "metadata": {},
   "source": [
    "We'll define `extract_img` to help us extract the image generated by the image generator agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff882a30-0f1f-4c6a-8951-9e11df48bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img(sender: autogen.Agent, recipient: autogen.Agent) -> Optional[Image]:\n",
    "    # From notebook/agentchat_dalle_and_gpt4v.ipynb\n",
    "    \"\"\"\n",
    "    Extracts an image from the last message of an agent and converts it to a PIL image.\n",
    "\n",
    "    This function searches the last message sent by the given agent for an image tag,\n",
    "    extracts the image data, and then converts this data into a PIL (Python Imaging Library) image object.\n",
    "\n",
    "    Parameters:\n",
    "        agent (Agent): An instance of an agent from which the last message will be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: A PIL image object created from the extracted image data.\n",
    "\n",
    "    Note:\n",
    "    - The function assumes that the last message contains an <img> tag with image data.\n",
    "    - The image data is extracted using a regular expression that searches for <img> tags.\n",
    "    - It's important that the agent's last message contains properly formatted image data for successful extraction.\n",
    "    - The `_to_pil` function is used to convert the extracted image data into a PIL image.\n",
    "    - If no <img> tag is found, or if the image data is not correctly formatted, the function may raise an error.\n",
    "    \"\"\"\n",
    "    last_message = recipient.last_message(sender)[\"content\"]\n",
    "    img_data = None\n",
    "\n",
    "    if isinstance(last_message, str):\n",
    "        img_data = re.findall(\"<img (.*)>\", last_message)\n",
    "        if img_data:\n",
    "            img_data = img_data[0]\n",
    "    elif isinstance(last_message, list):\n",
    "        # The GPT-4V format, where the content is an array of data\n",
    "        assert isinstance(last_message[0], dict)\n",
    "        img_data = last_message[0].get(\"image_url\", {}).get(\"url\")\n",
    "\n",
    "    if img_data:\n",
    "        return img_utils._to_pil(img_data)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed01470b-ad59-483d-990e-772d27e0764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    user = autogen.UserProxyAgent(name=\"user\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=0)\n",
    "    manager = art_studio()\n",
    "\n",
    "    user.initiate_chat(\n",
    "        manager,\n",
    "        message=\"\"\"Create an image with black background, a happy robot is showing a sign with \"I Love AutoGen\".\"\"\",\n",
    "    )\n",
    "\n",
    "    dalle = manager.groupchat.agent_by_name(\"dalle\")\n",
    "\n",
    "    assert dalle is not None\n",
    "    image = extract_img(dalle, manager)\n",
    "    if image is not None:\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")  # Turn off axis numbers\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "943d83dc-ee69-49b9-b2b4-cfc3d7718b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to chat_manager):\n",
      "\n",
      "Create an image with black background, a happy robot is showing a sign with \"I Love AutoGen\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wael/workspaces/autogen/autogen/agentchat/user_proxy_agent.py:83: UserWarning: Using None to signal a default code_execution_config is deprecated. Use {} to use default or False to disable code execution.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mdalle\u001b[0m (to chat_manager):\n",
      "\n",
      "Generated an image with the prompt: To create the image described in the text, you would prompt the image generator with the following specifications:\n",
      "- Set the background color to black.\n",
      "- Include a happy robot character.\n",
      "- Instruct the robot to hold a sign with the text \"I Love AutoGen\" displayed on it.<image>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mcritic\u001b[0m (to chat_manager):\n",
      "\n",
      "CRITICS: the image needs to improve on the clarity of the text on the sign, the robot's expression to clearly convey happiness, and the color contrast to make the image more visually appealing.\n",
      "\n",
      "PROMPT: here is the updated prompt!\n",
      "- Choose a vibrant color for the robot to stand out against the black background.\n",
      "- Ensure the robot has a distinctly joyful expression, with features like a smile or 'sparkling' eyes.\n",
      "- The sign should have a contrasting color (e.g., white or bright color) to make the text \"I Love AutoGen\" pop and ensure it's easily readable.\n",
      "- Consider adding a subtle glow or outline to the text to enhance visibility.\n",
      "- Optionally, add some thematic decorations to the robot or background that relate to automation or technology to reinforce the message.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type PngImageFile is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m user \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mUserProxyAgent(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, human_input_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEVER\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_consecutive_auto_reply\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m manager \u001b[38;5;241m=\u001b[39m art_studio()\n\u001b[0;32m----> 5\u001b[0m \u001b[43muser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mCreate an image with black background, a happy robot is showing a sign with \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI Love AutoGen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m dalle \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mgroupchat\u001b[38;5;241m.\u001b[39magent_by_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdalle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dalle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/conversable_agent.py:928\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, **context)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m    930\u001b[0m     context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, ConversableAgent\u001b[38;5;241m.\u001b[39mDEFAULT_summary_method),\n\u001b[1;32m    931\u001b[0m     recipient,\n\u001b[1;32m    932\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcontext\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    933\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    934\u001b[0m )\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/conversable_agent.py:620\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    618\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 620\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m     )\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/conversable_agent.py:782\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/conversable_agent.py:1784\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1784\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/groupchat.py:611\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    609\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m groupchat\u001b[38;5;241m.\u001b[39mselect_speaker(speaker, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/conversable_agent.py:1784\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1784\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/conversable_agent.py:1181\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1181\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/agentchat/conversable_agent.py:1200\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1200\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1205\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/oai/client.py:584\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cache_client \u001b[38;5;28;01mas\u001b[39;00m cache:\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# Try to get the response from cache\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mget_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m         request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[1;32m    587\u001b[0m         response: ModelClient\u001b[38;5;241m.\u001b[39mModelClientResponseProtocol \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/workspaces/autogen/autogen/oai/openai_utils.py:75\u001b[0m, in \u001b[0;36mget_key\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     69\u001b[0m         config\u001b[38;5;241m.\u001b[39mpop(key)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# if isinstance(config, dict):\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#     return tuple(get_key(x) for x in sorted(config.items()))\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# if isinstance(config, list):\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#     return tuple(get_key(x) for x in config)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# return config\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.11/lib/python3.11/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.11/lib/python3.11/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.11/lib/python3.11/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.11/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type PngImageFile is not JSON serializable"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d1e4f-9843-4dc4-aaaf-dab7e84a7870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
