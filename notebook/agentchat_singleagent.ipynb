{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Agent Understanding\n",
    "\n",
    "Note book works on the Assisstant Agent, from instantiation to experimenting\n",
    "\n",
    "**Different tasks** may require different models, and the config_list aids in dynamically selecting the appropriate model configuration, managing API keys, endpoints, and versions for efficient operation of the intelligent assistant. In summary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'api_key': 'sk-888888888888888888888888888888', 'model': 'gpt-3.5-turbo'},\n",
       " {'api_key': 'sk-888888888888888888888888888888', 'model': 'gpt-4'},\n",
       " {'api_key': 'sk-00000', 'model': 'llama2'},\n",
       " {'api_key': 'sk-00000', 'model': 'codellama'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autogen\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# First need to create the config_list that can be passed to the Agents. Using the dotenv route.\n",
    "# Notebook oai_openai_utils.ipynb (https://github.com/Kamalabot/autogen/blob/main/notebook/oai_openai_utils.ipynb)\n",
    "# provides the clear intro\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../.env',\n",
    "    model_api_key_map= {\n",
    "        \"gpt-3.5-turbo\": \"OPENAI_API_KEY\",\n",
    "        \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        \"llama2\":\"HF_KEY\",\n",
    "        \"codellama\":\"HF_KEY\"\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\":{\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-4\",\n",
    "            \"llama2\",\n",
    "            \"codellama\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'api_key': 'sk-888888888888888888888888888888', 'model': 'gpt-3.5-turbo'},\n",
       " {'api_key': 'sk-00000',\n",
       "  'base_url': 'https://localhost:8888/',\n",
       "  'api_type': 'local',\n",
       "  'api_version': 'v2',\n",
       "  'model': 'codellama2'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_list01 = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path=\"../.env\",\n",
    "    model_api_key_map= {\n",
    "        \"gpt-3.5-turbo\": \"OPENAI_API_KEY\",\n",
    "        \"codellama2\":{\n",
    "            \"api_key_env_var\": \"HF_KEY\",\n",
    "            \"api_type\": \"local\",\n",
    "            \"api_version\": \"v2\",\n",
    "            \"base_url\":\"https://localhost:8888/\"\n",
    "        },\n",
    "        \"gpt-4\": \"OPENAI_API_KEY\"\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"codellama2\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "config_list01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Starting to work with the [agentchat_oai_assistant_twoagents_basic.ipynb](https://github.com/Kamalabot/autogen/blob/main/notebook/agentchat_oai_assistant_twoagents_basic.ipynb) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import os\n",
    "from autogen import AssistantAgent\n",
    "from autogen import UserProxyAgent\n",
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'api_key': 'sk-888888888888888888888888888888',\n",
       "  'model': 'gpt-3.5-turbo-1106'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_35_config = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path=\"../.env\",\n",
    "    model_api_key_map={\n",
    "        \"gpt-3.5-turbo-1106\":\"OPENAI_API_KEY\",\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\":{\n",
    "            \"gpt-3.5-turbo-1106\"       \n",
    "        }\n",
    "    }\n",
    ")\n",
    "gpt_35_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPTAssistant initiation variables\n",
    "\n",
    "name (str): name of the agent.\n",
    "            instructions (str): instructions for the OpenAI assistant configuration.\n",
    "            When instructions is not None, the system message of the agent will be\n",
    "            set to the provided instructions and used in the assistant run, irrespective\n",
    "            of the overwrite_instructions flag. But when instructions is None,\n",
    "            and the assistant does not exist, the system message will be set to\n",
    "            AssistantAgent.DEFAULT_SYSTEM_MESSAGE. If the assistant exists, the\n",
    "            system message will be set to the existing assistant instructions.\n",
    "\n",
    "llm_config (dict or False): llm inference configuration.\n",
    "\n",
    "                - assistant_id: ID of the assistant to use. If None, a new assistant will be created.\n",
    "\n",
    "                - model: Model to use for the assistant (gpt-4-1106-preview, gpt-3.5-turbo-1106).\n",
    "\n",
    "                - check_every_ms: check thread run status interval\n",
    "\n",
    "                - tools: Give Assistants access to OpenAI-hosted tools like Code Interpreter and Knowledge Retrieval,\n",
    "                        or build your own tools using Function calling. ref https://platform.openai.com/docs/assistants/tools\n",
    "\n",
    "                - file_ids: files used by retrieval in run\n",
    "\n",
    "overwrite_instructions (bool): whether to overwrite the instructions of an existing assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI assistant.\n",
      "Solve tasks using your coding and language skills.\n",
      "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
      "    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
      "    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
      "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
      "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
      "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
      "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
      "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\n",
      "Reply \"TERMINATE\" in the end when everything is done.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(AssistantAgent.DEFAULT_SYSTEM_MESSAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4'}\n"
     ]
    }
   ],
   "source": [
    "print(AssistantAgent.DEFAULT_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Learning about the assistant](https://platform.openai.com/docs/assistants/overview) from Openai\n",
    "\n",
    "* Helps to build AI assistants within your own applications.\n",
    "\n",
    "* Leverages models, tools and knowledge to respond to user queries.\n",
    "\n",
    "* Three types of tools: Code Interpreter, Retrieval, and Function calling. \n",
    "\n",
    "* Assistants can access multiple tools in parallel, access persistent threads and access files in several formats.\n",
    "\n",
    "#### Steps involved in using the Assistant\n",
    "\n",
    "1 - Creating the Assistant using the above steps\n",
    "\n",
    "2 - Create a thread inside OpenAI environment using thread = client.beta.threads.create()\n",
    "\n",
    "Threads and Messages represent a conversation session between an Assistant and a user. There is no limit to the number of Messages you can store in a Thread\n",
    "\n",
    "3 - Add message to the thread\n",
    "\n",
    "4 - Assistants can be made to respond by initiating a Run using \n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    " \n",
    "  thread_id=thread.id,\n",
    " \n",
    "  assistant_id=assistant.id,\n",
    " \n",
    "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\"\n",
    "\n",
    ")\n",
    "\n",
    "5 - Check the run status using  client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "6 - messages = client.beta.threads.messages.list(thread_id=thread.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload a file with an \"assistants\" purpose\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "\n",
    "  instructions=\"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n",
    "\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "\n",
    "  tools=[{\"type\": \"code_interpreter\"}]\n",
    "\n",
    ")\n",
    "\n",
    "file = client.files.create(\n",
    "\n",
    "  file=open(\"speech.py\", \"rb\"),\n",
    "\n",
    "  purpose='assistants'\n",
    "\n",
    ")\n",
    "\n",
    "#### Create an assistant using the file ID\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "\n",
    "  instructions=\"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n",
    "\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "\n",
    "  tools=[{\"type\": \"code_interpreter\"}],\n",
    "\n",
    "  file_ids=[file.id]\n",
    "\n",
    ")\n",
    "#### Creating Threads\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    " \n",
    "  messages=[\n",
    " \n",
    "    {\n",
    " \n",
    "      \"role\": \"user\",\n",
    " \n",
    "      \"content\": \"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n",
    " \n",
    "      \"file_ids\": [file.id]\n",
    " \n",
    "    }])\n",
    "\n",
    "#### Knowledge Retrieval \n",
    "\n",
    "Upload a file with an \"assistants\" purpose\n",
    "\n",
    "file = client.files.create(\n",
    " \n",
    "  file=open(\"knowledge.pdf\", \"rb\"),\n",
    " \n",
    "  purpose='assistants'\n",
    "\n",
    ")\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    " \n",
    "  instructions=\"You are a customer support chatbot. Use your knowledge base to best respond to customer queries.\",\n",
    " \n",
    "  model=\"gpt-4-1106-preview\",\n",
    " \n",
    "  tools=[{\"type\": \"retrieval\"}]\n",
    "\n",
    ")\n",
    "\n",
    "#### Function calling\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  \n",
    "  instructions=\"You are a weather bot. Use the provided functions to answer questions.\",\n",
    "  \n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  \n",
    "  tools=[{\n",
    "  \n",
    "      \"type\": \"function\",\n",
    "  \n",
    "    \"function\": {\n",
    "  \n",
    "      \"name\": \"getCurrentWeather\",\n",
    "  \n",
    "      \"description\": \"Get the weather in location\",\n",
    "  \n",
    "      \"parameters\": {\n",
    "  \n",
    "        \"type\": \"object\",\n",
    "  \n",
    "        \"properties\": {\n",
    "  \n",
    "          \"location\": {\"type\": \"string\", \"description\": \"The city and state e.g. San Francisco, CA\"},\n",
    "  \n",
    "          \"unit\": {\"type\": \"string\", \"enum\": [\"c\", \"f\"]}\n",
    "  \n",
    "        },\n",
    "  \n",
    "        \"required\": [\"location\"]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "asst_id = os.environ.get('ASST_ID')\n",
    "asst_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.agentchat.contrib.gpt_assistant_agent:overwrite_instructions is False. Provided instructions will be used without permanently modifying the assistant in the API.\n"
     ]
    }
   ],
   "source": [
    "# Ensure that Openai library is updated before working on the assistants\n",
    "# If the assistant id is not set, then each run of the cell creates a new assistant inside OpenAI\n",
    "# The above assistant is created inside the OpenAI backend, which can be viewed at [assistant page](https://platform.openai.com/assistants)\n",
    "llm_config = {\n",
    "    \"config_list\": gpt_35_config,\n",
    "    \"assistant_id\": asst_id,\n",
    "    \"model\":\"gpt-3.5-turbo\",\n",
    "}\n",
    "\n",
    "gpt_assistant = GPTAssistantAgent(name=\"assistant\",\n",
    "                                  instructions=AssistantAgent.DEFAULT_SYSTEM_MESSAGE,\n",
    "                                  llm_config=llm_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand UserProxy Agent Init parameters\n",
    "\n",
    "name (str): name of the agent.\n",
    "\n",
    "is_termination_msg (function): a function that takes a message in the form of a dictionary\n",
    "    and returns a boolean value indicating if this received message is a termination message.\n",
    "    The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
    "\n",
    "max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.\n",
    "    default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
    "    The limit only plays a role when human_input_mode is not \"ALWAYS\".\n",
    "\n",
    "human_input_mode (str): whether to ask for human inputs every time a message is received.\n",
    "    Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
    "\n",
    "    (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
    "        Under this mode, the conversation stops when the human input is \"exit\",\n",
    "        or when is_termination_msg is True and there is no human input.\n",
    "\n",
    "    (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
    "        the number of auto reply reaches the max_consecutive_auto_reply.\n",
    "\n",
    "    (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
    "        when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
    "\n",
    "function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions.\n",
    "\n",
    "code_execution_config (dict or False): config for the code execution.\n",
    "    To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n",
    "\n",
    "    - work_dir (Optional, str): The working directory for the code execution.\n",
    "        If None, a default working directory will be used.\n",
    "        The default working directory is the \"extensions\" directory under\n",
    "        \"path_to_autogen\".\n",
    "\n",
    "    - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n",
    "        If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
    "        with the first image successfully pulled.\n",
    "        If None, False or empty, the code will be executed in the current environment.\n",
    "        Default is True, which will be converted into a list.\n",
    "        If the code is executed in the current environment,\n",
    "        the code must be trusted.\n",
    "\n",
    "    - timeout (Optional, int): The maximum execution time in seconds.\n",
    "\n",
    "    - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.\n",
    "\n",
    "default_auto_reply (str or dict or None): the default auto reply message when no code execution or llm based reply is generated.\n",
    "\n",
    "llm_config (dict or False): llm inference configuration.\n",
    "    Please refer to [OpenAIWrapper.create](/docs/reference/oai/client#create)\n",
    "    for available options.\n",
    "    Default to false, which disables llm-based auto reply.\n",
    "\n",
    "system_message (str): system message for ChatCompletion inference.\n",
    "    Only used when llm_config is not False. Use it to reprogram the agent.Args:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTAssistant has been understood, now move to UserProxyAgent\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    code_execution_config={\n",
    "        \"work_dir\":\"auto_code\"\n",
    "    },\n",
    "    is_termination_msg= lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode='NEVER',\n",
    "    max_consecutive_auto_reply=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Say hello to the world\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To say hello to the world, you can simply print the string \"Hello, world!\" in Python. Here's the code:\n",
      "\n",
      "```python\n",
      "print(\"Hello, world!\")\n",
      "```\n",
      "\n",
      "Run this code and it will output \"Hello, world!\" to the console.\n",
      "\n",
      "\n",
      "```python\n",
      "print(\"Hello, world!\")\n",
      "```\n",
      "\n",
      "This code will print \"Hello, world!\" to the console.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.code_utils:execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n",
      "WARNING:autogen.code_utils:SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.code_utils:execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n",
      "WARNING:autogen.code_utils:SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Hello, world!\n",
      "\n",
      "Hello, world!\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The code executed successfully and printed \"Hello, world!\" twice. The output suggests that the code was run twice. Please make sure to run the code only once to avoid duplicate outputs. If the issue persists, let me know and I'll be happy to assist you further.\n",
      "\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(gpt_assistant, message=\"Say hello to the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Write a python code to eval 2 * 20\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Apologies for the error. Please use the following Python code to evaluate the expression \"2 * 20\":\n",
      "\n",
      "```python\n",
      "result = 2 * 20\n",
      "print(result)\n",
      "```\n",
      "\n",
      "This code will compute the expression `2 * 20` and print the result, which should be `40`.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.code_utils:execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n",
      "WARNING:autogen.code_utils:SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "40\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The code executed successfully and the output is `40`, which is the result of evaluating `2 * 20`. This means that the expression `2 * 20` is equal to 40. If you have any more questions, feel free to ask.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(gpt_assistant, message=\"Write a python code to eval 2 * 20\", clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asst_iHWfEf9BtNKEzC6Z0jH8AVwo'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check what has happenend inside OpenAI\n",
    "\n",
    "gpt_assistant.assistant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.oai.client.OpenAIWrapper at 0x295ac446050>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_assistant.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<autogen.agentchat.user_proxy_agent.UserProxyAgent at 0x295ae42da90>: Thread(id='thread_D0EaOAFCELSV626U6H11guG5', created_at=1700132022, metadata={}, object='thread')}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_assistant._openai_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {<autogen.agentchat.user_proxy_agent.UserProxyAgent at 0x295ae42da90>: [{'content': 'Say hello to the world',\n",
       "               'role': 'user'},\n",
       "              {'content': 'To say hello to the world, you can simply print the string \"Hello, world!\" in Python. Here\\'s the code:\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nRun this code and it will output \"Hello, world!\" to the console.\\n\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nThis code will print \"Hello, world!\" to the console.\\n',\n",
       "               'role': 'assistant'},\n",
       "              {'content': 'exitcode: 0 (execution succeeded)\\nCode output: \\nHello, world!\\n\\nHello, world!\\n',\n",
       "               'role': 'user'},\n",
       "              {'content': 'Great! The code executed successfully and printed \"Hello, world!\" twice. The output suggests that the code was run twice. Please make sure to run the code only once to avoid duplicate outputs. If the issue persists, let me know and I\\'ll be happy to assist you further.\\n\\n\\nTERMINATE\\n',\n",
       "               'role': 'assistant'},\n",
       "              {'content': 'Write a python code to eval 2 * 20',\n",
       "               'role': 'user'},\n",
       "              {'content': 'Apologies for the error. Please use the following Python code to evaluate the expression \"2 * 20\":\\n\\n```python\\nresult = 2 * 20\\nprint(result)\\n```\\n\\nThis code will compute the expression `2 * 20` and print the result, which should be `40`.\\n',\n",
       "               'role': 'assistant'},\n",
       "              {'content': 'exitcode: 0 (execution succeeded)\\nCode output: \\n40\\n',\n",
       "               'role': 'user'},\n",
       "              {'content': 'Great! The code executed successfully and the output is `40`, which is the result of evaluating `2 * 20`. This means that the expression `2 * 20` is equal to 40. If you have any more questions, feel free to ask.\\n',\n",
       "               'role': 'assistant'}]})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_assistant._oai_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "open_ai_key = os.environ.get('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=open_ai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[Assistant](data=[Assistant(id='asst_iHWfEf9BtNKEzC6Z0jH8AVwo', created_at=1700130823, description=None, file_ids=[], instructions='You are a helpful AI assistant.\\nSolve tasks using your coding and language skills.\\nIn the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\\n    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\\n    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\\nSolve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\\nWhen using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can\\'t modify your code. So do not suggest incomplete code which requires users to modify. Don\\'t use a code block if it\\'s not intended to be executed by the user.\\nIf you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don\\'t include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use \\'print\\' function for the output when relevant. Check the execution result returned by the user.\\nIf the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can\\'t be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\\nWhen you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\\nReply \"TERMINATE\" in the end when everything is done.\\n    ', metadata={}, model='gpt-3.5-turbo', name='assistant', object='assistant', tools=[])], object='list', first_id='asst_iHWfEf9BtNKEzC6Z0jH8AVwo', last_id='asst_iHWfEf9BtNKEzC6Z0jH8AVwo', has_more=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the assistants\n",
    "client.beta.assistants.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assitantused = client.beta.assistants.retrieve(assistant_id=asst_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assitantused.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI assistant.\n",
      "Solve tasks using your coding and language skills.\n",
      "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
      "    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
      "    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
      "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
      "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
      "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
      "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
      "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\n",
      "Reply \"TERMINATE\" in the end when everything is done.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(assitantused.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.beta.threads.retrieve(thread_id=\"thread_D0EaOAFCELSV626U6H11guG5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = client.beta.threads.runs.list(thread_id=\"thread_D0EaOAFCELSV626U6H11guG5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_data = runs.data\n",
    "runs_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
