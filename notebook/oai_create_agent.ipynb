{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple Agent Chat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1004af6a7fbfcd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement\n",
    "\n",
    "Auto Agent setup need `vllm>=0.2.0` and `fastchat>=0.2.30`, you can use the following command to install the latest version of [vLLM](https://github.com/vllm-project/vllm) and [FastChat](https://github.com/lm-sys/FastChat).\n",
    "```\n",
    "pip install vllm, \"fastchat[model_worker]\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec78dda8e3826d8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install vllm \"fastchat[model_worker]\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e9ae50658be975"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import autogen\n",
    "import time\n",
    "import socket\n",
    "import subprocess as sp\n",
    "from typing import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T06:18:25.637679500Z",
     "start_time": "2023-10-31T06:18:25.613603400Z"
    }
   },
   "id": "8d06a99bd05026bb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class AgentCreator:\n",
    "    \"\"\"\n",
    "    Descriptions\n",
    "    \"\"\"\n",
    "    host: str\n",
    "    task: str\n",
    "    config_path: str\n",
    "    open_ports: List[str] = []\n",
    "    agent_procs: Dict[str, Tuple[sp.Popen, str]] = {}\n",
    "    openai_server_name: str = 'openai'\n",
    "    endpoint_building_timeout: Optional[int]\n",
    "    agent_procs_assign: Dict[str, Tuple[autogen.AssistantAgent, str]] = {}\n",
    "    max_tokens: int = 945\n",
    "    \n",
    "    user_proxy: autogen.UserProxyAgent = None\n",
    "    group_chat_manager_config: dict = None\n",
    "    manager_system_message: str = 'Group chat manager.'\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            task: str,\n",
    "            host: str = 'localhost',\n",
    "            config_path: str = 'OAI_CONFIG_LIST',\n",
    "            endpoint_building_timeout: Optional[int] = 180\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task: description of a task.\n",
    "            endpoint_building_timeout: timeout for building up an endpoint server.\n",
    "            config_path: path of the OpenAI api configs.\n",
    "            host: endpoint host.\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.endpoint_building_timeout = endpoint_building_timeout\n",
    "        self.config_path = config_path\n",
    "        self.host = host\n",
    "\n",
    "        print('Initializing usable port...')\n",
    "        for port in range(8000, 65535):\n",
    "            if self._is_port_open(host, port):\n",
    "                self.open_ports.append(str(port))\n",
    "        print(f'{len(self.open_ports)} ports found.')\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_port_open(host, port):\n",
    "        \"\"\"Check if a port is open.\"\"\"\n",
    "        try:\n",
    "            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            s.settimeout(10)\n",
    "            s.bind((host, int(port)))\n",
    "            s.close()\n",
    "            return True\n",
    "        except OSError:\n",
    "            return False\n",
    "        \n",
    "    def set_task(self, task: str):\n",
    "        self.task = task\n",
    "\n",
    "    def create_agent(\n",
    "            self,\n",
    "            agent_name: str,\n",
    "            model_name_or_hf_repo: str,\n",
    "            llm_config: dict,\n",
    "            system_message: Optional[str] = autogen.AssistantAgent.DEFAULT_SYSTEM_MESSAGE,\n",
    "            world_size: Optional[int] = 1\n",
    "    ) -> autogen.AssistantAgent:\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name that identify the function of the agent (e.g., Coder, Product Manager,...)\n",
    "            model_name_or_hf_repo:\n",
    "            llm_config: specific configs for LLM (e.g., config_list, seed, temperature, ...).\n",
    "            system_message: system prompt use to format an agent's behavior.\n",
    "            world_size: the max size of parallel tensors (in most of the cases, this is identical to the amount of GPUs).\n",
    "\n",
    "        Returns:\n",
    "            agent: a set-up agent.\n",
    "        \"\"\"\n",
    "        config_list = autogen.config_list_from_json(\n",
    "            self.config_path,\n",
    "            filter_dict={\n",
    "                'model': [model_name_or_hf_repo]\n",
    "            }\n",
    "        )\n",
    "        if 'gpt-' in model_name_or_hf_repo:\n",
    "            server_id = self.openai_server_name\n",
    "        else:\n",
    "            model_name = model_name_or_hf_repo.split('/')[-1]\n",
    "            server_id = f'{model_name}_{self.host}'\n",
    "            if self.agent_procs.get(server_id, None) is None:\n",
    "                while True:\n",
    "                    port = self.open_ports.pop()\n",
    "                    if self._is_port_open(self.host, port):\n",
    "                        break\n",
    "\n",
    "                # Use vLLM to set up a server with OpenAI API support.\n",
    "                agent_proc = sp.Popen(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                                       f'--host', f'{self.host}',\n",
    "                                       f'--port', f'{port}',\n",
    "                                       f'--model', f'{model_name_or_hf_repo}',\n",
    "                                       f'--tensor-parallel-size', f'{world_size}'], stdout=sp.PIPE, stderr=sp.STDOUT)\n",
    "                timeout_start = time.time()\n",
    "\n",
    "                while True:\n",
    "                    server_stdout = agent_proc.stdout.readline()\n",
    "                    if server_stdout != b'':\n",
    "                        print(server_stdout)\n",
    "                    timeout_end = time.time()\n",
    "                    if b\"running\" in server_stdout:\n",
    "                        print(\n",
    "                            f'Running {model_name_or_hf_repo} on http://{self.host}:{port} '\n",
    "                            f'with tensor parallel size {world_size}.')\n",
    "                        break\n",
    "                    elif b\"address already in use\" in server_stdout:\n",
    "                        raise RuntimeError(f'{self.host}:{port} already in use. Fail to set up the endpoint for '\n",
    "                                           f'{model_name_or_hf_repo} on {self.host}:{port}.')\n",
    "                    elif timeout_end - timeout_start > self.endpoint_building_timeout:\n",
    "                        raise RuntimeError(f'Timeout exceed. Fail to set up the endpoint for '\n",
    "                                           f'{model_name_or_hf_repo} on {self.host}:{port}.')\n",
    "                self.agent_procs[server_id] = (agent_proc, port)\n",
    "            else:\n",
    "                port = self.agent_procs[server_id][1]\n",
    "\n",
    "            config_list[0]['api_base'] = f'http://{self.host}:{port}/v1'\n",
    "\n",
    "        current_config = llm_config.copy()\n",
    "        current_config.update({\n",
    "            'config_list': config_list,\n",
    "            'model': model_name_or_hf_repo,\n",
    "            'max_tokens': self.max_tokens\n",
    "        })\n",
    "        agent = autogen.AssistantAgent(name=agent_name,\n",
    "                                       llm_config=current_config.copy(),\n",
    "                                       system_message=system_message)\n",
    "        self.agent_procs_assign[agent_name] = (agent, server_id)\n",
    "        return agent\n",
    "\n",
    "    def clear_agent(\n",
    "            self,\n",
    "            agent_name: str = None,\n",
    "            recycle_endpoint: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name of agent.\n",
    "            recycle_endpoint: trigger for recycle the endpoint server. If true, the endpoint will be recycled\n",
    "                when there is no agent depending on.\n",
    "        \"\"\"\n",
    "        _, server_id = self.agent_procs_assign[agent_name]\n",
    "        del self.agent_procs_assign[agent_name]\n",
    "        if recycle_endpoint:\n",
    "            if server_id == self.openai_server_name:\n",
    "                return\n",
    "            else:\n",
    "                for _, iter_sid in self.agent_procs_assign.values():\n",
    "                    if server_id == iter_sid:\n",
    "                        return\n",
    "                self.agent_procs[server_id][0].terminate()\n",
    "                self.open_ports.append(server_id.split('_')[-1])\n",
    "\n",
    "    def clear_all(self):\n",
    "        \"\"\"\n",
    "        Clear all cached agents.\n",
    "        \"\"\"\n",
    "        for agent_name in [agent_name for agent_name in self.agent_procs_assign.keys()]:\n",
    "            self.clear_agent(agent_name)\n",
    "\n",
    "    def build(\n",
    "            self,\n",
    "            default_llm_config: dict,\n",
    "            code: bool = None\n",
    "    ):\n",
    "        # TODO: not completed.\n",
    "        # build endpoint; create agent; get config (manager); initialize user proxy; task start; clear agent\n",
    "        \n",
    "        self.create_agent('Coder_gpt-35', 'gpt-3.5-turbo', default_llm_config)\n",
    "        self.create_agent('Product_manager_gpt-35', 'gpt-3.5-turbo', default_llm_config)\n",
    "        \n",
    "        self.user_proxy = autogen.UserProxyAgent(\n",
    "            name=\"User_proxy\",\n",
    "            system_message=\"A human admin.\",\n",
    "            code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "            human_input_mode=\"TERMINATE\"\n",
    "        )\n",
    "        \n",
    "        config_list = autogen.config_list_from_json(\n",
    "            self.config_path,\n",
    "            filter_dict={\n",
    "                'model': ['gpt-4']\n",
    "            }\n",
    "        )\n",
    "        self.group_chat_manager_config = default_llm_config.copy()\n",
    "        self.group_chat_manager_config['config_list'] = config_list\n",
    "        self.manager_system_message = 'Group chat manager.'\n",
    "        \n",
    "\n",
    "    def start(\n",
    "            self,\n",
    "            max_round: Optional[int] = 12,\n",
    "            init_messages: Optional[List[dict]] = [],\n",
    "            initiate_agent_name: str = 'user'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            max_round: the maximum number of rounds.\n",
    "            init_messages: input messages before the task start. This can be the chat history from other group chat\n",
    "                or some preliminary of the task.\n",
    "            initiate_agent_name: the name of an agent use to initialize the group chat.\n",
    "        \"\"\"\n",
    "        agent_list = [agent for agent, _ in self.agent_procs_assign.values()]\n",
    "        if self.user_proxy is not None:\n",
    "            agent_list.append(self.user_proxy)\n",
    "        group_chat = autogen.GroupChat(agents=agent_list, messages=init_messages, max_round=max_round)\n",
    "\n",
    "        manager = autogen.GroupChatManager(groupchat=group_chat,\n",
    "                                           llm_config=self.group_chat_manager_config,\n",
    "                                           system_message=self.manager_system_message)\n",
    "\n",
    "        if initiate_agent_name == \"user\" and self.user_proxy is not None:\n",
    "            self.user_proxy.initiate_chat(manager, message=self.task)\n",
    "        else:\n",
    "            for agent in agent_list:\n",
    "                if initiate_agent_name == agent.name():\n",
    "                    agent.initiate_chat(manager, message=self.task)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T06:18:27.355162100Z",
     "start_time": "2023-10-31T06:18:27.310913400Z"
    }
   },
   "id": "d729233c9524f875"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test (GPT)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d4b09d29b740bf7"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing usable port...\n",
      "57522 ports found.\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-35\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to search for papers related to GPT-4. Then, we can analyze the abstract or content of the paper to identify its potential applications in software.\n",
      "\n",
      "Here is the Python code to accomplish this task:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "xml_data = response.text\n",
      "paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "paper_abstract = xml_data.split(\"<summary>\")[1].split(\"</summary>\")[0]\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Abstract:\", paper_abstract)\n",
      "```\n",
      "\n",
      "Please note that the above code uses the `requests` library to make an HTTP request to the arXiv API and parse the XML response. Make sure you have the `requests` library installed (`pip install requests`) before running the code.\n",
      "\n",
      "After executing the code, it will print the title and abstract of the latest paper on GPT-4. You can then analyze the abstract to identify potential applications in software.\n",
      "\n",
      "Please let me know if you need any further assistance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 10, in <module>\n",
      "    paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-35\u001B[0m (to chat_manager):\n",
      "\n",
      "I apologize for the error. It seems that the code failed to extract the paper details from the XML response. This could be due to changes in the XML structure or the absence of the expected tags.\n",
      "\n",
      "To resolve this issue, we can use the `feedparser` library to parse the XML response and extract the paper details. Here's an updated version of the code:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import feedparser\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response using feedparser\n",
      "feed = feedparser.parse(response.text)\n",
      "\n",
      "# Extract the paper details\n",
      "if len(feed.entries) > 0:\n",
      "    paper_title = feed.entries[0].title\n",
      "    paper_abstract = feed.entries[0].summary\n",
      "    paper_link = feed.entries[0].link\n",
      "    paper_published = feed.entries[0].published\n",
      "\n",
      "    # Print the paper details\n",
      "    print(\"Latest Paper on GPT-4:\")\n",
      "    print(\"Title:\", paper_title)\n",
      "    print(\"Abstract:\", paper_abstract)\n",
      "    print(\"Link:\", paper_link)\n",
      "    print(\"Published:\", paper_published)\n",
      "else:\n",
      "    print(\"No papers found on GPT-4.\")\n",
      "\n",
      "```\n",
      "\n",
      "Please make sure you have the `requests` and `feedparser` libraries installed (`pip install requests feedparser`) before running the code.\n",
      "\n",
      "After executing the code, it will print the title, abstract, link, and published date of the latest paper on GPT-4. You can then analyze the abstract to identify potential applications in software.\n",
      "\n",
      "Let me know if you need any further assistance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Latest Paper on GPT-4:\n",
      "Title: Building Real-World Meeting Summarization Systems using Large Language\n",
      "  Models: A Practical Perspective\n",
      "Abstract: This paper studies how to effectively build meeting summarization systems for\n",
      "real-world usage using large language models (LLMs). For this purpose, we\n",
      "conduct an extensive evaluation and comparison of various closed-source and\n",
      "open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings\n",
      "reveal that most closed-source LLMs are generally better in terms of\n",
      "performance. However, much smaller open-source models like LLaMA- 2 (7B and\n",
      "13B) could still achieve performance comparable to the large closed-source\n",
      "models even in zero-shot scenarios. Considering the privacy concerns of\n",
      "closed-source models for only being accessible via API, alongside the high cost\n",
      "associated with using fine-tuned versions of the closed-source models, the\n",
      "opensource models that can achieve competitive performance are more\n",
      "advantageous for industrial use. Balancing performance with associated costs\n",
      "and privacy concerns, the LLaMA-2-7B model looks more promising for industrial\n",
      "usage. In sum, this paper offers practical insights on using LLMs for\n",
      "real-world business meeting summarization, shedding light on the trade-offs\n",
      "between performance and cost.\n",
      "Link: http://arxiv.org/abs/2310.19233v1\n",
      "Published: 2023-10-30T02:25:21Z\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_gpt-35\u001B[0m (to chat_manager):\n",
      "\n",
      "Based on the latest paper titled \"Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective\" on GPT-4, here are some potential applications of GPT-4 in software:\n",
      "\n",
      "1. Meeting Summarization: The paper specifically focuses on building meeting summarization systems using large language models like GPT-4. GPT-4 can be used to automatically generate summaries of business meetings, saving time and effort for participants.\n",
      "\n",
      "2. Natural Language Processing (NLP): GPT-4, being a large language model, can be used for various NLP tasks such as text generation, sentiment analysis, question answering, and language translation. It can enhance the capabilities of software applications that require understanding and generating human-like text.\n",
      "\n",
      "3. Chatbots and Virtual Assistants: GPT-4 can be used to power chatbots and virtual assistants, enabling more natural and context-aware conversations. It can improve the user experience by providing accurate and relevant responses to user queries.\n",
      "\n",
      "4. Content Generation: GPT-4 can be used to generate content for various purposes, such as writing articles, creating product descriptions, or generating code snippets. It can assist software developers and content creators in generating high-quality content efficiently.\n",
      "\n",
      "5. Language Modeling and Understanding: GPT-4 can be used to improve language modeling and understanding in software applications. It can help in tasks like text completion, grammar correction, and language understanding, making software more intelligent and user-friendly.\n",
      "\n",
      "These are just a few potential applications of GPT-4 in software. The actual applications may vary depending on the specific use case and requirements.\n",
      "\n",
      "Let me know if there's anything else I can help you with.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-35\u001B[0m (to chat_manager):\n",
      "\n",
      "That's a great list of potential applications of GPT-4 in software! You've covered some key areas where GPT-4 can be leveraged to enhance software capabilities. These applications can greatly benefit businesses and users by automating tasks, improving user experiences, and generating high-quality content.\n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_gpt-35\u001B[0m (to chat_manager):\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-31 15:18:49] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elpis_ubuntu/LLM/autogen/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-4 in organization org-sbI6D5OYthpTDqe4nmNmzQoS on tokens per min. Limit: 10000 / min. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    }
   ],
   "source": [
    "config_path = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST'\n",
    "default_llm_config = {\n",
    "    'temperature': 0\n",
    "}\n",
    "\n",
    "administrator = AgentCreator(\n",
    "    task=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    config_path=config_path\n",
    ")\n",
    "administrator.build(default_llm_config)\n",
    "administrator.start(\n",
    "    initiate_agent_name=\"user\"\n",
    ")\n",
    "administrator.clear_all()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T06:19:27.389747500Z",
     "start_time": "2023-10-31T06:18:28.729808500Z"
    }
   },
   "id": "7d52e3d9a1bf91cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ca05bc74723e163"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
