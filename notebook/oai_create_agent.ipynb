{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple Agent Chat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1004af6a7fbfcd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement\n",
    "\n",
    "Auto Agent setup need `vllm>=0.2.0` and `fastchat>=0.2.30`, you can use the following command to install the latest version of [vLLM](https://github.com/vllm-project/vllm) and [FastChat](https://github.com/lm-sys/FastChat).\n",
    "```\n",
    "pip install vllm, \"fastchat[model_worker]\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec78dda8e3826d8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install vllm \"fastchat[model_worker]\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e9ae50658be975"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import autogen\n",
    "import subprocess as sp\n",
    "import time\n",
    "from typing import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T19:40:21.535921400Z",
     "start_time": "2023-10-29T19:40:20.942693900Z"
    }
   },
   "id": "8d06a99bd05026bb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class AgentCreator:\n",
    "    \"\"\"\n",
    "    Descriptions\n",
    "    \"\"\"\n",
    "\n",
    "    agent_procs: Dict[str, sp.Popen] = {}\n",
    "    agent_procs_assign: Dict[str, Tuple[autogen.AssistantAgent, str]] = {}\n",
    "    openai_server_name: str = 'openai'\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            endpoint_building_timeout: Optional[int] = 180\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            endpoint_building_timeout: timeout for building up an endpoint server.\n",
    "        \"\"\"\n",
    "        self.endpoint_building_timeout = endpoint_building_timeout\n",
    "\n",
    "    def build_endpoint(\n",
    "            self,\n",
    "            model_name_or_hf_repo: str,\n",
    "            world_size: Optional[int] = 1,\n",
    "            host: Optional[str] = \"127.0.0.1\",\n",
    "            port: Optional[str] = \"8000\",\n",
    "            config_path: Optional[str] = './OAI_CONFIG_LIST'\n",
    "    ) -> Tuple[dict, str]:\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            model_name_or_hf_repo: model name (e.g., \"gpt-3.5-turbo\", \"gpt-4\") or huggingface repository\n",
    "                (e.g., \"meta-llama/Llama-2-7b-chat-hf\") use for starting an endpoint.\n",
    "            world_size: the max size of parallel tensors (in most of the cases, this is identical to the amount of GPUs).\n",
    "            host: the endpoint's API address.\n",
    "            port: the port of the API address the endpoint listen to.\n",
    "            config_path: config list path of OpenAI api.\n",
    "\n",
    "        Returns:\n",
    "            (agent config list, endpoint server id)\n",
    "        \"\"\"\n",
    "        config_list = autogen.config_list_from_json(\n",
    "            config_path,\n",
    "            filter_dict={\n",
    "                'model': [model_name_or_hf_repo]\n",
    "            }\n",
    "        )\n",
    "        if \"gpt-\" in model_name_or_hf_repo:\n",
    "            server_id = self.openai_server_name\n",
    "        else:\n",
    "            model_name = model_name_or_hf_repo.split('/')[-1]\n",
    "            config_list[0]['api_base'] = f\"http://{host}:{port}/v1\"\n",
    "            server_id = f'{model_name}_{host}_{port}'\n",
    "            if self.agent_procs.get(server_id, None) is None:\n",
    "                # Use vLLM to set up a server with OpenAI API support.\n",
    "                agent_proc = sp.Popen(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                                       f'--host', f'{host}',\n",
    "                                       f'--port', f'{port}',\n",
    "                                       f'--model', f'{model_name_or_hf_repo}',\n",
    "                                       f'--tensor-parallel-size', f'{world_size}'], stdout=sp.PIPE, stderr=sp.STDOUT)\n",
    "                timeout_start = time.time()\n",
    "                while True:\n",
    "                    server_stdout = agent_proc.stdout.readline()\n",
    "                    timeout_end = time.time()\n",
    "                    if b\"running\" in server_stdout:\n",
    "                        print(f\"Running {model_name_or_hf_repo} on http://{host}:{port} with tensor parallel size {world_size}.\")\n",
    "                        break\n",
    "                    elif timeout_end - timeout_start > self.endpoint_building_timeout:\n",
    "                        raise RuntimeError(f'Timeout exceed. Fail to set up the endpoint for '\n",
    "                                           f'{model_name_or_hf_repo} on {host}:{port}.')\n",
    "                self.agent_procs[server_id] = agent_proc\n",
    "\n",
    "        return config_list, server_id\n",
    "\n",
    "    def create_agent(\n",
    "            self,\n",
    "            agent_name: str,\n",
    "            llm_config: dict,\n",
    "            server_id: str,\n",
    "            **kwargs\n",
    "    ) -> autogen.AssistantAgent:\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name that identify the function of the agent (e.g., Coder, Product Manager,...)\n",
    "            llm_config: specific configs for LLM (e.g., config_list, seed, temperature, ...).\n",
    "            server_id: endpoint server id.\n",
    "\n",
    "        Returns:\n",
    "            agent: a set-up agent.\n",
    "        \"\"\"\n",
    "        agent = autogen.AssistantAgent(name=agent_name,\n",
    "                                       llm_config=llm_config,\n",
    "                                       **kwargs)\n",
    "        self.agent_procs_assign[agent_name] = (agent, server_id)\n",
    "        return agent\n",
    "\n",
    "    def clear_agent(\n",
    "            self,\n",
    "            agent_name: str = None,\n",
    "            recycle_endpoint: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name of agent.\n",
    "            recycle_endpoint: trigger for recycle the endpoint server. If true, the endpoint will be recycled\n",
    "                when there is no agent depending on.\n",
    "        \"\"\"\n",
    "        _, server_id = self.agent_procs_assign[agent_name]\n",
    "        del self.agent_procs_assign[agent_name]\n",
    "        if recycle_endpoint:\n",
    "            if server_id == self.openai_server_name:\n",
    "                return\n",
    "            else:\n",
    "                for _, iter_sid in self.agent_procs_assign.values():\n",
    "                    if server_id == iter_sid:\n",
    "                        return\n",
    "                self.agent_procs[server_id].terminate()\n",
    "    \n",
    "    def clear_all(self):\n",
    "        \"\"\"\n",
    "        Clear all cached agents.\n",
    "        \"\"\"\n",
    "        for agent_name in [agent_name for agent_name in self.agent_procs_assign.keys()]:\n",
    "            self.clear_agent(agent_name)\n",
    "\n",
    "    def start(\n",
    "            self,\n",
    "            task: str,\n",
    "            manager_llm_config: dict,\n",
    "            manager_server_id: str,\n",
    "            max_round: Optional[int] = 12,\n",
    "            init_messages: Optional[List[dict]] = [],\n",
    "            user_proxy: autogen.UserProxyAgent = None,\n",
    "            initiate_agent_name: str = \"user\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            task: task description.\n",
    "            manager_llm_config: LLM configuration for a GroupChatManager.\n",
    "            manager_server_id: id of the server the manager depending on.\n",
    "            max_round: the maximum number of rounds.\n",
    "            init_messages: input messages before the task start. This can be the chat history from other group chat\n",
    "                or some preliminary of the task.\n",
    "            user_proxy: a user proxy for human interaction.\n",
    "            initiate_agent_name: the name of an agent use to initialize the group chat.\n",
    "        \"\"\"\n",
    "        agent_list = [agent for agent, _ in self.agent_procs_assign.values()]\n",
    "        if user_proxy is not None:\n",
    "            agent_list.append(user_proxy)\n",
    "        group_chat = autogen.GroupChat(agents=agent_list, messages=init_messages, max_round=max_round)\n",
    "        manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=manager_llm_config)\n",
    "        self.agent_procs_assign['manager'] = (manager, manager_server_id)\n",
    "\n",
    "        if initiate_agent_name == \"user\" and user_proxy is not None:\n",
    "            user_proxy.initiate_chat(manager, message=task)\n",
    "        else:\n",
    "            for agent in agent_list:\n",
    "                if initiate_agent_name == agent.name():\n",
    "                    agent.initiate_chat(manager, message=task)\n",
    "        \n",
    "        self.clear_agent('manager')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T19:40:21.588597Z",
     "start_time": "2023-10-29T19:40:21.544449200Z"
    }
   },
   "id": "d729233c9524f875"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2c3f521de23faa"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running facebook/opt-125m on http://127.0.0.1:8000 with tensor parallel size 1.\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to search for papers related to GPT-4. Then, we can analyze the paper to identify its potential applications in software.\n",
      "\n",
      "Here is the Python code to accomplish this task:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "xml_data = response.text\n",
      "paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "paper_summary = xml_data.split(\"<summary>\")[1].split(\"</summary>\")[0]\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Summary:\", paper_summary)\n",
      "\n",
      "# Analyze the potential applications in software\n",
      "potential_applications = [\"Natural Language Processing\", \"Text Generation\", \"Chatbots\", \"Language Translation\", \"Sentiment Analysis\"]\n",
      "print(\"\\nPotential Applications in Software:\")\n",
      "for application in potential_applications:\n",
      "    if application.lower() in paper_summary.lower():\n",
      "        print(\"-\", application)\n",
      "\n",
      "```\n",
      "\n",
      "Please note that the code uses the `requests` library to make an HTTP request to the arXiv API and parse the XML response. Make sure you have the `requests` library installed (`pip install requests`) before running the code.\n",
      "\n",
      "When you run the code, it will output the latest paper on GPT-4 from arXiv, including the title and summary. It will also analyze the paper's summary to identify potential applications in software and print them.\n",
      "\n",
      "Please let me know if you need any further assistance!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_opt-125m\u001B[0m (to chat_manager):\n",
      "\n",
      " If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "I apologize for the repeated message. If you have a specific problem that you would like to solve using Python code, please provide the details of the problem and I will be happy to assist you with a Python solution.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_opt-125m\u001B[0m (to chat_manager):\n",
      "\n",
      "  If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-30 04:38:04] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 403, in handle_error_response\n",
      "    error_data = resp[\"error\"]\n",
      "KeyError: 'error'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elpis_ubuntu/LLM/autogen/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "  File \"/home/elpis_ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py\", line 405, in handle_error_response\n",
      "    raise error.APIError(\n",
      "openai.error.APIError: Invalid response object from API: '{\"object\":\"error\",\"message\":\"This model\\'s maximum context length is 2048 tokens. However, you requested 2092 tokens (1892 in the messages, 200 in the completion). Please reduce the length of the messages or completion.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}' (HTTP response code was 400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:403\u001B[0m, in \u001B[0;36mAPIRequestor.handle_error_response\u001B[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001B[0m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 403\u001B[0m     error_data \u001B[38;5;241m=\u001B[39m \u001B[43mresp\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43merror\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m):\n",
      "\u001B[0;31mKeyError\u001B[0m: 'error'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAPIError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:224\u001B[0m, in \u001B[0;36mCompletion._get_response\u001B[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mopenai_completion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    226\u001B[0m     ServiceUnavailableError,\n\u001B[1;32m    227\u001B[0m     APIConnectionError,\n\u001B[1;32m    228\u001B[0m ):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# transient error\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    138\u001B[0m (\n\u001B[1;32m    139\u001B[0m     deployment_id,\n\u001B[1;32m    140\u001B[0m     engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m     api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m )\n\u001B[0;32m--> 153\u001B[0m response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:298\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    288\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    289\u001B[0m     method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    290\u001B[0m     url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m     request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    297\u001B[0m )\n\u001B[0;32m--> 298\u001B[0m resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:700\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 700\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    707\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:763\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_error_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    764\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_error\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:405\u001B[0m, in \u001B[0;36mAPIRequestor.handle_error_response\u001B[0;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001B[0m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m):\n\u001B[0;32m--> 405\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mAPIError(\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid response object from API: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m (HTTP response code \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    407\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwas \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (rbody, rcode),\n\u001B[1;32m    408\u001B[0m         rbody,\n\u001B[1;32m    409\u001B[0m         rcode,\n\u001B[1;32m    410\u001B[0m         resp,\n\u001B[1;32m    411\u001B[0m     )\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minternal_message\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m error_data:\n",
      "\u001B[0;31mAPIError\u001B[0m: Invalid response object from API: '{\"object\":\"error\",\"message\":\"This model\\'s maximum context length is 2048 tokens. However, you requested 2092 tokens (1892 in the messages, 200 in the completion). Please reduce the length of the messages or completion.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}' (HTTP response code was 400)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 32\u001B[0m\n\u001B[1;32m     24\u001B[0m user_proxy \u001B[38;5;241m=\u001B[39m autogen\u001B[38;5;241m.\u001B[39mUserProxyAgent(\n\u001B[1;32m     25\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser_proxy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     26\u001B[0m     system_message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA human admin.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     27\u001B[0m     code_execution_config\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlast_n_messages\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwork_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroupchat\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     28\u001B[0m     human_input_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTERMINATE\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     29\u001B[0m )\n\u001B[1;32m     31\u001B[0m task \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFind a latest paper about gpt-4 on arxiv and find its potential applications in software.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 32\u001B[0m \u001B[43madministrator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmg_llm_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgpt3_5_server_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_proxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_proxy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitiate_agent_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     38\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m administrator\u001B[38;5;241m.\u001B[39mclear_all()\n",
      "Cell \u001B[0;32mIn[2], line 161\u001B[0m, in \u001B[0;36mAgentCreator.start\u001B[0;34m(self, task, manager_llm_config, manager_server_id, max_round, init_messages, user_proxy, initiate_agent_name)\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_procs_assign[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmanager\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (manager, manager_server_id)\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m initiate_agent_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m user_proxy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 161\u001B[0m     \u001B[43muser_proxy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitiate_chat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m agent_list:\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:531\u001B[0m, in \u001B[0;36mConversableAgent.initiate_chat\u001B[0;34m(self, recipient, clear_history, silent, **context)\u001B[0m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001B[39;00m\n\u001B[1;32m    518\u001B[0m \n\u001B[1;32m    519\u001B[0m \u001B[38;5;124;03mReset the consecutive auto reply counter.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001B[39;00m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_chat(recipient, clear_history)\n\u001B[0;32m--> 531\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_init_message\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecipient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:334\u001B[0m, in \u001B[0;36mConversableAgent.send\u001B[0;34m(self, message, recipient, request_reply, silent)\u001B[0m\n\u001B[1;32m    332\u001B[0m valid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_append_oai_message(message, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant\u001B[39m\u001B[38;5;124m\"\u001B[39m, recipient)\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m valid:\n\u001B[0;32m--> 334\u001B[0m     \u001B[43mrecipient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreceive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_reply\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    337\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMessage can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    338\u001B[0m     )\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:462\u001B[0m, in \u001B[0;36mConversableAgent.receive\u001B[0;34m(self, message, sender, request_reply, silent)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request_reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m request_reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreply_at_receive[sender] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 462\u001B[0m reply \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_reply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat_messages\u001B[49m\u001B[43m[\u001B[49m\u001B[43msender\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(reply, sender, silent\u001B[38;5;241m=\u001B[39msilent)\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:781\u001B[0m, in \u001B[0;36mConversableAgent.generate_reply\u001B[0;34m(self, messages, sender, exclude)\u001B[0m\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_trigger(reply_func_tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m\"\u001B[39m], sender):\n\u001B[0;32m--> 781\u001B[0m     final, reply \u001B[38;5;241m=\u001B[39m \u001B[43mreply_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreply_func_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfig\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m final:\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m reply\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/groupchat.py:164\u001B[0m, in \u001B[0;36mGroupChatManager.run_chat\u001B[0;34m(self, messages, sender, config)\u001B[0m\n\u001B[1;32m    162\u001B[0m     speaker \u001B[38;5;241m=\u001B[39m groupchat\u001B[38;5;241m.\u001B[39mselect_speaker(speaker, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# let the speaker speak\u001B[39;00m\n\u001B[0;32m--> 164\u001B[0m     reply \u001B[38;5;241m=\u001B[39m \u001B[43mspeaker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_reply\u001B[49m\u001B[43m(\u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;66;03m# let the admin agent speak if interrupted\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m groupchat\u001B[38;5;241m.\u001B[39madmin_name \u001B[38;5;129;01min\u001B[39;00m groupchat\u001B[38;5;241m.\u001B[39magent_names:\n\u001B[1;32m    168\u001B[0m         \u001B[38;5;66;03m# admin agent is one of the participants\u001B[39;00m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:781\u001B[0m, in \u001B[0;36mConversableAgent.generate_reply\u001B[0;34m(self, messages, sender, exclude)\u001B[0m\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_trigger(reply_func_tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m\"\u001B[39m], sender):\n\u001B[0;32m--> 781\u001B[0m     final, reply \u001B[38;5;241m=\u001B[39m \u001B[43mreply_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreply_func_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfig\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m final:\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m reply\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:606\u001B[0m, in \u001B[0;36mConversableAgent.generate_oai_reply\u001B[0;34m(self, messages, sender, config)\u001B[0m\n\u001B[1;32m    603\u001B[0m     messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_oai_messages[sender]\n\u001B[1;32m    605\u001B[0m \u001B[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001B[39;00m\n\u001B[0;32m--> 606\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43moai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_oai_system_message\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mllm_config\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m, oai\u001B[38;5;241m.\u001B[39mChatCompletion\u001B[38;5;241m.\u001B[39mextract_text_or_function_call(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:803\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001B[0m\n\u001B[1;32m    801\u001B[0m     base_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_retry_period\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 803\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[43m        \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlast\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbase_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    808\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    810\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:834\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001B[0m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m diskcache\u001B[38;5;241m.\u001B[39mCache(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcache_path) \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_cache:\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mset_cache(seed)\n\u001B[0;32m--> 834\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:239\u001B[0m, in \u001B[0;36mCompletion._get_response\u001B[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001B[0m\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;66;03m# transient error\u001B[39;00m\n\u001B[1;32m    238\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrying in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_wait_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds...\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 239\u001B[0m     \u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_wait_time\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (RateLimitError, Timeout) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    241\u001B[0m     time_left \u001B[38;5;241m=\u001B[39m max_retry_period \u001B[38;5;241m-\u001B[39m (time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time \u001B[38;5;241m+\u001B[39m retry_wait_time)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "administrator = AgentCreator()\n",
    "config_path = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST'\n",
    "\n",
    "# build endpoint; create agent; get config (manager); initialize user proxy; task start; clear agent\n",
    "gpt3_5_config_list, gpt3_5_server_id = administrator.build_endpoint('gpt-3.5-turbo', config_path=config_path)\n",
    "opt_config_list, opt_server_id = administrator.build_endpoint('facebook/opt-125m', config_path=config_path)\n",
    "\n",
    "coder_llm_config = {\n",
    "    'config_list': gpt3_5_config_list,\n",
    "    'temperature': 0,\n",
    "}\n",
    "pm_llm_config = {\n",
    "    'config_list': opt_config_list,\n",
    "    'temperature': 0,\n",
    "    'max_tokens': 200\n",
    "}\n",
    "mg_llm_config = {\n",
    "    'config_list': gpt3_5_config_list,\n",
    "    'temperature': 0,\n",
    "}\n",
    "\n",
    "administrator.create_agent('Coder_gpt-3_5', coder_llm_config, gpt3_5_server_id)\n",
    "administrator.create_agent('Product_manager_opt-125m', pm_llm_config, opt_server_id)\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "    human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "\n",
    "task = \"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    "administrator.start(\n",
    "    task,\n",
    "    mg_llm_config,\n",
    "    gpt3_5_server_id,\n",
    "    user_proxy=user_proxy,\n",
    "    initiate_agent_name=\"user\"\n",
    ")\n",
    "administrator.clear_all()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T19:38:08.180214600Z",
     "start_time": "2023-10-29T19:37:50.542041500Z"
    }
   },
   "id": "dc7e531b53e5d7d6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to search for papers related to GPT-4. Then, we can analyze the paper to identify its potential applications in software.\n",
      "\n",
      "Here is the Python code to accomplish this task:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "xml_data = response.text\n",
      "paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "paper_summary = xml_data.split(\"<summary>\")[1].split(\"</summary>\")[0]\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Summary:\", paper_summary)\n",
      "\n",
      "# Analyze the potential applications in software\n",
      "potential_applications = [\"Natural Language Processing\", \"Text Generation\", \"Chatbots\", \"Language Translation\", \"Sentiment Analysis\"]\n",
      "print(\"\\nPotential Applications in Software:\")\n",
      "for application in potential_applications:\n",
      "    if application.lower() in paper_summary.lower():\n",
      "        print(\"-\", application)\n",
      "\n",
      "```\n",
      "\n",
      "Please note that the code uses the `requests` library to make an HTTP request to the arXiv API and parse the XML response. Make sure you have the `requests` library installed (`pip install requests`) before running the code.\n",
      "\n",
      "When you run the code, it will output the latest paper on GPT-4 from arXiv, including the title and summary. It will also analyze the paper's summary to identify potential applications in software and print them.\n",
      "\n",
      "Please let me know if you need any further assistance!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_gpt-4\u001B[0m (to chat_manager):\n",
      "\n",
      "# filename: gpt4_paper_search.py\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "root = ET.fromstring(response.content)\n",
      "entry = root.find('{http://www.w3.org/2005/Atom}entry')\n",
      "paper_title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
      "paper_summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Summary:\", paper_summary)\n",
      "\n",
      "# Analyze the potential applications in software\n",
      "potential_applications = [\"Natural Language Processing\", \"Text Generation\", \"Chatbots\", \"Language Translation\", \"Sentiment Analysis\"]\n",
      "print(\"\\nPotential Applications in Software:\")\n",
      "for application in potential_applications:\n",
      "    if application.lower() in paper_summary.lower():\n",
      "        print(\"-\", application)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 10, in <module>\n",
      "    paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "I apologize for the error. It seems that the code I provided to extract the paper details from the XML response is not working as expected. Let's try an alternative approach using the `xml.etree.ElementTree` module to parse the XML response.\n",
      "\n",
      "Please use the updated code below:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "root = ET.fromstring(response.content)\n",
      "entry = root.find('{http://www.w3.org/2005/Atom}entry')\n",
      "paper_title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
      "paper_summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Summary:\", paper_summary)\n",
      "\n",
      "# Analyze the potential applications in software\n",
      "potential_applications = [\"Natural Language Processing\", \"Text Generation\", \"Chatbots\", \"Language Translation\", \"Sentiment Analysis\"]\n",
      "print(\"\\nPotential Applications in Software:\")\n",
      "for application in potential_applications:\n",
      "    if application.lower() in paper_summary.lower():\n",
      "        print(\"-\", application)\n",
      "```\n",
      "\n",
      "Please make sure you have the `requests` library installed (`pip install requests`) before running the code.\n",
      "\n",
      "Let me know if you encounter any further issues!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Latest Paper on GPT-4:\n",
      "Title: Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models\n",
      "Summary:   With LLMs shifting their role from statistical modeling of language to\n",
      "serving as general-purpose AI agents, how should LLM evaluations change?\n",
      "Arguably, a key ability of an AI agent is to flexibly combine, as needed, the\n",
      "basic skills it has learned. The capability to combine skills plays an\n",
      "important role in (human) pedagogy and also in a paper on emergence phenomena\n",
      "(Arora & Goyal, 2023).\n",
      "  This work introduces Skill-Mix, a new evaluation to measure ability to\n",
      "combine skills. Using a list of $N$ skills the evaluator repeatedly picks\n",
      "random subsets of $k$ skills and asks the LLM to produce text combining that\n",
      "subset of skills. Since the number of subsets grows like $N^k$, for even modest\n",
      "$k$ this evaluation will, with high probability, require the LLM to produce\n",
      "text significantly different from any text in the training set. The paper\n",
      "develops a methodology for (a) designing and administering such an evaluation,\n",
      "and (b) automatic grading (plus spot-checking by humans) of the results using\n",
      "GPT-4 as well as the open LLaMA-2 70B model.\n",
      "  Administering a version of to popular chatbots gave results that, while\n",
      "generally in line with prior expectations, contained surprises. Sizeable\n",
      "differences exist among model capabilities that are not captured by their\n",
      "ranking on popular LLM leaderboards (\"cramming for the leaderboard\").\n",
      "Furthermore, simple probability calculations indicate that GPT-4's reasonable\n",
      "performance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior\n",
      "(Bender et al., 2021), i.e., it combines skills in ways that it had not seen\n",
      "during training.\n",
      "  We sketch how the methodology can lead to a Skill-Mix based eco-system of\n",
      "open evaluations for AI capabilities of future models.\n",
      "\n",
      "\n",
      "Potential Applications in Software:\n",
      "- Chatbots\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_gpt-4\u001B[0m (to chat_manager):\n",
      "\n",
      "The latest paper on GPT-4 from arXiv is titled \"Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models\". The summary of the paper is as follows:\n",
      "\n",
      "\"With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora & Goyal, 2023). This work introduces Skill-Mix, a new evaluation to measure ability to combine skills. Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text significantly different from any text in the training set. The paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using GPT-4 as well as the open LLaMA-2 70B model. Administering a version of to popular chatbots gave results that, while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards (\"cramming for the leaderboard\"). Furthermore, simple probability calculations indicate that GPT-4's reasonable performance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior (Bender et al., 2021), i.e., it combines skills in ways that it had not seen during training. We sketch how the methodology can lead to a Skill-Mix based eco-system of open evaluations for AI capabilities of future models.\"\n",
      "\n",
      "Based on the summary, the potential application of GPT-4 in software identified is in the development of Chatbots. The paper discusses the use of GPT-4 in creating chatbots that can flexibly combine different skills to produce text that is significantly different from any text in the training set. This suggests that GPT-4 could be used to create more advanced and versatile chatbots that can handle a wider range of tasks and conversations.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "administrator = AgentCreator()\n",
    "config_path = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST'\n",
    "\n",
    "# build endpoint; create agent; get config (manager); initialize user proxy; task start; clear agent\n",
    "gpt3_5_config_list, gpt3_5_server_id = administrator.build_endpoint('gpt-3.5-turbo', config_path=config_path)\n",
    "gpt4_config_list, gpt4_server_id = administrator.build_endpoint('gpt-4', config_path=config_path)\n",
    "\n",
    "coder_llm_config = {\n",
    "    'config_list': gpt3_5_config_list,\n",
    "    'temperature': 0,\n",
    "}\n",
    "pm_llm_config = {\n",
    "    'config_list': gpt4_config_list,\n",
    "    'temperature': 0,\n",
    "}\n",
    "mg_llm_config = {\n",
    "    'config_list': gpt3_5_config_list,\n",
    "    'temperature': 0,\n",
    "}\n",
    "\n",
    "administrator.create_agent('Coder_gpt-3_5', coder_llm_config, gpt3_5_server_id)\n",
    "administrator.create_agent('Product_manager_gpt-4', pm_llm_config, gpt4_server_id)\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "    human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "\n",
    "task = \"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    "administrator.start(\n",
    "    task,\n",
    "    mg_llm_config,\n",
    "    gpt3_5_server_id,\n",
    "    user_proxy=user_proxy,\n",
    "    initiate_agent_name=\"user\"\n",
    ")\n",
    "administrator.clear_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T19:41:44.526680Z",
     "start_time": "2023-10-29T19:40:26.388225100Z"
    }
   },
   "id": "7d52e3d9a1bf91cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ca05bc74723e163"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
