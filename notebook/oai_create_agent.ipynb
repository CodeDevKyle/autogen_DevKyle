{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple Agent Chat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1004af6a7fbfcd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement\n",
    "\n",
    "Auto Agent setup need `vllm>=0.2.0`, you can use the following command to install the latest version of [vLLM](https://github.com/vllm-project/vllm).\n",
    "```\n",
    "pip install vllm\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec78dda8e3826d8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install vllm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e9ae50658be975"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import autogen\n",
    "import subprocess as sp\n",
    "from typing import *"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d06a99bd05026bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AgentCreator:\n",
    "    \n",
    "    agent_procs: Dict[str, sp.Popen] = {}\n",
    "    agent_procs_assign: Dict[str, Tuple[autogen.AssistantAgent, str]] = {}\n",
    "    openai_server_name: str = 'openai'\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        logging_path: Optional[str] = \"logs/\",\n",
    "    ):\n",
    "        self.logging_path = logging_path\n",
    "        \n",
    "    def get_config(\n",
    "        self,\n",
    "        model_name_or_hf_repo: str,\n",
    "        world_size: Optional[int] = 1,\n",
    "        host: Optional[str] = \"127.0.0.1\",\n",
    "        port: Optional[str] = \"8000\",\n",
    "        seed: Optional[int] = 42,\n",
    "        temperature: Optional[int] = 0,\n",
    "    ) -> Tuple[List, str]:\n",
    "    \n",
    "        if \"gpt-\" in model_name_or_hf_repo:\n",
    "            config = autogen.config_list_from_json(\n",
    "                \"OAI_CONFIG_LIST\",\n",
    "                filter_dict={\n",
    "                    \"model\": [model_name_or_hf_repo]\n",
    "                },\n",
    "            )\n",
    "            server_id = self.openai_server_name\n",
    "        else:\n",
    "            model_name = model_name_or_hf_repo.split('/')[-1]\n",
    "            config = [autogen.get_config_list(\n",
    "                api_keys = [\"EMPTY\"],\n",
    "                api_bases = [f\"{host}:{port}\"],\n",
    "            )]\n",
    "            server_id = f'{model_name}_{host}_{port}_{seed}_{temperature}'\n",
    "            if self.agent_procs.get(server_id, None) is None:\n",
    "                # Use vllm to set up a server with OpenAI API support\n",
    "                agent_proc = sp.Popen(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                                       '--host', host,\n",
    "                                       '--port', port,\n",
    "                                       '--model', model_name_or_hf_repo,\n",
    "                                       '--tensor-parallel-size', world_size,\n",
    "                                       '>', f\"{self.logging_path}/serving_{model_name}.log\"])\n",
    "                self.agent_procs[server_id] = agent_proc\n",
    "        \n",
    "        return config, server_id\n",
    "    \n",
    "    def create_agent(\n",
    "        self, \n",
    "        agent_name: str,\n",
    "        seed: int = 42,\n",
    "        temperature: int = 0,\n",
    "        **kwargs\n",
    "    ) -> autogen.AssistantAgent:\n",
    "        config, server_id = self.get_config(seed=seed, temperature=temperature, **kwargs)\n",
    "        agent = autogen.AssistantAgent(name=agent_name,\n",
    "                                       llm_config={\n",
    "                                           \"seed\": seed,  # seed for caching and reproducibility\n",
    "                                           \"config_list\": config,  # a list of OpenAI API configurations\n",
    "                                           \"temperature\": temperature,  # temperature for sampling\n",
    "                                       }, **kwargs)\n",
    "        self.agent_procs_assign[agent_name] = (agent, server_id)\n",
    "        return agent\n",
    "\n",
    "    def clear_agent(\n",
    "        self,\n",
    "        agent_name: str,\n",
    "        recycle_endpoint: bool = True\n",
    "    ):\n",
    "        _, server_id = self.agent_procs_assign[agent_name]\n",
    "        del self.agent_procs_assign[agent_name]\n",
    "        if recycle_endpoint:\n",
    "            if server_id == self.openai_server_name:\n",
    "                return\n",
    "            else:\n",
    "                for _, iter_sid in self.agent_procs_assign.values():\n",
    "                    if server_id == iter_sid:\n",
    "                        return\n",
    "                self.agent_procs[server_id].terminate()\n",
    "    \n",
    "    def start(self,\n",
    "              task: str,\n",
    "              manager_config: dict = {},\n",
    "              max_round: Optional[int] = 12,\n",
    "              init_messages: Optional[List[dict]] = [],\n",
    "              user_name: Optional[str] = \"User Proxy\",\n",
    "              user_system_message: Optional[str] = \"A human admin.\",\n",
    "              **kwargs):\n",
    "        agent_list = [agent for agent, _ in self.agent_procs_assign]\n",
    "        user_proxy = autogen.UserProxyAgent(\n",
    "            name=user_name,\n",
    "            system_message=user_system_message,\n",
    "            **kwargs\n",
    "        )\n",
    "        agent_list.append(user_proxy)\n",
    "        groupchat = autogen.GroupChat(agents=agent_list, messages=init_messages, max_round=max_round)\n",
    "        manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=manager_config)\n",
    "        \n",
    "        user_proxy.initiate_chat(manager, message=task)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d729233c9524f875"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
