{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple Agent Chat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1004af6a7fbfcd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement\n",
    "\n",
    "Auto Agent setup need `vllm>=0.2.0` and `fastchat>=0.2.30`, you can use the following command to install the latest version of [vLLM](https://github.com/vllm-project/vllm) and [FastChat](https://github.com/lm-sys/FastChat).\n",
    "```\n",
    "pip install vllm, \"fastchat[model_worker]\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec78dda8e3826d8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install vllm \"fastchat[model_worker]\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e9ae50658be975"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import autogen\n",
    "import subprocess as sp\n",
    "import time\n",
    "from typing import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T09:44:51.998713900Z",
     "start_time": "2023-10-29T09:44:51.309000800Z"
    }
   },
   "id": "8d06a99bd05026bb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AgentCreator:\n",
    "    \"\"\"\n",
    "    Descriptions\n",
    "    \"\"\"\n",
    "\n",
    "    agent_procs: Dict[str, sp.Popen] = {}\n",
    "    agent_procs_assign: Dict[str, Tuple[autogen.AssistantAgent, str]] = {}\n",
    "    openai_server_name: str = 'openai'\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            endpoint_building_timeout: Optional[int] = 180\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            endpoint_building_timeout: timeout for building up an endpoint server.\n",
    "        \"\"\"\n",
    "        self.endpoint_building_timeout = endpoint_building_timeout\n",
    "\n",
    "    def get_config(\n",
    "            self,\n",
    "            model_name_or_hf_repo: str,\n",
    "            world_size: Optional[int] = 1,\n",
    "            host: Optional[str] = \"127.0.0.1\",\n",
    "            port: Optional[str] = \"8000\",\n",
    "            seed: Optional[int] = 42,\n",
    "            temperature: Optional[int] = 0,\n",
    "            max_tokens: Optional[int] = 200\n",
    "    ) -> Tuple[List, str]:\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            model_name_or_hf_repo: model name (e.g., \"gpt-3.5-turbo\", \"gpt-4\") or huggingface repository\n",
    "                (e.g., \"meta-llama/Llama-2-7b-chat-hf\") use for starting an endpoint.\n",
    "            world_size: the max size of parallel tensors (in most of the cases, this is identical to the amount of GPUs).\n",
    "            host: the endpoint's API address.\n",
    "            port: the port of the API address the endpoint listen to.\n",
    "            seed: random seed for agent.\n",
    "            temperature: temperature for an agent that use to control the \"preciseness\" of the answer.\n",
    "            max_tokens: max number of tokens that the Agent can reply.\n",
    "\n",
    "        Returns:\n",
    "            (agent config list, endpoint server id)\n",
    "        \"\"\"\n",
    "        if \"gpt-\" in model_name_or_hf_repo:\n",
    "            config = autogen.config_list_from_json(\n",
    "                \"/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST\",\n",
    "                filter_dict={\n",
    "                    \"model\": [model_name_or_hf_repo]\n",
    "                },\n",
    "            )\n",
    "            server_id = self.openai_server_name\n",
    "        else:\n",
    "            model_name = model_name_or_hf_repo.split('/')[-1]\n",
    "            config = autogen.get_config_list(\n",
    "                api_keys=[\"EMPTY\"],\n",
    "                api_bases=[f\"http://{host}:{port}/v1\"],\n",
    "            )\n",
    "            config[0].update({\n",
    "                \"model\": model_name_or_hf_repo,\n",
    "                \"max_tokens\": max_tokens\n",
    "            })\n",
    "            server_id = f'{model_name}_{host}_{port}_{seed}_{temperature}'\n",
    "            if self.agent_procs.get(server_id, None) is None:\n",
    "                # Use vLLM to set up a server with OpenAI API support.\n",
    "                agent_proc = sp.Popen(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                                       f'--host', f'{host}',\n",
    "                                       f'--port', f'{port}',\n",
    "                                       f'--model', f'{model_name_or_hf_repo}',\n",
    "                                       f'--tensor-parallel-size', f'{world_size}'], stdout=sp.PIPE, stderr=sp.STDOUT)\n",
    "                timeout_start = time.time()\n",
    "                while True:\n",
    "                    server_stdout = agent_proc.stdout.readline()\n",
    "                    timeout_end = time.time()\n",
    "                    if b\"running\" in server_stdout:\n",
    "                        print(f\"Running {model_name_or_hf_repo} endpoint on http://{host}:{port} with tensor parallel size {world_size}.\")\n",
    "                        break\n",
    "                    elif timeout_end - timeout_start > self.endpoint_building_timeout:\n",
    "                        raise RuntimeError(f'Timeout exceed. Fail to set up the endpoint for '\n",
    "                                           f'{model_name_or_hf_repo} on {host}:{port}.')\n",
    "                self.agent_procs[server_id] = agent_proc\n",
    "\n",
    "        return config, server_id\n",
    "\n",
    "    def create_agent(\n",
    "            self,\n",
    "            agent_name: str,\n",
    "            model_name_or_hf_repo: str,\n",
    "            seed: int = 42,\n",
    "            temperature: int = 0,\n",
    "            **kwargs\n",
    "    ) -> autogen.AssistantAgent:\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name that identify the function of the agent (e.g., Coder, Product Manager,...)\n",
    "            model_name_or_hf_repo: model name (e.g., \"gpt-3.5-turbo\", \"gpt-4\") or huggingface repository\n",
    "                (e.g., \"meta-llama/Llama-2-7b-chat-hf\") use for starting an endpoint.\n",
    "            seed: random seed for agent.\n",
    "            temperature: temperature for an agent that use to control the \"preciseness\" of the answer.\n",
    "\n",
    "        Returns:\n",
    "            agent: a set-up agent.\n",
    "        \"\"\"\n",
    "        config, server_id = self.get_config(model_name_or_hf_repo=model_name_or_hf_repo,\n",
    "                                            seed=seed,\n",
    "                                            temperature=temperature,\n",
    "                                            **kwargs)\n",
    "        agent = autogen.AssistantAgent(name=agent_name,\n",
    "                                       llm_config={\n",
    "                                           \"seed\": seed,  # seed for caching and reproducibility\n",
    "                                           \"config_list\": config,  # a list of OpenAI API configurations\n",
    "                                           \"temperature\": temperature,  # temperature for sampling\n",
    "                                       })\n",
    "        self.agent_procs_assign[agent_name] = (agent, server_id)\n",
    "        return agent\n",
    "\n",
    "    def clear_agent(\n",
    "            self,\n",
    "            agent_name: str = None,\n",
    "            recycle_endpoint: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name of agent.\n",
    "            recycle_endpoint: trigger for recycle the endpoint server. If true, the endpoint will be recycled\n",
    "                when there is no agent relying on.\n",
    "        \"\"\"\n",
    "        _, server_id = self.agent_procs_assign[agent_name]\n",
    "        del self.agent_procs_assign[agent_name]\n",
    "        if recycle_endpoint:\n",
    "            if server_id == self.openai_server_name:\n",
    "                return\n",
    "            else:\n",
    "                for _, iter_sid in self.agent_procs_assign.values():\n",
    "                    if server_id == iter_sid:\n",
    "                        return\n",
    "                self.agent_procs[server_id].terminate()\n",
    "    \n",
    "    def clear_all(self):\n",
    "        \"\"\"\n",
    "        Clear all cached agents.\n",
    "        \"\"\"\n",
    "        for agent_name in [agent_name for agent_name in self.agent_procs_assign.keys()]:\n",
    "            self.clear_agent(agent_name)\n",
    "\n",
    "    def start(\n",
    "            self,\n",
    "            task: str,\n",
    "            manager_config: dict,\n",
    "            max_round: Optional[int] = 12,\n",
    "            init_messages: Optional[List[dict]] = [],\n",
    "            user_proxy: autogen.UserProxyAgent = None,\n",
    "            initiate_agent_name: str = \"user\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            task: description of a task.\n",
    "            manager_config: LLM configuration for a GroupChatManager.\n",
    "            max_round: the maximum number of rounds.\n",
    "            init_messages: input messages before the task start. This can be the chat history from other group chat\n",
    "                or some preliminary of the task.\n",
    "            user_proxy: a user proxy for human interaction.\n",
    "            initiate_agent_name: the name of an agent use to initialize the group chat.\n",
    "        \"\"\"\n",
    "        agent_list = [agent for agent, _ in self.agent_procs_assign.values()]\n",
    "        if user_proxy is not None:\n",
    "            agent_list.append(user_proxy)\n",
    "        group_chat = autogen.GroupChat(agents=agent_list, messages=init_messages, max_round=max_round)\n",
    "        manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=manager_config)\n",
    "\n",
    "        if initiate_agent_name == \"user\" and user_proxy is not None:\n",
    "            user_proxy.initiate_chat(manager, message=task)\n",
    "        else:\n",
    "            for agent in agent_list:\n",
    "                if initiate_agent_name == agent.name():\n",
    "                    agent.initiate_chat(manager, message=task)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T09:48:20.633445Z",
     "start_time": "2023-10-29T09:48:20.587240200Z"
    }
   },
   "id": "d729233c9524f875"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2c3f521de23faa"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running facebook/opt-125m endpoint on http://127.0.0.1:8000 with tensor parallel size 1.\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to search for papers related to GPT-4. Then, we can analyze the paper to identify its potential applications in software.\n",
      "\n",
      "Here is the Python code to accomplish this task:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "xml_data = response.text\n",
      "paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "paper_summary = xml_data.split(\"<summary>\")[1].split(\"</summary>\")[0]\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Summary:\", paper_summary)\n",
      "\n",
      "# Analyze the potential applications in software\n",
      "potential_applications = [\"Natural Language Processing\", \"Text Generation\", \"Chatbots\", \"Language Translation\", \"Sentiment Analysis\"]\n",
      "print(\"\\nPotential Applications in Software:\")\n",
      "for application in potential_applications:\n",
      "    if application.lower() in paper_summary.lower():\n",
      "        print(\"-\", application)\n",
      "\n",
      "```\n",
      "\n",
      "Please note that the code uses the `requests` library to make an HTTP request to the arXiv API and parse the XML response. Make sure you have the `requests` library installed (`pip install requests`) before running the code.\n",
      "\n",
      "When you run the code, it will output the latest paper on GPT-4 from arXiv, including the title and summary. It will also analyze the paper's summary to identify potential applications in software and print them.\n",
      "\n",
      "Please let me know if you need any further assistance!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_opt-125m\u001B[0m (to chat_manager):\n",
      "\n",
      " If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_opt-125m\u001B[0m (to chat_manager):\n",
      "\n",
      "  If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "Understood! If you have a specific problem that you would like me to solve using Python code, please provide me with the details of the problem. I'll be happy to help you with a Python solution.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_opt-125m\u001B[0m (to chat_manager):\n",
      "\n",
      "  If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem.\n",
      "### Human: If you need to solve a problem with a python code, please use the python code to solve the problem\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion. If you have a specific problem that you would like me to solve using Python code, please provide me with the details of the problem. I'll be happy to help you with a Python solution.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 15\u001B[0m\n\u001B[1;32m      7\u001B[0m user_proxy \u001B[38;5;241m=\u001B[39m autogen\u001B[38;5;241m.\u001B[39mUserProxyAgent(\n\u001B[1;32m      8\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser_proxy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      9\u001B[0m     system_message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA human admin.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m     code_execution_config\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlast_n_messages\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwork_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroupchat\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     11\u001B[0m     human_input_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTERMINATE\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     12\u001B[0m )\n\u001B[1;32m     14\u001B[0m task \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFind a latest paper about gpt-4 on arxiv and find its potential applications in software.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 15\u001B[0m \u001B[43madministrator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmg_config\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_proxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_proxy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitiate_agent_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     20\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 172\u001B[0m, in \u001B[0;36mAgentCreator.start\u001B[0;34m(self, task, manager_config, max_round, init_messages, user_proxy, initiate_agent_name)\u001B[0m\n\u001B[1;32m    169\u001B[0m manager \u001B[38;5;241m=\u001B[39m autogen\u001B[38;5;241m.\u001B[39mGroupChatManager(groupchat\u001B[38;5;241m=\u001B[39mgroup_chat, llm_config\u001B[38;5;241m=\u001B[39mmanager_config)\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m initiate_agent_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m user_proxy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 172\u001B[0m     \u001B[43muser_proxy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitiate_chat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m agent_list:\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:531\u001B[0m, in \u001B[0;36mConversableAgent.initiate_chat\u001B[0;34m(self, recipient, clear_history, silent, **context)\u001B[0m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001B[39;00m\n\u001B[1;32m    518\u001B[0m \n\u001B[1;32m    519\u001B[0m \u001B[38;5;124;03mReset the consecutive auto reply counter.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001B[39;00m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_chat(recipient, clear_history)\n\u001B[0;32m--> 531\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_init_message\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecipient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:334\u001B[0m, in \u001B[0;36mConversableAgent.send\u001B[0;34m(self, message, recipient, request_reply, silent)\u001B[0m\n\u001B[1;32m    332\u001B[0m valid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_append_oai_message(message, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant\u001B[39m\u001B[38;5;124m\"\u001B[39m, recipient)\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m valid:\n\u001B[0;32m--> 334\u001B[0m     \u001B[43mrecipient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreceive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_reply\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    337\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMessage can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    338\u001B[0m     )\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:462\u001B[0m, in \u001B[0;36mConversableAgent.receive\u001B[0;34m(self, message, sender, request_reply, silent)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request_reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m request_reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreply_at_receive[sender] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 462\u001B[0m reply \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_reply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat_messages\u001B[49m\u001B[43m[\u001B[49m\u001B[43msender\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(reply, sender, silent\u001B[38;5;241m=\u001B[39msilent)\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:781\u001B[0m, in \u001B[0;36mConversableAgent.generate_reply\u001B[0;34m(self, messages, sender, exclude)\u001B[0m\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_trigger(reply_func_tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m\"\u001B[39m], sender):\n\u001B[0;32m--> 781\u001B[0m     final, reply \u001B[38;5;241m=\u001B[39m \u001B[43mreply_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreply_func_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfig\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m final:\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m reply\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/groupchat.py:164\u001B[0m, in \u001B[0;36mGroupChatManager.run_chat\u001B[0;34m(self, messages, sender, config)\u001B[0m\n\u001B[1;32m    162\u001B[0m     speaker \u001B[38;5;241m=\u001B[39m groupchat\u001B[38;5;241m.\u001B[39mselect_speaker(speaker, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# let the speaker speak\u001B[39;00m\n\u001B[0;32m--> 164\u001B[0m     reply \u001B[38;5;241m=\u001B[39m \u001B[43mspeaker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_reply\u001B[49m\u001B[43m(\u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;66;03m# let the admin agent speak if interrupted\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m groupchat\u001B[38;5;241m.\u001B[39madmin_name \u001B[38;5;129;01min\u001B[39;00m groupchat\u001B[38;5;241m.\u001B[39magent_names:\n\u001B[1;32m    168\u001B[0m         \u001B[38;5;66;03m# admin agent is one of the participants\u001B[39;00m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:781\u001B[0m, in \u001B[0;36mConversableAgent.generate_reply\u001B[0;34m(self, messages, sender, exclude)\u001B[0m\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_trigger(reply_func_tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m\"\u001B[39m], sender):\n\u001B[0;32m--> 781\u001B[0m     final, reply \u001B[38;5;241m=\u001B[39m \u001B[43mreply_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreply_func_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfig\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m final:\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m reply\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:606\u001B[0m, in \u001B[0;36mConversableAgent.generate_oai_reply\u001B[0;34m(self, messages, sender, config)\u001B[0m\n\u001B[1;32m    603\u001B[0m     messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_oai_messages[sender]\n\u001B[1;32m    605\u001B[0m \u001B[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001B[39;00m\n\u001B[0;32m--> 606\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43moai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_oai_system_message\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mllm_config\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m, oai\u001B[38;5;241m.\u001B[39mChatCompletion\u001B[38;5;241m.\u001B[39mextract_text_or_function_call(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:803\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001B[0m\n\u001B[1;32m    801\u001B[0m     base_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_retry_period\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 803\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[43m        \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlast\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbase_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    808\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    810\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:834\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001B[0m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m diskcache\u001B[38;5;241m.\u001B[39mCache(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcache_path) \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_cache:\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mset_cache(seed)\n\u001B[0;32m--> 834\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:224\u001B[0m, in \u001B[0;36mCompletion._get_response\u001B[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001B[0m\n\u001B[1;32m    222\u001B[0m         response \u001B[38;5;241m=\u001B[39m openai_completion\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mopenai_completion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    226\u001B[0m     ServiceUnavailableError,\n\u001B[1;32m    227\u001B[0m     APIConnectionError,\n\u001B[1;32m    228\u001B[0m ):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# transient error\u001B[39;00m\n\u001B[1;32m    230\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrying in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_wait_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds...\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    137\u001B[0m ):\n\u001B[1;32m    138\u001B[0m     (\n\u001B[1;32m    139\u001B[0m         deployment_id,\n\u001B[1;32m    140\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m     )\n\u001B[0;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:288\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    279\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    286\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    287\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 288\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_raw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupplied_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response(result, stream)\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:596\u001B[0m, in \u001B[0;36mAPIRequestor.request_raw\u001B[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    594\u001B[0m     _thread_context\u001B[38;5;241m.\u001B[39msession_create_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 596\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_thread_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m        \u001B[49m\u001B[43mabs_url\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mTIMEOUT_SECS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_thread_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    607\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mTimeout(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRequest timed out: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(e)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/requests/adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    483\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_proxy(conn)\n\u001B[1;32m    713\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[0;32m--> 714\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[1;32m    725\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[1;32m    726\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n\u001B[1;32m    728\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    461\u001B[0m             httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[1;32m    462\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    463\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    464\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    465\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m--> 466\u001B[0m             \u001B[43msix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;66;03m# Python 3\u001B[39;00m\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 461\u001B[0m         httplib_response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    463\u001B[0m         \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    464\u001B[0m         \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    465\u001B[0m         \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m    466\u001B[0m         six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1377\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "administrator = AgentCreator()\n",
    "\n",
    "# create agent; get config (manager); initialize user proxy; task start; clear agent\n",
    "administrator.create_agent('Coder_gpt-3_5', 'gpt-3.5-turbo', seed=41)\n",
    "administrator.create_agent('Product_manager_opt-125m', 'facebook/opt-125m', max_tokens=100, seed=41)\n",
    "mg_config, _ = administrator.get_config('gpt-3.5-turbo', seed=41)\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "    human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "\n",
    "task = \"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    "administrator.start(\n",
    "    task,\n",
    "    mg_config[0],\n",
    "    user_proxy=user_proxy,\n",
    "    initiate_agent_name=\"user\"\n",
    ")\n",
    "administrator.clear_all()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T09:37:45.902790700Z",
     "start_time": "2023-10-29T09:36:44.499788900Z"
    }
   },
   "id": "dc7e531b53e5d7d6"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-3_5\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to search for papers related to GPT-4. Then, we can analyze the abstract or content of the paper to identify its potential applications in software.\n",
      "\n",
      "Here is the Python code to accomplish this task:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "xml_data = response.text\n",
      "paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "paper_abstract = xml_data.split(\"<summary>\")[1].split(\"</summary>\")[0]\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Abstract:\", paper_abstract)\n",
      "```\n",
      "\n",
      "Please note that the above code uses the `requests` library to make an HTTP request to the arXiv API and parse the XML response. Make sure you have the `requests` library installed (`pip install requests`) before running the code.\n",
      "\n",
      "After executing the code, it will print the title and abstract of the latest paper on GPT-4. You can then analyze the abstract to identify potential applications in software.\n",
      "\n",
      "Please let me know if you need any further assistance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_gpt-4\u001B[0m (to chat_manager):\n",
      "\n",
      "I'm sorry for the confusion, but the code I provided is not executable because it's not in a code block. Here is the correct version:\n",
      "\n",
      "```python\n",
      "# filename: arxiv_search.py\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "soup = BeautifulSoup(response.content, 'html.parser')\n",
      "entry = soup.find('entry')\n",
      "\n",
      "if entry:\n",
      "    paper_title = entry.title.text\n",
      "    paper_abstract = entry.summary.text\n",
      "\n",
      "    # Print the paper details\n",
      "    print(\"Latest Paper on GPT-4:\")\n",
      "    print(\"Title:\", paper_title)\n",
      "    print(\"Abstract:\", paper_abstract)\n",
      "else:\n",
      "    print(\"No papers found related to GPT-4.\")\n",
      "```\n",
      "\n",
      "Please note that the above code uses the `requests` and `BeautifulSoup` libraries to make an HTTP request to the arXiv API and parse the XML response. Make sure you have these libraries installed (`pip install requests beautifulsoup4`) before running the code.\n",
      "\n",
      "After executing the code, it will print the title and abstract of the latest paper on GPT-4. You can then analyze the abstract to identify potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_gpt-4\u001B[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion, but the code I provided is not executable because it's not in a code block. Here is the correct version:\n",
      "\n",
      "```python\n",
      "# filename: arxiv_search.py\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "soup = BeautifulSoup(response.content, 'html.parser')\n",
      "entry = soup.find('entry')\n",
      "\n",
      "if entry:\n",
      "    paper_title = entry.title.text\n",
      "    paper_abstract = entry.summary.text\n",
      "\n",
      "    # Print the paper details\n",
      "    print(\"Latest Paper on GPT-4:\")\n",
      "    print(\"Title:\", paper_title)\n",
      "    print(\"Abstract:\", paper_abstract)\n",
      "else:\n",
      "    print(\"No papers found related to GPT-4.\")\n",
      "```\n",
      "\n",
      "Please note that the above code uses the `requests` and `BeautifulSoup` libraries to make an HTTP request to the arXiv API and parse the XML response. Make sure you have these libraries installed (`pip install requests beautifulsoup4`) before running the code.\n",
      "\n",
      "After executing the code, it will print the title and abstract of the latest paper on GPT-4. You can then analyze the abstract to identify potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Latest Paper on GPT-4:\n",
      "Title: Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models\n",
      "Abstract:   With LLMs shifting their role from statistical modeling of language to\n",
      "serving as general-purpose AI agents, how should LLM evaluations change?\n",
      "Arguably, a key ability of an AI agent is to flexibly combine, as needed, the\n",
      "basic skills it has learned. The capability to combine skills plays an\n",
      "important role in (human) pedagogy and also in a paper on emergence phenomena\n",
      "(Arora & Goyal, 2023).\n",
      "  This work introduces Skill-Mix, a new evaluation to measure ability to\n",
      "combine skills. Using a list of $N$ skills the evaluator repeatedly picks\n",
      "random subsets of $k$ skills and asks the LLM to produce text combining that\n",
      "subset of skills. Since the number of subsets grows like $N^k$, for even modest\n",
      "$k$ this evaluation will, with high probability, require the LLM to produce\n",
      "text significantly different from any text in the training set. The paper\n",
      "develops a methodology for (a) designing and administering such an evaluation,\n",
      "and (b) automatic grading (plus spot-checking by humans) of the results using\n",
      "GPT-4 as well as the open LLaMA-2 70B model.\n",
      "  Administering a version of to popular chatbots gave results that, while\n",
      "generally in line with prior expectations, contained surprises. Sizeable\n",
      "differences exist among model capabilities that are not captured by their\n",
      "ranking on popular LLM leaderboards (\"cramming for the leaderboard\").\n",
      "Furthermore, simple probability calculations indicate that GPT-4's reasonable\n",
      "performance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior\n",
      "(Bender et al., 2021), i.e., it combines skills in ways that it had not seen\n",
      "during training.\n",
      "  We sketch how the methodology can lead to a Skill-Mix based eco-system of\n",
      "open evaluations for AI capabilities of future models.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mProduct_manager_gpt-4\u001B[0m (to chat_manager):\n",
      "\n",
      "Based on the abstract of the latest paper titled \"Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models\", the potential applications of GPT-4 in software can be inferred as follows:\n",
      "\n",
      "1. **General-Purpose AI Agents**: GPT-4 can serve as a general-purpose AI agent, which means it can be used in a wide range of software applications that require AI capabilities. This includes but is not limited to chatbots, virtual assistants, content generation tools, and more.\n",
      "\n",
      "2. **Combining Skills**: The paper introduces a new evaluation called Skill-Mix to measure the ability to combine skills. This suggests that GPT-4 can be used in software applications where multiple AI skills need to be combined. For example, a software application might require an AI to understand natural language, generate relevant responses, and also exhibit some level of reasoning or decision-making.\n",
      "\n",
      "3. **Automatic Grading**: The paper discusses automatic grading of the results produced by GPT-4. This suggests potential applications in educational software or any other software that requires automatic grading or evaluation of text.\n",
      "\n",
      "4. **Beyond \"Stochastic Parrot\" Behavior**: The paper suggests that GPT-4's performance indicates it goes beyond simply repeating patterns it has seen during training. This implies that GPT-4 could be used in software applications that require novel text generation or creative problem-solving.\n",
      "\n",
      "Please note that these are potential applications inferred from the abstract of the paper. The actual applications could be more specific and varied depending on the detailed content of the paper and the specific requirements of the software.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "administrator = AgentCreator()\n",
    "\n",
    "# create agent; get config (manager); initialize user proxy; task start; clear agent\n",
    "administrator.create_agent('Coder_gpt-3_5', 'gpt-3.5-turbo')\n",
    "administrator.create_agent('Product_manager_gpt-4', 'gpt-4')\n",
    "mg_config, _ = administrator.get_config('gpt-3.5-turbo', seed=41)\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "    human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "\n",
    "task = \"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    "administrator.start(\n",
    "    task,\n",
    "    mg_config[0],\n",
    "    user_proxy=user_proxy,\n",
    "    initiate_agent_name=\"user\"\n",
    ")\n",
    "administrator.clear_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T09:48:33.383593800Z",
     "start_time": "2023-10-29T09:48:32.580332400Z"
    }
   },
   "id": "7d52e3d9a1bf91cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ca05bc74723e163"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
