{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple Agent Chat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1004af6a7fbfcd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement\n",
    "\n",
    "Auto Agent setup need `vllm>=0.2.0` and `fastchat>=0.2.30`, you can use the following command to install the latest version of [vLLM](https://github.com/vllm-project/vllm) and [FastChat](https://github.com/lm-sys/FastChat).\n",
    "```\n",
    "pip install vllm, \"fastchat[model_worker]\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec78dda8e3826d8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install vllm \"fastchat[model_worker]\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e9ae50658be975"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import autogen\n",
    "import time\n",
    "import socket\n",
    "import subprocess as sp\n",
    "from typing import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T13:25:24.206940Z",
     "start_time": "2023-11-03T13:25:24.205018900Z"
    }
   },
   "id": "8d06a99bd05026bb"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class AgentCreator:\n",
    "    \"\"\"\n",
    "    Descriptions\n",
    "    \"\"\"\n",
    "    host: str\n",
    "    task: str\n",
    "    config_path: str\n",
    "    open_ports: List[str] = []\n",
    "    agent_procs: Dict[str, Tuple[sp.Popen, str]] = {}\n",
    "    openai_server_name: str = 'openai'\n",
    "    endpoint_building_timeout: Optional[int]\n",
    "    agent_procs_assign: Dict[str, Tuple[autogen.AssistantAgent, str]] = {}\n",
    "    max_tokens: int = 945\n",
    "\n",
    "    user_proxy: autogen.UserProxyAgent = None\n",
    "    group_chat_manager_config: dict = None\n",
    "    initiate_agent_name: str = 'user'\n",
    "    manager_system_message: str = 'Group chat manager.'\n",
    "\n",
    "    CODING_PROMPT: str = '''Does the following task need programming \n",
    "    (i.e., access external API or tool by coding) to solve?\n",
    "\n",
    "    TASK: {task}\n",
    "\n",
    "    Answer only YES or NO.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            task: str,\n",
    "            host: str = 'localhost',\n",
    "            config_path: str = 'OAI_CONFIG_LIST',\n",
    "            endpoint_building_timeout: Optional[int] = 180\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task: description of a task.\n",
    "            endpoint_building_timeout: timeout for building up an endpoint server.\n",
    "            config_path: path of the OpenAI api configs.\n",
    "            host: endpoint host.\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.endpoint_building_timeout = endpoint_building_timeout\n",
    "        self.config_path = config_path\n",
    "        self.host = host\n",
    "\n",
    "        print('Initializing usable port...')\n",
    "        for port in range(8000, 65535):\n",
    "            if self._is_port_open(host, port):\n",
    "                self.open_ports.append(str(port))\n",
    "        print(f'{len(self.open_ports)} ports found.')\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_port_open(host, port):\n",
    "        \"\"\"Check if a port is open.\"\"\"\n",
    "        try:\n",
    "            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            s.settimeout(10)\n",
    "            s.bind((host, int(port)))\n",
    "            s.close()\n",
    "            return True\n",
    "        except OSError:\n",
    "            return False\n",
    "\n",
    "    def set_task(self, task: str):\n",
    "        self.task = task\n",
    "\n",
    "    def create_agent(\n",
    "            self,\n",
    "            agent_name: str,\n",
    "            model_name_or_hf_repo: str,\n",
    "            llm_config: dict,\n",
    "            system_message: Optional[str] = autogen.AssistantAgent.DEFAULT_SYSTEM_MESSAGE,\n",
    "            world_size: Optional[int] = 1\n",
    "    ) -> autogen.AssistantAgent:\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name that identify the function of the agent (e.g., Coder, Product Manager,...)\n",
    "            model_name_or_hf_repo:\n",
    "            llm_config: specific configs for LLM (e.g., config_list, seed, temperature, ...).\n",
    "            system_message: system prompt use to format an agent's behavior.\n",
    "            world_size: the max size of parallel tensors (in most of the cases, this is identical to the amount of GPUs).\n",
    "\n",
    "        Returns:\n",
    "            agent: a set-up agent.\n",
    "        \"\"\"\n",
    "        config_list = autogen.config_list_from_json(\n",
    "            self.config_path,\n",
    "            filter_dict={\n",
    "                'model': [model_name_or_hf_repo]\n",
    "            }\n",
    "        )\n",
    "        if 'gpt-' in model_name_or_hf_repo:\n",
    "            server_id = self.openai_server_name\n",
    "        else:\n",
    "            model_name = model_name_or_hf_repo.split('/')[-1]\n",
    "            server_id = f'{model_name}_{self.host}'\n",
    "            if self.agent_procs.get(server_id, None) is None:\n",
    "                while True:\n",
    "                    port = self.open_ports.pop()\n",
    "                    if self._is_port_open(self.host, port):\n",
    "                        break\n",
    "\n",
    "                # Use vLLM to set up a server with OpenAI API support.\n",
    "                agent_proc = sp.Popen(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                                       f'--host', f'{self.host}',\n",
    "                                       f'--port', f'{port}',\n",
    "                                       f'--model', f'{model_name_or_hf_repo}',\n",
    "                                       f'--tensor-parallel-size', f'{world_size}'], stdout=sp.PIPE, stderr=sp.STDOUT)\n",
    "                timeout_start = time.time()\n",
    "\n",
    "                while True:\n",
    "                    server_stdout = agent_proc.stdout.readline()\n",
    "                    if server_stdout != b'':\n",
    "                        print(server_stdout)\n",
    "                    timeout_end = time.time()\n",
    "                    if b\"running\" in server_stdout:\n",
    "                        print(\n",
    "                            f'Running {model_name_or_hf_repo} on http://{self.host}:{port} '\n",
    "                            f'with tensor parallel size {world_size}.')\n",
    "                        break\n",
    "                    elif b\"address already in use\" in server_stdout:\n",
    "                        raise RuntimeError(f'{self.host}:{port} already in use. Fail to set up the endpoint for '\n",
    "                                           f'{model_name_or_hf_repo} on {self.host}:{port}.')\n",
    "                    elif timeout_end - timeout_start > self.endpoint_building_timeout:\n",
    "                        raise RuntimeError(f'Timeout exceed. Fail to set up the endpoint for '\n",
    "                                           f'{model_name_or_hf_repo} on {self.host}:{port}.')\n",
    "                self.agent_procs[server_id] = (agent_proc, port)\n",
    "            else:\n",
    "                port = self.agent_procs[server_id][1]\n",
    "\n",
    "            config_list[0]['api_base'] = f'http://{self.host}:{port}/v1'\n",
    "\n",
    "        current_config = llm_config.copy()\n",
    "        current_config.update({\n",
    "            'config_list': config_list,\n",
    "            'model': model_name_or_hf_repo,\n",
    "            'max_tokens': self.max_tokens\n",
    "        })\n",
    "        agent = autogen.AssistantAgent(name=agent_name,\n",
    "                                       llm_config=current_config.copy(),\n",
    "                                       system_message=system_message)\n",
    "        self.agent_procs_assign[agent_name] = (agent, server_id)\n",
    "        return agent\n",
    "\n",
    "    def clear_agent(\n",
    "            self,\n",
    "            agent_name: str = None,\n",
    "            recycle_endpoint: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            agent_name: the name of agent.\n",
    "            recycle_endpoint: trigger for recycle the endpoint server. If true, the endpoint will be recycled\n",
    "                when there is no agent depending on.\n",
    "        \"\"\"\n",
    "        _, server_id = self.agent_procs_assign[agent_name]\n",
    "        del self.agent_procs_assign[agent_name]\n",
    "        if recycle_endpoint:\n",
    "            if server_id == self.openai_server_name:\n",
    "                return\n",
    "            else:\n",
    "                for _, iter_sid in self.agent_procs_assign.values():\n",
    "                    if server_id == iter_sid:\n",
    "                        return\n",
    "                self.agent_procs[server_id][0].terminate()\n",
    "                self.open_ports.append(server_id.split('_')[-1])\n",
    "\n",
    "    def clear_all(self):\n",
    "        \"\"\"\n",
    "        Clear all cached agents.\n",
    "        \"\"\"\n",
    "        for agent_name in [agent_name for agent_name in self.agent_procs_assign.keys()]:\n",
    "            self.clear_agent(agent_name)\n",
    "\n",
    "    def build(\n",
    "            self,\n",
    "            default_llm_config: dict,\n",
    "            coding: bool = None\n",
    "    ):\n",
    "        # TODO: not completed.\n",
    "        config_list = autogen.config_list_from_json(\n",
    "            self.config_path,\n",
    "            filter_dict={\n",
    "                'model': ['gpt-4']\n",
    "            }\n",
    "        )\n",
    "        agent_configs = [('Coder_gpt-35', 'gpt-3.5-turbo'), ('Product_manager', 'gpt-3.5-turbo')]\n",
    "        \n",
    "        for agent_config in agent_configs:\n",
    "            self.create_agent(agent_config[0], agent_config[1], default_llm_config)\n",
    "        \n",
    "        if coding is None:\n",
    "            resp = autogen.ChatCompletion.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": self.CODING_PROMPT.format(task=self.task)}],\n",
    "                **config_list[0]\n",
    "            ).choices[0].message.content\n",
    "            coding = True if resp == 'YES' else False\n",
    "\n",
    "        if coding is True:\n",
    "            self.user_proxy = autogen.UserProxyAgent(\n",
    "                name=\"User_proxy\",\n",
    "                system_message=\"A human admin.\",\n",
    "                code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "                human_input_mode=\"TERMINATE\"\n",
    "            )\n",
    "        else:\n",
    "            self.initiate_agent_name = agent_configs[0][0]\n",
    "        \n",
    "        self.group_chat_manager_config = default_llm_config.copy()\n",
    "        self.group_chat_manager_config['config_list'] = config_list\n",
    "        self.manager_system_message = 'Group chat manager.'\n",
    "\n",
    "    def start(\n",
    "            self,\n",
    "            max_round: Optional[int] = 12,\n",
    "            init_messages: Optional[List[dict]] = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Descriptions\n",
    "\n",
    "        Args:\n",
    "            max_round: the maximum number of rounds.\n",
    "            init_messages: input messages before the task start. This can be the chat history from other group chat\n",
    "                or some preliminary of the task.\n",
    "            initiate_agent_name: the name of an agent use to initialize the group chat.\n",
    "        \"\"\"\n",
    "        agent_list = [agent for agent, _ in self.agent_procs_assign.values()]\n",
    "        if self.user_proxy is not None:\n",
    "            agent_list.append(self.user_proxy)\n",
    "        group_chat = autogen.GroupChat(agents=agent_list, messages=init_messages, max_round=max_round)\n",
    "\n",
    "        manager = autogen.GroupChatManager(groupchat=group_chat,\n",
    "                                           llm_config=self.group_chat_manager_config,\n",
    "                                           system_message=self.manager_system_message)\n",
    "\n",
    "        if self.initiate_agent_name == \"user\" and self.user_proxy is not None:\n",
    "            self.user_proxy.initiate_chat(manager, message=self.task)\n",
    "        else:\n",
    "            for agent in agent_list:\n",
    "                if self.initiate_agent_name == agent.name():\n",
    "                    agent.initiate_chat(manager, message=self.task)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T13:51:43.383138700Z",
     "start_time": "2023-11-03T13:51:43.367423200Z"
    }
   },
   "id": "d729233c9524f875"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test (GPT)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d4b09d29b740bf7"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing usable port...\n",
      "115036 ports found.\n",
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33mCoder_gpt-35\u001B[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to search for papers related to GPT-4. Then, we can analyze the abstract or content of the paper to identify its potential applications in software.\n",
      "\n",
      "Here is the Python code to accomplish this task:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Search for papers related to GPT-4 on arXiv\n",
      "search_query = \"GPT-4\"\n",
      "api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
      "response = requests.get(api_url)\n",
      "\n",
      "# Parse the XML response and extract the paper details\n",
      "xml_data = response.text\n",
      "paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "paper_abstract = xml_data.split(\"<summary>\")[1].split(\"</summary>\")[0]\n",
      "\n",
      "# Print the paper details\n",
      "print(\"Latest Paper on GPT-4:\")\n",
      "print(\"Title:\", paper_title)\n",
      "print(\"Abstract:\", paper_abstract)\n",
      "```\n",
      "\n",
      "Please note that the above code uses the `requests` library to make an HTTP request to the arXiv API and parse the XML response. Make sure you have the `requests` library installed (`pip install requests`) before running the code.\n",
      "\n",
      "After executing the code, it will print the title and abstract of the latest paper on GPT-4. You can then analyze the abstract to identify potential applications in software.\n",
      "\n",
      "Please let me know if you need any further assistance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001B[0m\n",
      "\u001B[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser_proxy\u001B[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 10, in <module>\n",
      "    paper_title = xml_data.split(\"<title>\")[2].split(\"</title>\")[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 11\u001B[0m\n\u001B[1;32m      6\u001B[0m administrator \u001B[38;5;241m=\u001B[39m AgentCreator(\n\u001B[1;32m      7\u001B[0m     task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFind a latest paper about gpt-4 on arxiv and find its potential applications in software.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m     config_path\u001B[38;5;241m=\u001B[39mconfig_path\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     10\u001B[0m administrator\u001B[38;5;241m.\u001B[39mbuild(default_llm_config)\n\u001B[0;32m---> 11\u001B[0m \u001B[43madministrator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m administrator\u001B[38;5;241m.\u001B[39mclear_all()\n",
      "Cell \u001B[0;32mIn[32], line 242\u001B[0m, in \u001B[0;36mAgentCreator.start\u001B[0;34m(self, max_round, init_messages)\u001B[0m\n\u001B[1;32m    237\u001B[0m manager \u001B[38;5;241m=\u001B[39m autogen\u001B[38;5;241m.\u001B[39mGroupChatManager(groupchat\u001B[38;5;241m=\u001B[39mgroup_chat,\n\u001B[1;32m    238\u001B[0m                                    llm_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_chat_manager_config,\n\u001B[1;32m    239\u001B[0m                                    system_message\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanager_system_message)\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitiate_agent_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_proxy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 242\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_proxy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitiate_chat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m agent_list:\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:531\u001B[0m, in \u001B[0;36mConversableAgent.initiate_chat\u001B[0;34m(self, recipient, clear_history, silent, **context)\u001B[0m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001B[39;00m\n\u001B[1;32m    518\u001B[0m \n\u001B[1;32m    519\u001B[0m \u001B[38;5;124;03mReset the consecutive auto reply counter.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001B[39;00m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_chat(recipient, clear_history)\n\u001B[0;32m--> 531\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_init_message\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecipient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:334\u001B[0m, in \u001B[0;36mConversableAgent.send\u001B[0;34m(self, message, recipient, request_reply, silent)\u001B[0m\n\u001B[1;32m    332\u001B[0m valid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_append_oai_message(message, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant\u001B[39m\u001B[38;5;124m\"\u001B[39m, recipient)\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m valid:\n\u001B[0;32m--> 334\u001B[0m     \u001B[43mrecipient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreceive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_reply\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    337\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMessage can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    338\u001B[0m     )\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:462\u001B[0m, in \u001B[0;36mConversableAgent.receive\u001B[0;34m(self, message, sender, request_reply, silent)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request_reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m request_reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreply_at_receive[sender] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 462\u001B[0m reply \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_reply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat_messages\u001B[49m\u001B[43m[\u001B[49m\u001B[43msender\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reply \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(reply, sender, silent\u001B[38;5;241m=\u001B[39msilent)\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:781\u001B[0m, in \u001B[0;36mConversableAgent.generate_reply\u001B[0;34m(self, messages, sender, exclude)\u001B[0m\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_trigger(reply_func_tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m\"\u001B[39m], sender):\n\u001B[0;32m--> 781\u001B[0m     final, reply \u001B[38;5;241m=\u001B[39m \u001B[43mreply_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msender\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreply_func_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfig\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m final:\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m reply\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/groupchat.py:162\u001B[0m, in \u001B[0;36mGroupChatManager.run_chat\u001B[0;34m(self, messages, sender, config)\u001B[0m\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# select the next speaker\u001B[39;00m\n\u001B[0;32m--> 162\u001B[0m     speaker \u001B[38;5;241m=\u001B[39m \u001B[43mgroupchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_speaker\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspeaker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# let the speaker speak\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     reply \u001B[38;5;241m=\u001B[39m speaker\u001B[38;5;241m.\u001B[39mgenerate_reply(sender\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/groupchat.py:91\u001B[0m, in \u001B[0;36mGroupChat.select_speaker\u001B[0;34m(self, last_speaker, selector)\u001B[0m\n\u001B[1;32m     87\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m     88\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGroupChat is underpopulated with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_agents\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m agents. Direct communication would be more efficient.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     89\u001B[0m         )\n\u001B[1;32m     90\u001B[0m selector\u001B[38;5;241m.\u001B[39mupdate_system_message(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect_speaker_msg(agents))\n\u001B[0;32m---> 91\u001B[0m final, name \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_oai_reply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmessages\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRead the above conversation. Then select the next role from \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43m[\u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[38;5;250;43m \u001B[39;49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[38;5;250;43m \u001B[39;49m\u001B[43magent\u001B[49m\u001B[38;5;250;43m \u001B[39;49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[38;5;250;43m \u001B[39;49m\u001B[43magents\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m to play. Only return the role.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m final:\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# i = self._random.randint(0, len(self._agent_names) - 1)  # randomly pick an id\u001B[39;00m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnext_agent(last_speaker, agents)\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/agentchat/conversable_agent.py:606\u001B[0m, in \u001B[0;36mConversableAgent.generate_oai_reply\u001B[0;34m(self, messages, sender, config)\u001B[0m\n\u001B[1;32m    603\u001B[0m     messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_oai_messages[sender]\n\u001B[1;32m    605\u001B[0m \u001B[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001B[39;00m\n\u001B[0;32m--> 606\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43moai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_oai_system_message\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mllm_config\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m, oai\u001B[38;5;241m.\u001B[39mChatCompletion\u001B[38;5;241m.\u001B[39mextract_text_or_function_call(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:803\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001B[0m\n\u001B[1;32m    801\u001B[0m     base_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_retry_period\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 803\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[43m        \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlast\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbase_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    808\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    810\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:834\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001B[0m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m diskcache\u001B[38;5;241m.\u001B[39mCache(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcache_path) \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_cache:\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mset_cache(seed)\n\u001B[0;32m--> 834\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraise_on_ratelimit_or_timeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/LLM/autogen/autogen/oai/completion.py:224\u001B[0m, in \u001B[0;36mCompletion._get_response\u001B[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001B[0m\n\u001B[1;32m    222\u001B[0m         response \u001B[38;5;241m=\u001B[39m openai_completion\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mopenai_completion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    226\u001B[0m     ServiceUnavailableError,\n\u001B[1;32m    227\u001B[0m     APIConnectionError,\n\u001B[1;32m    228\u001B[0m ):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# transient error\u001B[39;00m\n\u001B[1;32m    230\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrying in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_wait_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds...\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    137\u001B[0m ):\n\u001B[1;32m    138\u001B[0m     (\n\u001B[1;32m    139\u001B[0m         deployment_id,\n\u001B[1;32m    140\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m     )\n\u001B[0;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:288\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    279\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    286\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    287\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 288\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_raw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupplied_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response(result, stream)\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/openai/api_requestor.py:596\u001B[0m, in \u001B[0;36mAPIRequestor.request_raw\u001B[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    594\u001B[0m     _thread_context\u001B[38;5;241m.\u001B[39msession_create_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 596\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_thread_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m        \u001B[49m\u001B[43mabs_url\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mTIMEOUT_SECS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_thread_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    607\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mTimeout(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRequest timed out: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(e)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/requests/adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    483\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_proxy(conn)\n\u001B[1;32m    713\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[0;32m--> 714\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[1;32m    725\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[1;32m    726\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n\u001B[1;32m    728\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    461\u001B[0m             httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[1;32m    462\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    463\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    464\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    465\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m--> 466\u001B[0m             \u001B[43msix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;66;03m# Python 3\u001B[39;00m\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 461\u001B[0m         httplib_response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    463\u001B[0m         \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    464\u001B[0m         \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    465\u001B[0m         \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m    466\u001B[0m         six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1377\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.10/ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "config_path = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST'\n",
    "default_llm_config = {\n",
    "    'temperature': 0\n",
    "}\n",
    "\n",
    "administrator = AgentCreator(\n",
    "    task=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    config_path=config_path\n",
    ")\n",
    "administrator.build(default_llm_config)\n",
    "administrator.start()\n",
    "administrator.clear_all()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T13:52:10.449037900Z",
     "start_time": "2023-11-03T13:52:04.890203400Z"
    }
   },
   "id": "7d52e3d9a1bf91cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ca05bc74723e163"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
