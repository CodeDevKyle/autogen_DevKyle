{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pftZ-ZF1_BA"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPUGFpKP1_BH"
   },
   "source": [
    "# Demonstrating the `AgentEval` framework using the task of solving math problems as an example\n",
    "\n",
    "This notebook aims to demonstrate how to `AgentEval` implemented through [AutoGen](https://github.com/microsoft/autogen) works, where we use a math problem-solving task as an example. \n",
    "`AgentEval` consists of two key components:\n",
    "\n",
    "- `CriticAgent`: This is an LLM-based agent that generates a list criteria $(c_1, \\dots, c_n)$ to help to evaluate a utility given task.\n",
    "\n",
    "- `QuantifierAgent`: This agent quantifies the performance of any sample task based on the criteria designed by the `CriticAgent` in the following way: $(c_1=a_1, \\dots, c_n=a_n)$\n",
    "\n",
    "![AgentEval](../website/blog/2023-11-20-AgentEval/img/agenteval-CQ.png)\n",
    "\n",
    "For more detailed explanations, please refer to the accompanying [blog post](https://microsoft.github.io/autogen/blog/2023/11/20/AgentEval)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install pyautogen, Docker, and OpenAI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
     "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
     "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
     "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
    },
    "id": "68lTZZyJ1_BI",
    "outputId": "15a55fab-e13a-4654-b8cb-ae117478d6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyautogen>=0.2.3 in /home/vscode/.local/lib/python3.10/site-packages (0.2.17)\n",
      "Requirement already satisfied: docker in /home/vscode/.local/lib/python3.10/site-packages (7.0.0)\n",
      "Requirement already satisfied: tiktoken in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen>=0.2.3) (0.6.0)\n",
      "Requirement already satisfied: termcolor in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen>=0.2.3) (2.4.0)\n",
      "Requirement already satisfied: diskcache in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen>=0.2.3) (5.6.3)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen>=0.2.3) (2.6.4)\n",
      "Requirement already satisfied: openai>=1.3 in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen>=0.2.3) (1.14.1)\n",
      "Requirement already satisfied: python-dotenv in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen>=0.2.3) (1.0.1)\n",
      "Requirement already satisfied: flaml in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen>=0.2.3) (2.1.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/site-packages (from docker) (2.2.1)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/site-packages (from docker) (24.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/site-packages (from docker) (2.31.0)\n",
      "Requirement already satisfied: sniffio in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (4.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (0.27.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (4.66.2)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/vscode/.local/lib/python3.10/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen>=0.2.3) (2.16.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/vscode/.local/lib/python3.10/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen>=0.2.3) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->docker) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->docker) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->docker) (3.3.2)\n",
      "Requirement already satisfied: NumPy>=1.17 in /home/vscode/.local/lib/python3.10/site-packages (from flaml->pyautogen>=0.2.3) (1.26.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/vscode/.local/lib/python3.10/site-packages (from tiktoken->pyautogen>=0.2.3) (2023.12.25)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/vscode/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen>=0.2.3) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen>=0.2.3) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/vscode/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen>=0.2.3) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/vscode/.local/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /home/vscode/.local/lib/python3.10/site-packages (from scipy) (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/vscode/.local/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pyautogen>=0.2.3\" docker\n",
    "%pip install scipy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxgqKJrd1_BJ"
   },
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "* The [`config_list_openai_aoai`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_openai_aoai) function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
    "  - OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
    "  - Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
    "  - Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
    "* The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file. It first looks for an environment variable with a specified name. The value of the environment variable needs to be a valid json string. If that variable is not found, it looks for a json file with the same name. It filters the configs by filter_dict.\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb) for full code examples of the different methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YRycFEDJ1_BJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import autogen\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"autogen\", \"agentchat\", \"contrib\")))\n",
    "from critic_agent import CriticAgent\n",
    "from quantifier_agent import QuantifierAgent\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"autogen\", \"agenteval\")))\n",
    "from agent_eval import generate_criteria, quantify_criteria\n",
    "from criterion import Criterion\n",
    "from task import Task\n",
    "from test_case import TestCase\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt4\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBZ-XFXy1_BJ"
   },
   "source": [
    "\n",
    "## Construct `CriticAgent`\n",
    "\n",
    "We construct the planning agent named `critic` and a user proxy agent for the critic named `critic_user`. We specify `human_input_mode` as \"NEVER\" in the user proxy agent, ensuring that it will never ask for human feedback. Additionally, we define the `ask_critic` function to send a message to the critic and retrieve the criteria from the critic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vPTtNkhk2V1"
   },
   "source": [
    "# Run the Critic\n",
    "\n",
    "To run the critic, we need a couple of math problem examples. One of them failed to solve the problem successfully, given in `agenteval-in-out/response_failed.txt`, and the other one was solved successfully, i.e., `agenteval-in-out/response_successful.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5H1WRs_wkiK0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcritic_user\u001b[0m (to critic):\n",
      "\n",
      "Task: Math problem solving.\n",
      "    Task description: Given any question, the system needs to solve the problem as consisely and accurately as possible\n",
      "    Task successful example: {'problem': 'What is the sum of all the distinct positive two-digit factors of 144?', 'level': 'Level 5', 'type': 'Number Theory', 'solution': 'Prime factorize $144=2^4\\\\cdot3^2$. The sum of the positive two-digit factors of 144 is $2^4+2\\\\cdot3^2+2^2\\\\cdot3+2^2\\\\cdot3^2+2^3\\\\cdot3+2^3\\\\cdot3^2+2^4\\\\cdot3=\\\\boxed{226}.$', 'problem_id': '0', 'response_with_ans': 'To find the sum of all the distinct positive two-digit factors of 144, we need to first find all these factors. We can do this by iterating through the numbers from 10 to 99 and checking if they are factors of 144. Then, we can sum these factors and print their sum.\\n\\nHere\\'s a Python script to accomplish this:\\n\\n```python\\ntwo_digit_factors = []\\n\\nfor i in range(10, 100):\\n    if 144 % i == 0:\\n        two_digit_factors.append(i)\\n\\nsum_of_factors = sum(two_digit_factors)\\nprint(\"The sum of all the distinct positive two-digit factors of 144 is:\", sum_of_factors)\\n```\\n\\nPlease run this script to find the desired sum.', 'round': 0, 'messages': [{'content': 'What is the sum of all the distinct positive two-digit factors of 144?', 'role': 'user'}, {'content': 'To find the sum of all the distinct positive two-digit factors of 144, we need to first find all these factors. We can do this by iterating through the numbers from 10 to 99 and checking if they are factors of 144. Then, we can sum these factors and print their sum.\\n\\nHere\\'s a Python script to accomplish this:\\n\\n```python\\ntwo_digit_factors = []\\n\\nfor i in range(10, 100):\\n    if 144 % i == 0:\\n        two_digit_factors.append(i)\\n\\nsum_of_factors = sum(two_digit_factors)\\nprint(\"The sum of all the distinct positive two-digit factors of 144 is:\", sum_of_factors)\\n```\\n\\nPlease run this script to find the desired sum.', 'role': 'assistant'}], 'time': 11.140539407730103, 'trial': -1}\n",
      "    Task failed example: {'problem': 'Find all $x$ that satisfy the inequality $(2x+10)(x+3)<(3x+9)(x+8)$. Express your answer in interval notation.', 'level': 'Level 5', 'type': 'Algebra', 'solution': 'We have \\\\begin{align*} (2x+10)(x+3)&<(3x+9)(x+8) \\\\quad \\\\Rightarrow\\n\\\\\\\\ 2(x+5)(x+3)&<3(x+3)(x+8) \\\\quad \\\\Rightarrow\\n\\\\\\\\ 2(x+5)(x+3)-3(x+3)(x+8)&<0 \\\\quad \\\\Rightarrow\\n\\\\\\\\ (2x+10-(3x+24))(x+3)&<0 \\\\quad \\\\Rightarrow\\n\\\\\\\\ (-x-14)(x+3)&<0 \\\\quad \\\\Rightarrow\\n\\\\\\\\ (x+14)(x+3)&>0.\\n\\\\end{align*} This inequality is satisfied if and only if $(x+14)$ and $(x+3)$ are either both positive or both negative.  Both factors are positive for $x>-3$ and both factors are negative for $x<-14$.  When $-14<x<-3$, one factor is positive and the other negative, so their product is negative.   Therefore, the range of $x$ that satisfies the inequality is $ \\\\boxed{(-\\\\infty, -14)\\\\cup(-3,\\\\infty)} $.', 'problem_id': '0', 'response_with_ans': \"I apologize for the confusion in my previous message. Let's find the factorized form of the quadratic using Python code.\\n\\n```python\\nfrom sympy import symbols, factor\\r\\nx = symbols('x')\\r\\nfactorized = factor(x**2 + 17*x + 42)\\r\\nfactorized\\r\\n```\\n\\nPlease run the Python code above to find the factorized form of the quadratic, and we can proceed to determine the intervals where the inequality holds true.\", 'round': 1, 'messages': [{'content': 'Find all $x$ that satisfy the inequality $(2x+10)(x+3)<(3x+9)(x+8)$. Express your answer in interval notation.', 'role': 'user'}, {'content': \"First, let's simplify and isolate the inequality to solve for $x$. For this, we need to expand the expressions on both sides and then move everything to one side.\\n\\nStep 1: Expand the expressions on both sides\\n$(2x+10)(x+3) = 2x^2 + 6x + 10x + 30 = 2x^2 + 16x + 30$\\n$(3x+9)(x+8) = 3x^2 + 24x + 9x + 72 = 3x^2 + 33x + 72$\\n\\nStep 2: Move everything to one side\\n$0 < 3x^2 + 33x + 72 - (2x^2 + 16x + 30) = x^2 + 17x + 42$\\n\\nNow we need to find the factorized form of the quadratic inequality $x^2 + 17x + 42 > 0$ and then find intervals where the inequality holds true.\\n\\nLet's first find the factorized form of the quadratic:\", 'role': 'assistant'}, {'content': '', 'role': 'user'}, {'content': \"I apologize for the confusion in my previous message. Let's find the factorized form of the quadratic using Python code.\\n\\n```python\\nfrom sympy import symbols, factor\\r\\nx = symbols('x')\\r\\nfactorized = factor(x**2 + 17*x + 42)\\r\\nfactorized\\r\\n```\\n\\nPlease run the Python code above to find the factorized form of the quadratic, and we can proceed to determine the intervals where the inequality holds true.\", 'role': 'assistant'}], 'time': 24.91333508491516, 'trial': -1}\n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcritic\u001b[0m (to critic_user):\n",
      "\n",
      "{\n",
      "    \"Accuracy\": {\n",
      "        \"description\": \"The correctness of the problem solving steps and solution\",\n",
      "        \"accepted_values\": [\"0 - Incorrect\", \"1 - Partially correct\", \"2 - Mostly correct\", \"3 - Completely correct\"]\n",
      "    },\n",
      "    \"Efficiency\": {\n",
      "        \"description\": \"The conciseness and optimization of the provided solution\",\n",
      "        \"accepted_values\": [\"0 - Not efficient\", \"1 - Somewhat efficient\", \"2 - Fairly efficient\", \"3 - Highly efficient\"]\n",
      "    },\n",
      "    \"Clarity\": {\n",
      "        \"description\": \"The understandability of the explanation or responses\",\n",
      "        \"accepted_values\": [\"0 - Not clear\", \"1 - Somewhat clear\", \"2 - Fairly clear\", \"3 - Very clear\"]\n",
      "    },\n",
      "    \"Completeness\": {\n",
      "        \"description\": \"The measure of how well the problem is covered\",\n",
      "        \"accepted_values\": [\"0 - Incomplete\", \"1 - Partially complete\", \"2 - Mostly complete\", \"3 - Fully complete\"]\n",
      "    },\n",
      "    \"Correct Use of Math Concepts and Formulas\": {\n",
      "        \"description\": \"The correctness in applying mathematical concepts and formulas to the problem\",\n",
      "        \"accepted_values\": [\"0 - Incorrect use\", \"1 - Partially correct use\", \"2 - Mostly correct use\", \"3 - Fully correct use\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Reading one successful and one failed example of the task\n",
    "success_str = open(\"../test/test_files/agenteval-in-out/samples/sample_math_response_successful.txt\", \"r\").read()\n",
    "response_successful = TestCase.parse_json_str(success_str).test_details\n",
    "failed_str = open(\"../test/test_files/agenteval-in-out/samples/sample_math_response_failed.txt\", \"r\").read()\n",
    "response_failed = TestCase.parse_json_str(failed_str).test_details\n",
    "\n",
    "task = Task.parse_json_str(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"name\": \"Math problem solving\",\n",
    "            \"description\": \"Given any question, the system needs to solve the problem as consisely and accurately as possible\",\n",
    "            \"successful_response\": response_successful,\n",
    "            \"failed_response\": response_failed,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "criteria = generate_criteria(task=task, llm_config={\"config_list\": config_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vu70o024lenI"
   },
   "source": [
    "# The Criteria\n",
    "Now, we print the designed criteria for assessing math problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9DsDB5hqvtG",
    "outputId": "0edd7a0c-b031-4f67-efc6-1a1e77066921"
   },
   "outputs": [],
   "source": [
    "current_task_name = \"_\".join(task.name.split()).lower()\n",
    "cr_file = open(f\"../test/test_files/agenteval-in-out/{current_task_name}_criteria.json\", \"w\")\n",
    "cr_file.write(Criterion.write_json(criteria))\n",
    "cr_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PETPZluOEGCR"
   },
   "source": [
    "*Note :* You can also define and use your own criteria by editing `criteria.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmpUZv_ylo9U"
   },
   "source": [
    "# The `QuantifierAgent`\n",
    "\n",
    "Once we have the criteria, we need to quantify a new sample based on the designed criteria and its accepted values. This will be done through `QuantifierAgent` agent as follows. \n",
    "We note that can skip the designed criteria by the agent and use your own defined criteria in `criteria_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4uUkZJh_subA"
   },
   "outputs": [],
   "source": [
    "criteria_file = f\"../test/test_files/agenteval-in-out/{current_task_name}_criteria.json\"\n",
    "criteria = open(criteria_file, \"r\").read()\n",
    "criteria = Criterion.parse_json_str(criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64rRJfB2l6lO"
   },
   "source": [
    "## Running the quantifier on a single test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we run the quantifier on a single math problem test case, `sample_test_case.json`, for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pf623aNbHZTG",
    "outputId": "0031871b-a438-43f5-d2b2-c99fa1ad0dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mquantifier_user\u001b[0m (to quantifier):\n",
      "\n",
      "Task: Math problem solving.\n",
      "    Task description: Given any question, the system needs to solve the problem as consisely and accurately as possible\n",
      "    Task successful example: {'problem': 'What is the sum of all the distinct positive two-digit factors of 144?', 'level': 'Level 5', 'type': 'Number Theory', 'solution': 'Prime factorize $144=2^4\\\\cdot3^2$. The sum of the positive two-digit factors of 144 is $2^4+2\\\\cdot3^2+2^2\\\\cdot3+2^2\\\\cdot3^2+2^3\\\\cdot3+2^3\\\\cdot3^2+2^4\\\\cdot3=\\\\boxed{226}.$', 'problem_id': '0', 'response_with_ans': 'To find the sum of all the distinct positive two-digit factors of 144, we need to first find all these factors. We can do this by iterating through the numbers from 10 to 99 and checking if they are factors of 144. Then, we can sum these factors and print their sum.\\n\\nHere\\'s a Python script to accomplish this:\\n\\n```python\\ntwo_digit_factors = []\\n\\nfor i in range(10, 100):\\n    if 144 % i == 0:\\n        two_digit_factors.append(i)\\n\\nsum_of_factors = sum(two_digit_factors)\\nprint(\"The sum of all the distinct positive two-digit factors of 144 is:\", sum_of_factors)\\n```\\n\\nPlease run this script to find the desired sum.', 'round': 0, 'messages': [{'content': 'What is the sum of all the distinct positive two-digit factors of 144?', 'role': 'user'}, {'content': 'To find the sum of all the distinct positive two-digit factors of 144, we need to first find all these factors. We can do this by iterating through the numbers from 10 to 99 and checking if they are factors of 144. Then, we can sum these factors and print their sum.\\n\\nHere\\'s a Python script to accomplish this:\\n\\n```python\\ntwo_digit_factors = []\\n\\nfor i in range(10, 100):\\n    if 144 % i == 0:\\n        two_digit_factors.append(i)\\n\\nsum_of_factors = sum(two_digit_factors)\\nprint(\"The sum of all the distinct positive two-digit factors of 144 is:\", sum_of_factors)\\n```\\n\\nPlease run this script to find the desired sum.', 'role': 'assistant'}], 'time': 11.140539407730103, 'trial': -1}\n",
      "    Task failed example: {'problem': 'Find all $x$ that satisfy the inequality $(2x+10)(x+3)<(3x+9)(x+8)$. Express your answer in interval notation.', 'level': 'Level 5', 'type': 'Algebra', 'solution': 'We have \\\\begin{align*} (2x+10)(x+3)&<(3x+9)(x+8) \\\\quad \\\\Rightarrow\\n\\\\\\\\ 2(x+5)(x+3)&<3(x+3)(x+8) \\\\quad \\\\Rightarrow\\n\\\\\\\\ 2(x+5)(x+3)-3(x+3)(x+8)&<0 \\\\quad \\\\Rightarrow\\n\\\\\\\\ (2x+10-(3x+24))(x+3)&<0 \\\\quad \\\\Rightarrow\\n\\\\\\\\ (-x-14)(x+3)&<0 \\\\quad \\\\Rightarrow\\n\\\\\\\\ (x+14)(x+3)&>0.\\n\\\\end{align*} This inequality is satisfied if and only if $(x+14)$ and $(x+3)$ are either both positive or both negative.  Both factors are positive for $x>-3$ and both factors are negative for $x<-14$.  When $-14<x<-3$, one factor is positive and the other negative, so their product is negative.   Therefore, the range of $x$ that satisfies the inequality is $ \\\\boxed{(-\\\\infty, -14)\\\\cup(-3,\\\\infty)} $.', 'problem_id': '0', 'response_with_ans': \"I apologize for the confusion in my previous message. Let's find the factorized form of the quadratic using Python code.\\n\\n```python\\nfrom sympy import symbols, factor\\r\\nx = symbols('x')\\r\\nfactorized = factor(x**2 + 17*x + 42)\\r\\nfactorized\\r\\n```\\n\\nPlease run the Python code above to find the factorized form of the quadratic, and we can proceed to determine the intervals where the inequality holds true.\", 'round': 1, 'messages': [{'content': 'Find all $x$ that satisfy the inequality $(2x+10)(x+3)<(3x+9)(x+8)$. Express your answer in interval notation.', 'role': 'user'}, {'content': \"First, let's simplify and isolate the inequality to solve for $x$. For this, we need to expand the expressions on both sides and then move everything to one side.\\n\\nStep 1: Expand the expressions on both sides\\n$(2x+10)(x+3) = 2x^2 + 6x + 10x + 30 = 2x^2 + 16x + 30$\\n$(3x+9)(x+8) = 3x^2 + 24x + 9x + 72 = 3x^2 + 33x + 72$\\n\\nStep 2: Move everything to one side\\n$0 < 3x^2 + 33x + 72 - (2x^2 + 16x + 30) = x^2 + 17x + 42$\\n\\nNow we need to find the factorized form of the quadratic inequality $x^2 + 17x + 42 > 0$ and then find intervals where the inequality holds true.\\n\\nLet's first find the factorized form of the quadratic:\", 'role': 'assistant'}, {'content': '', 'role': 'user'}, {'content': \"I apologize for the confusion in my previous message. Let's find the factorized form of the quadratic using Python code.\\n\\n```python\\nfrom sympy import symbols, factor\\r\\nx = symbols('x')\\r\\nfactorized = factor(x**2 + 17*x + 42)\\r\\nfactorized\\r\\n```\\n\\nPlease run the Python code above to find the factorized form of the quadratic, and we can proceed to determine the intervals where the inequality holds true.\", 'role': 'assistant'}], 'time': 24.91333508491516, 'trial': -1}\n",
      "    Evaluation dictionary: {\"Accuracy\": {\"description\": \"The correctness of the problem solving steps and solution\", \"accepted_values\": [\"0 - Incorrect\", \"1 - Partially correct\", \"2 - Mostly correct\", \"3 - Completely correct\"]}, \"Efficiency\": {\"description\": \"The conciseness and optimization of the provided solution\", \"accepted_values\": [\"0 - Not efficient\", \"1 - Somewhat efficient\", \"2 - Fairly efficient\", \"3 - Highly efficient\"]}, \"Clarity\": {\"description\": \"The understandability of the explanation or responses\", \"accepted_values\": [\"0 - Not clear\", \"1 - Somewhat clear\", \"2 - Fairly clear\", \"3 - Very clear\"]}, \"Completeness\": {\"description\": \"The measure of how well the problem is covered\", \"accepted_values\": [\"0 - Incomplete\", \"1 - Partially complete\", \"2 - Mostly complete\", \"3 - Fully complete\"]}, \"Correct Use of Math Concepts and Formulas\": {\"description\": \"The correctness in applying mathematical concepts and formulas to the problem\", \"accepted_values\": [\"0 - Incorrect use\", \"1 - Partially correct use\", \"2 - Mostly correct use\", \"3 - Fully correct use\"]}}actual test case to evaluate: {'problem': 'Find $24^{-1} \\\\pmod{11^2}$. That is, find the residue $b$ for which $24b \\\\equiv 1\\\\pmod{11^2}$.\\n\\nExpress your answer as an integer from $0$ to $11^2-1$, inclusive.', 'level': 'Level 5', 'type': 'Number Theory', 'solution': 'Since $5 \\\\times 24 = 120 = 121 - 1$, it follows that $-5 \\\\times 24 \\\\equiv 1 \\\\pmod{121}$. Adding 121 to $-5$ to make it positive, we find $(-5 + 121) \\\\times 24 \\\\equiv 116 \\\\times 24 \\\\equiv 1 \\\\pmod{121}$, so it follows that the modular inverse of $24$ is $\\\\boxed{116}$ when taken modulo $121$.', 'problem_id': '5', 'response_with_ans': 'To find the modular inverse of 24 modulo 11^2, we can use the Extended Euclidean Algorithm. Here is a Python function to compute the modular inverse using this algorithm:\\n\\n```python\\ndef mod_inverse(a, m):\\n    g, x, _ = extended_gcd(a, m)\\n    if g != 1:\\n        raise Exception(f\"{a} and {m} are not coprime.\")\\n    return x % m\\n\\ndef extended_gcd(a, b):\\n    if a == 0:\\n        return b, 0, 1\\n    else:\\n        g, x, y = extended_gcd(b % a, a)\\n        return g, y - (b // a) * x, x\\n```\\n\\nLet\\'s use the above function to find the modular inverse of 24 modulo 11^2:\\n\\n```python\\na = 24\\nm = 11**2\\nmod_inverse(a, m)\\n```\\n\\nI will execute the above code to find the modular inverse of 24 modulo 11^2.', 'round': 0, 'messages': [{'content': 'Find $24^{-1} \\\\pmod{11^2}$. That is, find the residue $b$ for which $24b \\\\equiv 1\\\\pmod{11^2}$.\\n\\nExpress your answer as an integer from $0$ to $11^2-1$, inclusive.', 'role': 'user'}, {'content': 'To find the modular inverse of 24 modulo 11^2, we can use the Extended Euclidean Algorithm. Here is a Python function to compute the modular inverse using this algorithm:\\n\\n```python\\ndef mod_inverse(a, m):\\n    g, x, _ = extended_gcd(a, m)\\n    if g != 1:\\n        raise Exception(f\"{a} and {m} are not coprime.\")\\n    return x % m\\n\\ndef extended_gcd(a, b):\\n    if a == 0:\\n        return b, 0, 1\\n    else:\\n        g, x, y = extended_gcd(b % a, a)\\n        return g, y - (b // a) * x, x\\n```\\n\\nLet\\'s use the above function to find the modular inverse of 24 modulo 11^2:\\n\\n```python\\na = 24\\nm = 11**2\\nmod_inverse(a, m)\\n```\\n\\nI will execute the above code to find the modular inverse of 24 modulo 11^2.', 'role': 'assistant'}], 'time': 13.481226921081543, 'trial': -1}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mquantifier\u001b[0m (to quantifier_user):\n",
      "\n",
      "{\"Accuracy\": {\"description\": \"The correctness of the problem solving steps and solution\", \"accepted_values\": \"3 - Completely correct\"}, \"Efficiency\": {\"description\": \"The conciseness and optimization of the provided solution\", \"accepted_values\": \"3 - Highly efficient\"}, \"Clarity\": {\"description\": \"The understandability of the explanation or responses\", \"accepted_values\": \"3 - Very clear\"}, \"Completeness\": {\"description\": \"The measure of how well the problem is covered\", \"accepted_values\": \"3 - Fully complete\"}, \"Correct Use of Math Concepts and Formulas\": {\"description\": \"The correctness in applying mathematical concepts and formulas to the problem\", \"accepted_values\": \"3 - Fully correct use\"}}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "actual correctness: True\n",
      "predicted coprrectness:\n",
      " {\"Accuracy\": {\"description\": \"The correctness of the problem solving steps and solution\", \"accepted_values\": \"3 - Completely correct\"}, \"Efficiency\": {\"description\": \"The conciseness and optimization of the provided solution\", \"accepted_values\": \"3 - Highly efficient\"}, \"Clarity\": {\"description\": \"The understandability of the explanation or responses\", \"accepted_values\": \"3 - Very clear\"}, \"Completeness\": {\"description\": \"The measure of how well the problem is covered\", \"accepted_values\": \"3 - Fully complete\"}, \"Correct Use of Math Concepts and Formulas\": {\"description\": \"The correctness in applying mathematical concepts and formulas to the problem\", \"accepted_values\": \"3 - Fully correct use\"}}\n"
     ]
    }
   ],
   "source": [
    "test_case = open(\"../test/test_files/agenteval-in-out/samples/sample_test_case.json\", \"r\").read()\n",
    "quantifier_output = quantify_criteria(\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    criteria=criteria,\n",
    "    task=task,\n",
    "    test_case=TestCase.parse_json_str(test_case),\n",
    ")\n",
    "print(\"actual correctness:\", quantifier_output[\"actual_success\"])\n",
    "print(\"predicted coprrectness:\\n\", quantifier_output[\"estimated_performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VtdM44WEGCS"
   },
   "source": [
    "# Run `AgentEval` on the logs\n",
    "\n",
    "In the example below, log_path points to the sample logs folder to run the quantifier. The current sample belongs to the prealgebra category which will be downloaded from [here](https://github.com/julianakiseleva/autogen/tree/agenteval/test/test_files/agenteval-in-out/samples).\n",
    "In case you want to replicate the results described in the blog post, you can download all the logs for math problems using the following [link](https://github.com/julianakiseleva/autogen/tree/agenteval/model-logs/math-problems/agentchat). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-27 18:04:36--  https://github.com/julianakiseleva/autogen/raw/ddabd4f0e7c13a50e33cf8462e79358666371477/test/test_files/agenteval-in-out/prealgebra.zip\n",
      "Resolving github.com (github.com)... 20.29.134.23\n",
      "Connecting to github.com (github.com)|20.29.134.23|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/julianakiseleva/autogen/ddabd4f0e7c13a50e33cf8462e79358666371477/test/test_files/agenteval-in-out/prealgebra.zip [following]\n",
      "--2024-03-27 18:04:36--  https://raw.githubusercontent.com/julianakiseleva/autogen/ddabd4f0e7c13a50e33cf8462e79358666371477/test/test_files/agenteval-in-out/prealgebra.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28567 (28K) [application/zip]\n",
      "Saving to: ‘prealgebra.zip’\n",
      "\n",
      "prealgebra.zip      100%[===================>]  27.90K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-03-27 18:04:36 (18.8 MB/s) - ‘prealgebra.zip’ saved [28567/28567]\n",
      "\n",
      "Archive:  prealgebra.zip\n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/\n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/9.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/9.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/16.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/16.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/8.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/8.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/15.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/15.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/6.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/6.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/3.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/3.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/4.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/4.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/18.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/18.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/1.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/1.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/14.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/14.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/2.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/2.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/10.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/10.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/7.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/7.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/log.txt\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/log.txt  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/13.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/13.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/17.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/17.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/11.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/11.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/12.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/12.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/0.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/0.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/19.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/19.json  \n",
      "warning:  skipped \"../\" path component(s) in ../prealgebra/5.json\n",
      "  inflating: ../test/test_files/agenteval-in-out/agentchat_results/prealgebra/5.json  \n"
     ]
    }
   ],
   "source": [
    "# You can set your own log path - we also limited the number of samples to avoid additional costs.\n",
    "# By removing the condition about limitations on the number of samples per category, you can run it on all 120 problems\n",
    "\n",
    "log_path = \"../test/test_files/agenteval-in-out/agentchat_results/\"\n",
    "\n",
    "# The file is no longer in the repo, we can download it from an older commit\n",
    "!wget https://github.com/julianakiseleva/autogen/raw/ddabd4f0e7c13a50e33cf8462e79358666371477/test/test_files/agenteval-in-out/prealgebra.zip\n",
    "!unzip -o prealgebra.zip -d {log_path}\n",
    "!rm  prealgebra.zip\n",
    "\n",
    "assert Path(log_path).exists(), f\"The log path '{log_path}' does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZdIbHPFEGCS",
    "outputId": "83c0a51b-f184-494b-81a0-d4b4a3667319"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TestCase' has no attribute 'create_from_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         gameid \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file_name\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m             test_case \u001b[38;5;241m=\u001b[39m \u001b[43mTestCase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_file\u001b[49m(log_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file_name)\n\u001b[1;32m     10\u001b[0m             quantifier_output \u001b[38;5;241m=\u001b[39m quantify_criteria(\n\u001b[1;32m     11\u001b[0m                 llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list}, criteria\u001b[38;5;241m=\u001b[39mcriteria, task\u001b[38;5;241m=\u001b[39mtask, test_case\u001b[38;5;241m=\u001b[39mtest_case\n\u001b[1;32m     12\u001b[0m             )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# store the evaluated problems\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TestCase' has no attribute 'create_from_file'"
     ]
    }
   ],
   "source": [
    "criteria_file = \"../test/test_files/agenteval-in-out/samples/sample_math_criteria.json\"\n",
    "criteria = Criterion.parse_json_str(open(criteria_file, \"r\").read())\n",
    "outcome = {}\n",
    "\n",
    "for prefix in os.listdir(log_path):\n",
    "    for file_name in os.listdir(log_path + \"/\" + prefix):\n",
    "        gameid = prefix + \"_\" + file_name\n",
    "        if file_name.split(\".\")[-1] == \"json\":\n",
    "            test_case = TestCase.parse_json_str(open(log_path + \"/\" + prefix + \"/\" + file_name, \"r\").read())\n",
    "            quantifier_output = quantify_criteria(\n",
    "                llm_config={\"config_list\": config_list}, criteria=criteria, task=task, test_case=test_case\n",
    "            )\n",
    "\n",
    "# store the evaluated problems\n",
    "with open(\"../test/test_files/agenteval-in-out/evaluated_problems.json\", \"w\") as file:\n",
    "    json.dump(outcome, file, indent=2)  # use `json.loads` to do the reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbrRRiP_EGCT"
   },
   "source": [
    "## Plotting the estimated performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find an example of how to visualize the obtained result in the histogram form (similar to the one in the blog post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKu2xZJcEGCT",
    "outputId": "7780bc7c-382f-4ad3-b8c6-ac6051302303"
   },
   "outputs": [],
   "source": [
    "# computing average and 95% interval for failed and successful cases on all criteria\n",
    "try:\n",
    "    # convert the criteria to dict type if it is already not\n",
    "    dictionary_for_eval = eval(open(criteria_file, \"r\").read())\n",
    "except:  # noqa: E722\n",
    "    pass\n",
    "\n",
    "criteria = list(dictionary_for_eval.keys())\n",
    "nl2int = {}\n",
    "for criterion in dictionary_for_eval:\n",
    "    score = 0\n",
    "    for v in dictionary_for_eval[criterion][\"accepted_values\"]:\n",
    "        nl2int[v] = score\n",
    "        score += 1\n",
    "print(nl2int)\n",
    "\n",
    "average_s = {}\n",
    "average_f = {}\n",
    "\n",
    "conf_interval_s = {}\n",
    "conf_interval_f = {}\n",
    "\n",
    "for criterion in criteria:\n",
    "    task = {\"s\": [], \"f\": []}\n",
    "\n",
    "    for game in outcome:\n",
    "        try:\n",
    "            tmp_dic = eval(outcome[game][\"estimated_performance\"])\n",
    "            if outcome[game][\"actual_success\"] == \"false\":\n",
    "                task[\"f\"].append(nl2int[tmp_dic[criterion]])\n",
    "            else:\n",
    "                task[\"s\"].append(nl2int[tmp_dic[criterion]])\n",
    "        except:  # noqa: E722\n",
    "            pass\n",
    "\n",
    "    average_f[criterion] = np.mean(task[\"f\"])\n",
    "    average_s[criterion] = np.mean(task[\"s\"])\n",
    "\n",
    "    conf_interval_s[criterion] = stats.norm.interval(0.95, loc=np.mean(task[\"s\"]), scale=stats.sem(task[\"s\"]))\n",
    "    conf_interval_f[criterion] = stats.norm.interval(0.95, loc=np.mean(task[\"f\"]), scale=stats.sem(task[\"f\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final plot would be saved in `../test/test_files/agenteval-in-out/estimated_performance.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "zqa86vwgEGCT",
    "outputId": "248cd0bc-0927-4d9f-b911-088bd76acf5d"
   },
   "outputs": [],
   "source": [
    "# Create a bar plot with error bars for the average values of \"s\" and \"f\" for each criterion\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bar_width = 0.1\n",
    "index = np.arange(len(criteria))\n",
    "\n",
    "\n",
    "plt.bar(\n",
    "    index,\n",
    "    list(average_s.values()),\n",
    "    bar_width,\n",
    "    label=f\"success ({len(task['s'])} samples)\",\n",
    "    color=\"darkblue\",\n",
    "    yerr=[(avg - conf_interval_s[key][0]) for key, avg in average_s.items()],\n",
    "    capsize=5,\n",
    ")\n",
    "plt.bar(\n",
    "    index + bar_width,\n",
    "    list(average_f.values()),\n",
    "    bar_width,\n",
    "    label=f\"failed ({len(task['f'])} samples)\",\n",
    "    color=\"lightblue\",\n",
    "    yerr=[(avg - conf_interval_f[key][0]) for key, avg in average_f.items()],\n",
    "    capsize=5,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Criteria\", fontsize=16)\n",
    "plt.ylabel(\"Average Value\", fontsize=16)\n",
    "plt.title(\n",
    "    \"Average Values of 3 different baselines cases with 95% Confidence Intervals - math problems \", fontsize=12, pad=10\n",
    ")  # Adjust titlepad to move the title further above\n",
    "plt.xticks(index + bar_width / 2, criteria, rotation=45, fontsize=14)\n",
    "plt.legend(loc=\"upper center\", fontsize=14, bbox_to_anchor=(0.5, 1), ncol=3)  # Adjust legend placement and ncol\n",
    "plt.tight_layout()  # Adjust subplot parameters to fit the labels\n",
    "plt.ylim(0, 5)\n",
    "plt.savefig(\"../test/test_files/agenteval-in-out/estimated_performance.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
