{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pftZ-ZF1_BA"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1G4nqqcp-MZvxO75mVKuOm6TNIQ7eTmuw?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPUGFpKP1_BH"
      },
      "source": [
        "# Demonstrating the `AgentEval` framework using the task of solving math problems as an example\n",
        "\n",
        "This notebook aims to demonstrate how to `AgentEval` implemented through [AutoGen](https://github.com/microsoft/autogen) works, where we use a math problem-solving task as an example. \n",
        "`AgentEval` consists of two key components:\n",
        "\n",
        "- `CriticAgent`: This is an LLM-based agent that generates a list criteria $(c_1, \\dots, c_n)$ to help to evaluate a utility given task.\n",
        "\n",
        "- `QuantifierAgent`: This agent quantifies the performance of any sample task based on the criteria designed by the `CriticAgent` in the following way: $(c_1=a_1, \\dots, c_n=a_n)$\n",
        "\n",
        "![AgentEval](../website/blog/2023-11-11-AgentEval/img/agenteval-CQ.png)\n",
        "\n",
        "For more detailed explanations, please refer to the accompanying [blog post](https://https://microsoft.github.io/autogen/blog/2023/11/11/AgentEval)\n",
        "\n",
        "## Requirements\n",
        "\n",
        "AutoGen requires `Python>=3.8`. To run this notebook example, please install pyautogen, Docker, and OpenAI:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
          "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
          "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
          "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
        },
        "id": "68lTZZyJ1_BI",
        "outputId": "15a55fab-e13a-4654-b8cb-ae117478d6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyautogen~=0.2.0b5 in /home/negar/.local/lib/python3.8/site-packages (0.2.0b5)\n",
            "Requirement already satisfied: docker in /home/negar/.local/lib/python3.8/site-packages (6.1.3)\n",
            "Requirement already satisfied: diskcache in /home/negar/.local/lib/python3.8/site-packages (from pyautogen~=0.2.0b5) (5.6.3)\n",
            "Requirement already satisfied: flaml in /home/negar/.local/lib/python3.8/site-packages (from pyautogen~=0.2.0b5) (2.1.1)\n",
            "Requirement already satisfied: openai~=1.2 in /home/negar/.local/lib/python3.8/site-packages (from pyautogen~=0.2.0b5) (1.2.4)\n",
            "Requirement already satisfied: python-dotenv in /home/negar/.local/lib/python3.8/site-packages (from pyautogen~=0.2.0b5) (1.0.0)\n",
            "Requirement already satisfied: termcolor in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from pyautogen~=0.2.0b5) (2.3.0)\n",
            "Requirement already satisfied: tiktoken in /home/negar/.local/lib/python3.8/site-packages (from pyautogen~=0.2.0b5) (0.5.1)\n",
            "Requirement already satisfied: packaging>=14.0 in /home/negar/.local/lib/python3.8/site-packages (from docker) (21.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /home/negar/.local/lib/python3.8/site-packages (from docker) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /home/negar/.local/lib/python3.8/site-packages (from docker) (2.1.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /home/negar/.local/lib/python3.8/site-packages (from docker) (1.6.4)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /home/negar/.local/lib/python3.8/site-packages (from openai~=1.2->pyautogen~=0.2.0b5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/negar/.local/lib/python3.8/site-packages (from openai~=1.2->pyautogen~=0.2.0b5) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/negar/.local/lib/python3.8/site-packages (from openai~=1.2->pyautogen~=0.2.0b5) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/negar/.local/lib/python3.8/site-packages (from openai~=1.2->pyautogen~=0.2.0b5) (2.5.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from openai~=1.2->pyautogen~=0.2.0b5) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/negar/.local/lib/python3.8/site-packages (from openai~=1.2->pyautogen~=0.2.0b5) (4.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from packaging>=14.0->docker) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/negar/.local/lib/python3.8/site-packages (from requests>=2.26.0->docker) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from requests>=2.26.0->docker) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from requests>=2.26.0->docker) (2023.7.22)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/negar/.local/lib/python3.8/site-packages (from flaml->pyautogen~=0.2.0b5) (1.24.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/negar/.local/lib/python3.8/site-packages (from tiktoken->pyautogen~=0.2.0b5) (2022.1.18)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/negar/.local/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen~=0.2.0b5) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /home/negar/.local/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen~=0.2.0b5) (1.1.3)\n",
            "Requirement already satisfied: httpcore in /home/negar/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai~=1.2->pyautogen~=0.2.0b5) (1.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/negar/.local/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai~=1.2->pyautogen~=0.2.0b5) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.1 in /home/negar/.local/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai~=1.2->pyautogen~=0.2.0b5) (2.14.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/negar/.local/lib/python3.8/site-packages (from httpcore->httpx<1,>=0.23.0->openai~=1.2->pyautogen~=0.2.0b5) (0.14.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: openai in /home/negar/.local/lib/python3.8/site-packages (1.2.4)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /home/negar/.local/lib/python3.8/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/negar/.local/lib/python3.8/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/negar/.local/lib/python3.8/site-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/negar/.local/lib/python3.8/site-packages (from openai) (2.5.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/negar/.local/lib/python3.8/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/negar/.local/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /home/negar/.local/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /home/negar/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/negar/.local/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.1 in /home/negar/.local/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/negar/.local/lib/python3.8/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/negar/.local/lib/python3.8/site-packages (from scipy) (1.24.4)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (3.7.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in /home/negar/.local/lib/python3.8/site-packages (from matplotlib) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/negar/.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from matplotlib) (9.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/negar/.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/negar/anaconda3/envs/alf/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyautogen~=0.2.0b5 docker\n",
        "%pip install openai\n",
        "%pip install scipy\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxgqKJrd1_BJ"
      },
      "source": [
        "## Set your API Endpoint\n",
        "\n",
        "* The [`config_list_openai_aoai`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_openai_aoai) function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
        "  - OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
        "  - Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
        "  - Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
        "* The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file. It first looks for an environment variable with a specified name. The value of the environment variable needs to be a valid json string. If that variable is not found, it looks for a json file with the same name. It filters the configs by filter_dict.\n",
        "\n",
        "It\"s OK to have only the OpenAI API key, or only the Azure OpenAI API key + base. If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choosing \"upload file\" icon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YRycFEDJ1_BJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:The specified config_list file 'OAI_CONFIG_LIST' does not exist.\n"
          ]
        }
      ],
      "source": [
        "import autogen\n",
        "\n",
        "config_list = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gpt-4\"],\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBZ-XFXy1_BJ"
      },
      "source": [
        "\n",
        "## Construct `CriticAgent`\n",
        "\n",
        "We construct the planning agent named `critic` and a user proxy agent for the critic named `critic_user`. We specify `human_input_mode` as \"NEVER\" in the user proxy agent, ensuring that it will never ask for human feedback. Additionally, we define the `ask_critic` function to send a message to the critic and retrieve the criteria from the critic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9XAeyjd11_BK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[autogen.oai.client: 11-17 14:10:46] {74} WARNING - openai client was provided with an empty config_list, which may not be intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:autogen.oai.client:openai client was provided with an empty config_list, which may not be intended.\n"
          ]
        },
        {
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m critic \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39;49mAssistantAgent(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcritic\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     llm_config \u001b[39m=\u001b[39;49m {\u001b[39m\"\u001b[39;49m\u001b[39mconfig_list\u001b[39;49m\u001b[39m\"\u001b[39;49m: config_list},\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     system_message \u001b[39m=\u001b[39;49m \u001b[39m\"\"\"\u001b[39;49m\u001b[39mYou are a helpful assistant. You suggest criteria for evaluating different tasks. They should be dinstinguishable, quantifieable and not redundant.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m    Convert the evaluation criteria into a dictionary where the keys are the criteria.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m    The value of each key is a dictionary as follows \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdescription\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m: criteria description , \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39maccepted_values\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m: possible accepted inputs for this key}\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m    Make sure the keys are criteria for assessing the given task.  \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39maccepted_values\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m include the acceptable inputs for each key that are fine-grained and preferrably mlti-graded levels. \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdescription\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m includes the criterion description.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m    Return the dictionary.\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m critic_user \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mUserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcritic_user\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     max_consecutive_auto_reply \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,  \u001b[39m# terminate without auto-reply\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     human_input_mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNEVER\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.117.3.96/home/negar/testnegar/autogen-agenteval/notebook/agenteval_cq_math.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mask_critic\u001b[39m(message):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/autogen/agentchat/assistant_agent.py:57\u001b[0m, in \u001b[0;36mAssistantAgent.__init__\u001b[0;34m(self, name, system_message, llm_config, is_termination_msg, max_consecutive_auto_reply, human_input_mode, code_execution_config, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     39\u001b[0m ):\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m        name (str): agent name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m            [ConversableAgent](conversable_agent#__init__).\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     58\u001b[0m         name,\n\u001b[1;32m     59\u001b[0m         system_message,\n\u001b[1;32m     60\u001b[0m         is_termination_msg,\n\u001b[1;32m     61\u001b[0m         max_consecutive_auto_reply,\n\u001b[1;32m     62\u001b[0m         human_input_mode,\n\u001b[1;32m     63\u001b[0m         code_execution_config\u001b[39m=\u001b[39;49mcode_execution_config,\n\u001b[1;32m     64\u001b[0m         llm_config\u001b[39m=\u001b[39;49mllm_config,\n\u001b[1;32m     65\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     66\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:115\u001b[0m, in \u001b[0;36mConversableAgent.__init__\u001b[0;34m(self, name, system_message, is_termination_msg, max_consecutive_auto_reply, human_input_mode, function_map, code_execution_config, llm_config, default_auto_reply)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm_config, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    114\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_config\u001b[39m.\u001b[39mupdate(llm_config)\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m OpenAIWrapper(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_config)\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_code_execution_config \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m code_execution_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m code_execution_config\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuman_input_mode \u001b[39m=\u001b[39m human_input_mode\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/autogen/oai/client.py:83\u001b[0m, in \u001b[0;36mOpenAIWrapper.__init__\u001b[0;34m(self, config_list, **base_config)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_list \u001b[39m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m         {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopenai_kwargs}}\n\u001b[1;32m     80\u001b[0m         \u001b[39mfor\u001b[39;00m config \u001b[39min\u001b[39;00m config_list\n\u001b[1;32m     81\u001b[0m     ]\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clients \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client(extra_kwargs, openai_config)]\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_list \u001b[39m=\u001b[39m [extra_kwargs]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/autogen/oai/client.py:138\u001b[0m, in \u001b[0;36mOpenAIWrapper._client\u001b[0;34m(self, config, openai_config)\u001b[0m\n\u001b[1;32m    136\u001b[0m openai_config \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopenai_config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopenai_kwargs}}\n\u001b[1;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_for_azure(openai_config, config)\n\u001b[0;32m--> 138\u001b[0m client \u001b[39m=\u001b[39m OpenAI(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopenai_config)\n\u001b[1;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m client\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "\n",
        "critic = autogen.AssistantAgent(\n",
        "    name = \"critic\",\n",
        "    llm_config = {\"config_list\": config_list},\n",
        "    system_message = \"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be dinstinguishable, quantifieable and not redundant.\n",
        "    Convert the evaluation criteria into a dictionary where the keys are the criteria.\n",
        "    The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key}\n",
        "    Make sure the keys are criteria for assessing the given task.  \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferrably mlti-graded levels. \"description\" includes the criterion description.\n",
        "    Return the dictionary.\"\"\"\n",
        ")\n",
        "\n",
        "critic_user = autogen.UserProxyAgent(\n",
        "    name = \"critic_user\",\n",
        "    max_consecutive_auto_reply = 0,  # terminate without auto-reply\n",
        "    human_input_mode = \"NEVER\",\n",
        ")\n",
        "\n",
        "def ask_critic(message):\n",
        "    \"\"\"\n",
        "    Initiate a chat with the critic user and return the last message received from the planner.\n",
        "\n",
        "    Args:\n",
        "    - message (str): The message to be sent to the critic user.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the last message received.\n",
        "    \"\"\"\n",
        "    critic_user.initiate_chat(critic, message=message)\n",
        "    # return the last received from the planner\n",
        "    return critic_user.messagelast_message()[\"content\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vPTtNkhk2V1"
      },
      "source": [
        "# Run the Critic\n",
        "\n",
        "To run the critic, we need a couple of math problem examples. One of them failed to solve the problem successfully, given in `agenteval-in-out/response_failed.txt`, and the other one was solved successfully, i.e., `agenteval-in-out/response_successful.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H1WRs_wkiK0"
      },
      "outputs": [],
      "source": [
        "def read_without_groundtruth(file_name):\n",
        "    \"\"\"\n",
        "    Read the mathproblem logs - bypassing any information about the ground truths.\n",
        "\n",
        "    Args:\n",
        "    - file_name (str): The single log file that wants to get evaluated.\n",
        "\n",
        "    Returns:\n",
        "    - str: The log file without any information about the ground truth answer of the problem.\n",
        "    \"\"\"\n",
        "    f  = open( file_name,\"r\").readlines()\n",
        "    output_dictionary = \"\"\n",
        "    for line in f:\n",
        "          if \"is_correct\" not in line and \"correct_ans\" not in line and  \"check_result\"  not in line:\n",
        "              output_dictionary += line\n",
        "          elif \"is_correct\" in line:\n",
        "                correctness = line.replace(\",\",\"\").split(\":\")[-1].rstrip().strip()\n",
        "    return [output_dictionary,correctness]\n",
        "\n",
        "# Reading one successful and one failed example of the task\n",
        "response_successful = read_without_groundtruth(\"../test/test_files/agenteval-in-out/samples/sample_math_response_successful.txt\")[0]\n",
        "response_failed = read_without_groundtruth(\"../test/test_files/agenteval-in-out/samples/sample_math_response_failed.txt\")[0]\n",
        "\n",
        "task = {\"name\": \"Math problem solving\",\n",
        "      \"description\": \"Given any question, the system needs to  solve the problem as consisely and  acccurately as possible\",\n",
        "      \"successful_response\" : response_successful,\n",
        "      \"failed_response\" : response_failed}\n",
        "\n",
        "sys_msg = f\"\"\"Task: {task[\"name\"]}.\n",
        "Task description: {task[\"description\"]}\n",
        "Task successfull example: {task[\"successful_response\"]}\n",
        "Task failed example: {task[\"failed_response\"]}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu70o024lenI"
      },
      "source": [
        "# The Criteria\n",
        "Now, we print the designed criteria for assessing math problems. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9DsDB5hqvtG",
        "outputId": "0edd7a0c-b031-4f67-efc6-1a1e77066921"
      },
      "outputs": [],
      "source": [
        "\n",
        "current_task_name = '_'.join(task[\"name\"].split()).lower()\n",
        "gen_criteria = critic_user.initiate_chat(critic, message=sys_msg)\n",
        "criteria = critic_user.last_message()\n",
        "cr_file = open(f\"../test/test_files/agenteval-in-out/{current_task_name}_criteria.json\",\"w\")\n",
        "cr_file.write(criteria[\"content\"])\n",
        "cr_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PETPZluOEGCR"
      },
      "source": [
        "*Note :* You can also define and use your own criteria by editing `criteria.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmpUZv_ylo9U"
      },
      "source": [
        "# The `QuantifierAgent`\n",
        "\n",
        "Once we have the criteria, we need to quantify a new sample based on the designed criteria and its accepted values. This will be done through `QuantifierAgent` agent as follows. \n",
        "We note that can skip the designed creteria by the agent and use your own defined criteria in `criteria_file`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uUkZJh_subA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "criteria_file = f\"../test/test_files/agenteval-in-out/{current_task_name}_criteria.json\"\n",
        "quantifier = autogen.AssistantAgent(\n",
        "    name = \"quantifier\",\n",
        "    llm_config = {\"config_list\": config_list},\n",
        "    system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria.\n",
        "    The criterion is given in a dictionary format where each key is a dintinct criteria.\n",
        "    The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key}\n",
        "    You are going to quantify each of the crieria for a given task based on the task decription.\n",
        "    Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.\n",
        "    Return only the dictionary.\"\"\"\n",
        ")\n",
        "\n",
        "quantifier_user = autogen.UserProxyAgent(\n",
        "    name = \"quantifier_user\",\n",
        "    max_consecutive_auto_reply = 0,  # terminate without auto-reply\n",
        "    human_input_mode = \"NEVER\",\n",
        ")\n",
        "\n",
        "dictionary_for_eval = open(criteria_file,\"r\").read()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64rRJfB2l6lO"
      },
      "source": [
        "## Running the quantifier on a single test case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ0H3sy8l-Ai"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_quantifier(file,criteria_file):\n",
        "  \"\"\"\n",
        "  Running quantifier agent on individual log.\n",
        "\n",
        "  Args:\n",
        "  - file (str): The log path.\n",
        "  - file (str): The criteria jason file path\n",
        "  Returns:\n",
        "  - dict: A dictionary including the actual success of the problem as well as estimated performance by the agent eval.\n",
        "  {\"actual_success\":actual_label, \"estimated_performance\" : a dictionary of all the criteria and their quantified estimated performance.} }\n",
        "  \"\"\"\n",
        "  dictionary_for_eval = open(criteria_file,\"r\").read()\n",
        "\n",
        "  test_case , actual_label = read_without_groundtruth(file)\n",
        "  print(\"actual label for this case: \", actual_label)\n",
        "  cq_results = quantifier_user.initiate_chat(quantifier, message = sys_msg + \\\n",
        "                                \"Evaluation dictionary: \" + str(dictionary_for_eval) +\\\n",
        "                                \"actual test case to evaluate: \" + test_case)\n",
        "  quantified_results = quantifier_user.last_message()\n",
        "  return {\"actual_success\": actual_label, \"estimated_performance\": quantified_results[\"content\"]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we run the quantifier on a single math problem test case, `sample_test_case.json`, for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf623aNbHZTG",
        "outputId": "0031871b-a438-43f5-d2b2-c99fa1ad0dbd"
      },
      "outputs": [],
      "source": [
        "test_case = \"../test/test_files/agenteval-in-out/samples/sample_test_case.json\"\n",
        "quantifier_output = get_quantifier(test_case,criteria_file)\n",
        "print(\"actual correctness:\" , quantifier_output[\"actual_success\"])\n",
        "print(\"predicted coprrectness:\\n\" , quantifier_output[\"estimated_performance\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VtdM44WEGCS"
      },
      "source": [
        "# Run `AgentEval` on the logs\n",
        "\n",
        "In the example below, log_path points to the sample logs folder to run the quantifier. The current sample belongs to the prealgebra category.\n",
        "In case you want to replicate the results described in the blog post, you can download all the logs for math problems using the following [link](https://github.com/julianakiseleva/autogen/tree/agenteval/model-logs/math-problems/agentchat). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://github.com/julianakiseleva/autogen/blob/agenteval/test/test_files/agenteval-in-out/samples/prealgebra.zip \n",
        "!unzip z.zip -d ../test/test_files/agenteval-in-out/agentchat_results\n",
        "!rm  https://github.com/julianakiseleva/autogen/blob/agenteval/test/test_files/agenteval-in-out/samples/prealgebra.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZdIbHPFEGCS",
        "outputId": "83c0a51b-f184-494b-81a0-d4b4a3667319"
      },
      "outputs": [],
      "source": [
        "# You can set your own log path - we also limited the number of samples to avoid additional costs.\n",
        "# By removing the condition about limitations on the number of samples per category, you can run it on all 120 problems\n",
        "\n",
        "log_path = \"../test/test_files/agenteval-in-out/agentchat_results/\"\n",
        "criteria_file = \"../test/test_files/agenteval-in-out/samples/sample_math_criteria.json\"\n",
        "outcome = {}\n",
        "\n",
        "for prefix in os.listdir(log_path):\n",
        "    for file_name in os.listdir(log_path+\"/\"+prefix):\n",
        "        gameid = prefix+\"_\"+file_name\n",
        "        if file_name.split('.')[-1]=='json':\n",
        "            outcome[gameid] = get_quantifier(log_path+\"/\"+prefix+\"/\"+file_name,criteria_file)\n",
        "\n",
        "# store the evaluated problems\n",
        "with open(\"../test/test_files/agenteval-in-out/evaluated_problems.json\",\"w\") as file:\n",
        "    json.dump(outcome,file,indent=2) # use `json.loads` to do the reverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbrRRiP_EGCT"
      },
      "source": [
        "## Plotting the estimated performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here you can find an example of how to visualize the obtained result in the histogram form (similar to the one in the blog post)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKu2xZJcEGCT",
        "outputId": "7780bc7c-382f-4ad3-b8c6-ac6051302303"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# computing average and 95% interval for failed and successful cases on all criteria\n",
        "try:\n",
        "  # convert the criteria to dict type if it is already not\n",
        "        dictionary_for_eval = eval(open(criteria_file,'r').read())\n",
        "except:\n",
        "       pass\n",
        "\n",
        "criteria = list(dictionary_for_eval.keys())\n",
        "nl2int = {}\n",
        "for criterion in dictionary_for_eval:\n",
        "     score = 0\n",
        "     for v in dictionary_for_eval[criterion][\"accepted_values\"]:\n",
        "        nl2int[v] = score\n",
        "        score += 1\n",
        "print(nl2int)\n",
        "\n",
        "average_s = {}\n",
        "average_f = {}\n",
        "\n",
        "conf_interval_s = {}\n",
        "conf_interval_f = {}\n",
        "\n",
        "for criterion in criteria:\n",
        "        task={\"s\": [] , \"f\" : []}\n",
        "\n",
        "        for game in outcome:\n",
        "                try:\n",
        "                        tmp_dic = eval(outcome[game][\"estimated_performance\"])\n",
        "                        if outcome[game][\"actual_success\"] == \"false\":\n",
        "                                task[\"f\"].append(nl2int[tmp_dic[criterion]])\n",
        "                        else:\n",
        "                                task[\"s\"].append(nl2int[tmp_dic[criterion]])\n",
        "                except:\n",
        "                        pass\n",
        "\n",
        "          \n",
        "        average_f[criterion] = np.mean(task['f'])\n",
        "        average_s[criterion] = np.mean(task['s'])\n",
        "\n",
        "        conf_interval_s[criterion] = stats.norm.interval(0.95, loc=np.mean(task['s']), scale=stats.sem(task['s']))\n",
        "        conf_interval_f[criterion] = stats.norm.interval(0.95, loc=np.mean(task['f']), scale=stats.sem(task['f']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final plot would be saved in `../test/test_files/agenteval-in-out/estimated_performance.png`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "zqa86vwgEGCT",
        "outputId": "248cd0bc-0927-4d9f-b911-088bd76acf5d"
      },
      "outputs": [],
      "source": [
        "# Create a bar plot with error bars for the average values of \"s\" and \"f\" for each criterion\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "bar_width = 0.1\n",
        "index = np.arange(len(criteria))\n",
        "\n",
        "\n",
        "plt.bar(index , list(average_s.values()), bar_width, label=f\"success ({len(task['s'])} samples)\", color=\"darkblue\", yerr=[(avg - conf_interval_s[key][0]) for key, avg in average_s.items()], capsize=5)\n",
        "plt.bar(index + bar_width, list(average_f.values()), bar_width, label=f\"failed ({len(task['f'])} samples)\", color=\"lightblue\", yerr=[(avg - conf_interval_f[key][0]) for key, avg in average_f.items()], capsize=5)\n",
        "\n",
        "plt.xlabel(\"Criteria\", fontsize=16)\n",
        "plt.ylabel(\"Average Value\", fontsize=16)\n",
        "plt.title(\"Average Values of 3 different baselines cases with 95% Confidence Intervals - math problems \", fontsize=12, pad=10)  # Adjust titlepad to move the title further above\n",
        "plt.xticks(index + bar_width / 2, criteria, rotation=45, fontsize=14)\n",
        "plt.legend(loc=\"upper center\", fontsize=14, bbox_to_anchor=(0.5, 1), ncol=3)  # Adjust legend placement and ncol\n",
        "plt.tight_layout()  # Adjust subplot parameters to fit the labels\n",
        "plt.ylim(0,5)\n",
        "plt.savefig(\"../test/test_files/agenteval-in-out/estimated_performance.png\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
