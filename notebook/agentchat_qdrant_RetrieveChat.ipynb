{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "# Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering\n",
    "\n",
    "[Qdrant](https://qdrant.tech/) is a high-performance vector search engine/database.\n",
    "\n",
    "This notebook demonstrates the usage of `QdrantRetrieveUserProxyAgent` for RAG, based on [agentchat_RetrieveChat.ipynb](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb).\n",
    "\n",
    "\n",
    "RetrieveChat is a conversational system for retrieve augmented code generation and question answering. In this notebook, we demonstrate how to utilize RetrieveChat to generate code and answer questions based on customized documentations that are not present in the LLM's training dataset. RetrieveChat uses the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`, which is similar to the usage of `AssistantAgent` and `UserProxyAgent` in other notebooks (e.g., [Automated Task Solving with Code Generation, Execution & Debugging](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)).\n",
    "\n",
    "We'll demonstrate usage of RetrieveChat with Qdrant for code generation and question answering w/ human feedback.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [retrievechat] option.\n",
    "```bash\n",
    "pip install \"pyautogen[retrievechat]\" \"flaml[automl]\" \"qdrant_client[fastembed]\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-3.5-turbo']\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-35-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 and gpt-3.5-turbo models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted file formats for `docs_path`:\n",
      "['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\n"
     ]
    }
   ],
   "source": [
    "# Accepted file formats for that can be stored in \n",
    "# a vector database instance\n",
    "from autogen.retrieve_utils import TEXT_FORMATS\n",
    "\n",
    "print(\"Accepted file formats for `docs_path`:\")\n",
    "print(TEXT_FORMATS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct agents for RetrieveChat\n",
    "\n",
    "We start by initialzing the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `QdrantRetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a retrieval augmented generation task for an initial prompt to be sent to the LLM assistant.\n",
    "\n",
    "### You can find the list of all the embedding models supported by Qdrant [here](https://qdrant.github.io/fastembed/examples/Supported_Models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import QdrantRetrieveUserProxyAgent\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\", \n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the QdrantRetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "# By default, the human_input_mode is \"ALWAYS\", which means the agent will ask for human input at every step. We set it to \"NEVER\" here.\n",
    "# `docs_path` is the path to the docs directory. By default, it is set to \"./docs\". Here we generated the documentations from FLAML's docstrings.\n",
    "# Navigate to the website folder and run `pydoc-markdown` and it will generate folder `reference` under `website/docs`.\n",
    "# `task` indicates the kind of task we're working on. In this example, it's a `code` task.\n",
    "# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\n",
    "# We use an in-memory QdrantClient instance here. Not recommended for production.\n",
    "# Get the installation instructions here: https://qdrant.tech/documentation/guides/installation/\n",
    "ragproxyagent = QdrantRetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": \"../website/docs/reference\",\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": QdrantClient(\":memory:\"),\n",
    "        \"embedding_model\": \"BAAI/bge-small-en-v1.5\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-1\"></a>\n",
    "### Example 1\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer a question and ask for human-in-loop feedbacks.\n",
    "\n",
    "Problem: Is there a function named `tune_automl` in FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAdding doc_id 0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 15 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Is there a function named `tune_automl` in FLAML?\n",
      "\n",
      "Context is: {\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"items\": [\n",
      "        {\n",
      "          \"items\": [\n",
      "            \"reference/agentchat/contrib/math_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_assistant_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/teachable_agent\",\n",
      "            \"reference/agentchat/contrib/text_analyzer_agent\"\n",
      "          ],\n",
      "          \"label\": \"agentchat.contrib\",\n",
      "          \"type\": \"category\"\n",
      "        },\n",
      "        \"reference/agentchat/agent\",\n",
      "        \"reference/agentchat/assistant_agent\",\n",
      "        \"reference/agentchat/conversable_agent\",\n",
      "        \"reference/agentchat/groupchat\",\n",
      "        \"reference/agentchat/user_proxy_agent\"\n",
      "      ],\n",
      "      \"label\": \"agentchat\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    {\n",
      "      \"items\": [\n",
      "        \"reference/oai/completion\",\n",
      "        \"reference/oai/openai_utils\"\n",
      "      ],\n",
      "      \"label\": \"oai\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    \"reference/code_utils\",\n",
      "    \"reference/math_utils\",\n",
      "    \"reference/retrieve_utils\"\n",
      "  ],\n",
      "  \"label\": \"Reference\",\n",
      "  \"type\": \"category\"\n",
      "}\n",
      "---\n",
      "sidebar_label: qdrant_retrieve_user_proxy_agent\n",
      "title: agentchat.contrib.qdrant_retrieve_user_proxy_agent\n",
      "---\n",
      "\n",
      "## QdrantRetrieveUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent)\n",
      "```\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"RetrieveChatAgent\",\n",
      "             human_input_mode: str | None = \"ALWAYS\",\n",
      "             is_termination_msg: Callable[[Dict], bool] | None = None,\n",
      "             retrieve_config: Dict | None = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `retrieve_config` _dict or None_ - config for the retrieve agent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",
      "  - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n",
      "  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n",
      "  - client (Optional, qdrant_client.QdrantClient(\":memory:\")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.\n",
      "  will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n",
      "  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n",
      "  or the url to a single file. Default is None, which works only if the collection is already created.\n",
      "  - collection_name (Optional, str): the name of the collection.\n",
      "  If key not provided, a default name `autogen-docs` will be used.\n",
      "  - model (Optional, str): the model to use for the retrieve chat.\n",
      "  If key not provided, a default model `gpt-4` will be used.\n",
      "  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.4` will be used.\n",
      "  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.8` will be used.\n",
      "  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n",
      "  \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n",
      "  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n",
      "  If chunk_mode is \"one_line\", this parameter will be ignored.\n",
      "  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n",
      "  If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used. All available models\n",
      "  can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.\n",
      "  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n",
      "  - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n",
      "  If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n",
      "  - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n",
      "  - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n",
      "  The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).\n",
      "  Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.\n",
      "  - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n",
      "  Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n",
      "  - parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.\n",
      "  - on_disk (Optional, bool): Whether to store the collection on disk. Default is False.\n",
      "  - quantization_config: Quantization configuration. If None, quantization will be disabled.\n",
      "  - hnsw_config: HNSW configuration. If None, default configuration will be used.\n",
      "  - payload_indexing: Whether to create a payload index for the document field. Default is False.\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "\n",
      "#### retrieve\\_docs\n",
      "\n",
      "```python\n",
      "@override\n",
      "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "\n",
      "#### create\\_qdrant\\_from\\_dir\n",
      "\n",
      "```python\n",
      "def create_qdrant_from_dir(\n",
      "        dir_path: str,\n",
      "        max_tokens: int = 4000,\n",
      "        client: QdrantClient = None,\n",
      "        collection_name: str = \"all-my-documents\",\n",
      "        chunk_mode: str = \"multi_lines\",\n",
      "        must_break_at_empty_line: bool = True,\n",
      "        embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n",
      "        custom_text_split_function: Callable = None,\n",
      "        parallel: int = 0,\n",
      "        on_disk: bool = False,\n",
      "        quantization_config: Optional[models.QuantizationConfig] = None,\n",
      "        hnsw_config: Optional[models.HnswConfigDiff] = None,\n",
      "        payload_indexing: bool = False,\n",
      "        qdrant_client_options: Optional[Dict] = {})\n",
      "```\n",
      "\n",
      "Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a url to\n",
      "a single file.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `dir_path` _str_ - the path to the directory, file or url.\n",
      "- `max_tokens` _Optional, int_ - the maximum number of tokens per chunk. Default is 4000.\n",
      "- `client` _Optional, QdrantClient_ - the QdrantClient instance. Default is None.\n",
      "- `collection_name` _Optional, str_ - the name of the collection. Default is \"all-my-documents\".\n",
      "- `chunk_mode` _Optional, str_ - the chunk mode. Default is \"multi_lines\".\n",
      "- `must_break_at_empty_line` _Optional, bool_ - Whether to break at empty line. Default is True.\n",
      "- `embedding_model` _Optional, str_ - the embedding model to use. Default is \"BAAI/bge-small-en-v1.5\". The list of all the available models can be at https://qdrant.github.io/fastembed/examples/Supported_Models/.\n",
      "- `parallel` _Optional, int_ - How many parallel workers to use for embedding. Defaults to the number of CPU cores\n",
      "- `on_disk` _Optional, bool_ - Whether to store the collection on disk. Default is False.\n",
      "- `quantization_config` - Quantization configuration. If None, quantization will be disabled. Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n",
      "- `hnsw_config` - HNSW configuration. If None, default configuration will be used. Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n",
      "- `payload_indexing` - Whether to create a payload index for the document field. Default is False.\n",
      "- `qdrant_client_options` - (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.\n",
      "\n",
      "#### query\\_qdrant\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32mUpdating context and resetting conversation.\u001b[0m\n",
      "\u001b[32mAdding doc_id 13 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 14 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 12 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Is there a function named `tune_automl` in FLAML?\n",
      "\n",
      "Context is: ---\n",
      "sidebar_label: retrieve_user_proxy_agent\n",
      "title: agentchat.contrib.retrieve_user_proxy_agent\n",
      "---\n",
      "\n",
      "## RetrieveUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class RetrieveUserProxyAgent(UserProxyAgent)\n",
      "```\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"RetrieveChatAgent\",\n",
      "             human_input_mode: Optional[str] = \"ALWAYS\",\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             retrieve_config: Optional[Dict] = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `retrieve_config` _dict or None_ - config for the retrieve agent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",
      "  - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n",
      "  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n",
      "  - client (Optional, chromadb.Client): the chromadb client. If key not provided, a default client `chromadb.Client()`\n",
      "  will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n",
      "  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n",
      "  or the url to a single file. Default is None, which works only if the collection is already created.\n",
      "  - collection_name (Optional, str): the name of the collection.\n",
      "  If key not provided, a default name `autogen-docs` will be used.\n",
      "  - model (Optional, str): the model to use for the retrieve chat.\n",
      "  If key not provided, a default model `gpt-4` will be used.\n",
      "  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.4` will be used.\n",
      "  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.8` will be used.\n",
      "  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n",
      "  \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n",
      "  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n",
      "  If chunk_mode is \"one_line\", this parameter will be ignored.\n",
      "  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n",
      "  If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available models\n",
      "  can be found at `https://www.sbert.net/docs/pretrained_models.html`. The default model is a\n",
      "  fast model. If you want to use a high performance model, `all-mpnet-base-v2` is recommended.\n",
      "  - embedding_function (Optional, Callable): the embedding function for creating the vector db. Default is None,\n",
      "  SentenceTransformer with the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or\n",
      "  other embedding functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n",
      "  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n",
      "  - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n",
      "  If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n",
      "  - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n",
      "  - get_or_create (Optional, bool): if True, will create/recreate a collection for the retrieve chat.\n",
      "  This is the same as that used in chromadb. Default is False. Will be set to False if docs_path is None.\n",
      "  - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n",
      "  The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).\n",
      "  Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.\n",
      "  - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n",
      "  Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "  \n",
      "  Example of overriding retrieve_docs:\n",
      "  If you have set up a customized vector db, and it's not compatible with chromadb, you can easily plug in it with below code.\n",
      "```python\n",
      "class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n",
      "    def query_vector_db(\n",
      "        self,\n",
      "        query_texts: List[str],\n",
      "        n_results: int = 10,\n",
      "        search_string: str = \"\",\n",
      "        **kwargs,\n",
      "    ) -> Dict[str, Union[List[str], List[List[str]]]]:\n",
      "        # define your own query function here\n",
      "        pass\n",
      "\n",
      "    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs):\n",
      "        results = self.query_vector_db(\n",
      "            query_texts=[problem],\n",
      "            n_results=n_results,\n",
      "            search_string=search_string,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        self._results = results\n",
      "        print(\"doc_ids: \", results[\"ids\"])\n",
      "```\n",
      "\n",
      "#### retrieve\\_docs\n",
      "\n",
      "```python\n",
      "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")\n",
      "```\n",
      "\n",
      "Retrieve docs based on the given problem and assign the results to the class property `_results`.\n",
      "In case you want to customize the retrieval process, such as using a different vector db whose APIs are not\n",
      "compatible with chromadb or filter results with metadata, you can override this function. Just keep the current\n",
      "parameters and add your own parameters with default values, and keep the results in below type.\n",
      "\n",
      "Type of the results: Dict[str, List[List[Any]]], should have keys \"ids\" and \"documents\", \"ids\" for the ids of\n",
      "the retrieved docs and \"documents\" for the contents of the retrieved docs. Any other keys are optional. Refer\n",
      "to `chromadb.api.types.QueryResult` as an example.\n",
      "ids: List[string]\n",
      "documents: List[List[string]]\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "\n",
      "#### generate\\_init\\_message\n",
      "\n",
      "```python\n",
      "def generate_init_message(problem: str,\n",
      "                          n_results: int = 20,\n",
      "                          search_string: str = \"\")\n",
      "```\n",
      "\n",
      "Generate an initial message with the given problem and prompt.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - the generated prompt ready to be sent to the assistant agent.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: retrieve_assistant_agent\n",
      "title: agentchat.contrib.retrieve_assistant_agent\n",
      "---\n",
      "\n",
      "## RetrieveAssistantAgent Objects\n",
      "\n",
      "```python\n",
      "class RetrieveAssistantAgent(AssistantAgent)\n",
      "```\n",
      "\n",
      "(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.\n",
      "\n",
      "RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.\n",
      "The default system message is designed to solve a task with LLM,\n",
      "including suggesting python code blocks and debugging.\n",
      "`human_input_mode` is default to \"NEVER\"\n",
      "and `code_execution_config` is default to False.\n",
      "This agent doesn't execute code by default, and expects the user to execute the code.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: math_user_proxy_agent\n",
      "title: agentchat.contrib.math_user_proxy_agent\n",
      "---\n",
      "\n",
      "## MathUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class MathUserProxyAgent(UserProxyAgent)\n",
      "```\n",
      "\n",
      "(Experimental) A MathChat agent that can handle math problems.\n",
      "\n",
      "#### MAX\\_CONSECUTIVE\\_AUTO\\_REPLY\n",
      "\n",
      "maximum number of consecutive auto replies (subject to future change)\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: Optional[str] = \"MathChatAgent\",\n",
      "             is_termination_msg: Optional[Callable[\n",
      "                 [Dict], bool]] = _is_termination_msg_mathchat,\n",
      "             human_input_mode: Optional[str] = \"NEVER\",\n",
      "             default_auto_reply: Optional[Union[str, Dict,\n",
      "                                                None]] = DEFAULT_REPLY,\n",
      "             max_invalid_q_per_step=3,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) (Default) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `default_auto_reply` _str or dict or None_ - the default auto reply message when no code execution or llm based reply is generated.\n",
      "- `max_invalid_q_per_step` _int_ - (ADDED) the maximum number of invalid queries per step.\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "\n",
      "#### generate\\_init\\_message\n",
      "\n",
      "```python\n",
      "def generate_init_message(problem,\n",
      "                          prompt_type=\"default\",\n",
      "                          customized_prompt=None)\n",
      "```\n",
      "\n",
      "Generate a prompt for the assitant agent with the given problem and prompt.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `prompt_type` _str_ - the type of the prompt. Possible values are \"default\", \"python\", \"wolfram\".\n",
      "  (1) \"default\": the prompt that allows the agent to choose between 3 ways to solve a problem:\n",
      "  1. write a python program to solve it directly.\n",
      "  2. solve it directly without python.\n",
      "  3. solve it step by step with python.\n",
      "  (2) \"python\":\n",
      "  a simplified prompt from the third way of the \"default\" prompt, that asks the assistant\n",
      "  to solve the problem step by step with python.\n",
      "  (3) \"two_tools\":\n",
      "  a simplified prompt similar to the \"python\" prompt, but allows the model to choose between\n",
      "  Python and Wolfram Alpha to solve the problem.\n",
      "- `customized_prompt` _str_ - a customized prompt to be used. If it is not None, the prompt_type will be ignored.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - the generated prompt ready to be sent to the assistant agent.\n",
      "\n",
      "#### execute\\_one\\_python\\_code\n",
      "\n",
      "```python\n",
      "def execute_one_python_code(pycode)\n",
      "```\n",
      "\n",
      "Execute python code blocks.\n",
      "\n",
      "Previous python code will be saved and executed together with the new code.\n",
      "the \"print\" function will also be added to the last line of the code if needed\n",
      "\n",
      "#### execute\\_one\\_wolfram\\_query\n",
      "\n",
      "```python\n",
      "def execute_one_wolfram_query(query: str)\n",
      "```\n",
      "\n",
      "Run one wolfram query and return the output.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `query` - string of the query.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `output` - string with the output of the query.\n",
      "- `is_success` - boolean indicating whether the query was successful.\n",
      "\n",
      "#### get\\_from\\_dict\\_or\\_env\n",
      "\n",
      "```python\n",
      "def get_from_dict_or_env(data: Dict[str, Any],\n",
      "                         key: str,\n",
      "                         env_key: str,\n",
      "                         default: Optional[str] = None) -> str\n",
      "```\n",
      "\n",
      "Get a value from a dictionary or an environment variable.\n",
      "\n",
      "## WolframAlphaAPIWrapper Objects\n",
      "\n",
      "```python\n",
      "class WolframAlphaAPIWrapper(BaseModel)\n",
      "```\n",
      "\n",
      "Wrapper for Wolfram Alpha.\n",
      "\n",
      "Docs for using:\n",
      "\n",
      "1. Go to wolfram alpha and sign up for a developer account\n",
      "2. Create an app and get your APP ID\n",
      "3. Save your APP ID into WOLFRAM_ALPHA_APPID env variable\n",
      "4. pip install wolframalpha\n",
      "\n",
      "#### wolfram\\_client\n",
      "\n",
      ":meta private:\n",
      "\n",
      "## Config Objects\n",
      "\n",
      "```python\n",
      "class Config()\n",
      "```\n",
      "\n",
      "Configuration for this pydantic object.\n",
      "\n",
      "#### validate\\_environment\n",
      "\n",
      "```python\n",
      "@root_validator(skip_on_failure=True)\n",
      "def validate_environment(cls, values: Dict) -> Dict\n",
      "```\n",
      "\n",
      "Validate that api key and python package exists in environment.\n",
      "\n",
      "#### run\n",
      "\n",
      "```python\n",
      "def run(query: str) -> str\n",
      "```\n",
      "\n",
      "Run query through WolframAlpha and parse result.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "Yes, there is a function named `tune_automl` in FLAML.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "qa_problem = \"Is there a function named `tune_automl` in FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-2\"></a>\n",
    "### Example 2\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer a question that is not related to code generation.\n",
    "\n",
    "Problem: Who is the author of FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAdding doc_id 0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 16 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 2 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 3 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Who is the author of FLAML?\n",
      "\n",
      "Context is: {\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"items\": [\n",
      "        {\n",
      "          \"items\": [\n",
      "            \"reference/agentchat/contrib/math_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_assistant_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/teachable_agent\",\n",
      "            \"reference/agentchat/contrib/text_analyzer_agent\"\n",
      "          ],\n",
      "          \"label\": \"agentchat.contrib\",\n",
      "          \"type\": \"category\"\n",
      "        },\n",
      "        \"reference/agentchat/agent\",\n",
      "        \"reference/agentchat/assistant_agent\",\n",
      "        \"reference/agentchat/conversable_agent\",\n",
      "        \"reference/agentchat/groupchat\",\n",
      "        \"reference/agentchat/user_proxy_agent\"\n",
      "      ],\n",
      "      \"label\": \"agentchat\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    {\n",
      "      \"items\": [\n",
      "        \"reference/oai/completion\",\n",
      "        \"reference/oai/openai_utils\"\n",
      "      ],\n",
      "      \"label\": \"oai\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    \"reference/code_utils\",\n",
      "    \"reference/math_utils\",\n",
      "    \"reference/retrieve_utils\"\n",
      "  ],\n",
      "  \"label\": \"Reference\",\n",
      "  \"type\": \"category\"\n",
      "}\n",
      "\n",
      "```python\n",
      "def query_qdrant(\n",
      "        query_texts: List[str],\n",
      "        n_results: int = 10,\n",
      "        client: QdrantClient = None,\n",
      "        collection_name: str = \"all-my-documents\",\n",
      "        search_string: str = \"\",\n",
      "        embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n",
      "        qdrant_client_options: Optional[Dict] = {}\n",
      ") -> List[List[QueryResponse]]\n",
      "```\n",
      "\n",
      "Perform a similarity search with filters on a Qdrant collection\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `query_texts` _List[str]_ - the query texts.\n",
      "- `n_results` _Optional, int_ - the number of results to return. Default is 10.\n",
      "- `client` _Optional, API_ - the QdrantClient instance. A default in-memory client will be instantiated if None.\n",
      "- `collection_name` _Optional, str_ - the name of the collection. Default is \"all-my-documents\".\n",
      "- `search_string` _Optional, str_ - the search string. Default is \"\".\n",
      "- `embedding_model` _Optional, str_ - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if embedding_function is not None.\n",
      "- `qdrant_client_options` - (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `List[List[QueryResponse]]` - the query result. The format is:\n",
      "  class QueryResponse(BaseModel, extra=\"forbid\"):  # type: ignore\n",
      "- `id` - Union[str, int]\n",
      "- `embedding` - Optional[List[float]]\n",
      "- `metadata` - Dict[str, Any]\n",
      "- `document` - str\n",
      "- `score` - float\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: math_utils\n",
      "title: math_utils\n",
      "---\n",
      "\n",
      "#### solve\\_problem\n",
      "\n",
      "```python\n",
      "def solve_problem(problem: str, **config) -> str\n",
      "```\n",
      "\n",
      "(Experimental) Solve the math problem.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - The problem statement.\n",
      "- `config` _Optional, dict_ - The configuration for the API call.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - The solution to the problem.\n",
      "\n",
      "#### remove\\_boxed\n",
      "\n",
      "```python\n",
      "def remove_boxed(string: str) -> Optional[str]\n",
      "```\n",
      "\n",
      "Source: https://github.com/hendrycks/math\n",
      "Extract the text within a \\boxed{...} environment.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "  > remove_boxed(\"\\boxed{\\frac{2}{3}}\")\n",
      "  \n",
      "  \\frac{2}{3}\n",
      "\n",
      "#### last\\_boxed\\_only\\_string\n",
      "\n",
      "```python\n",
      "def last_boxed_only_string(string: str) -> Optional[str]\n",
      "```\n",
      "\n",
      "Source: https://github.com/hendrycks/math\n",
      "Extract the last \\boxed{...} or \\fbox{...} element from a string.\n",
      "\n",
      "#### is\\_equiv\n",
      "\n",
      "```python\n",
      "def is_equiv(str1: Optional[str], str2: Optional[str]) -> float\n",
      "```\n",
      "\n",
      "Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in\n",
      "- units\n",
      "- fractions\n",
      "- square roots\n",
      "- superfluous LaTeX.\n",
      "Source: https://github.com/hendrycks/math\n",
      "\n",
      "#### is\\_equiv\\_chain\\_of\\_thought\n",
      "\n",
      "```python\n",
      "def is_equiv_chain_of_thought(str1: str, str2: str) -> float\n",
      "```\n",
      "\n",
      "Strips the solution first before calling `is_equiv`.\n",
      "\n",
      "#### eval\\_math\\_responses\n",
      "\n",
      "```python\n",
      "def eval_math_responses(responses, solution=None, **args)\n",
      "```\n",
      "\n",
      "Select a response for a math problem using voting, and check if the response is correct if the solution is provided.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `responses` _list_ - The list of responses.\n",
      "- `solution` _str_ - The canonical solution.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `dict` - The success metrics.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: retrieve_utils\n",
      "title: retrieve_utils\n",
      "---\n",
      "\n",
      "#### num\\_tokens\\_from\\_text\n",
      "\n",
      "```python\n",
      "def num_tokens_from_text(\n",
      "    text: str,\n",
      "    model: str = \"gpt-3.5-turbo-0613\",\n",
      "    return_tokens_per_name_and_message: bool = False,\n",
      "    custom_token_count_function: Callable = None\n",
      ") -> Union[int, Tuple[int, int, int]]\n",
      "```\n",
      "\n",
      "Return the number of tokens used by a text.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `text` _str_ - The text to count tokens for.\n",
      "- `model` _Optional, str_ - The model to use for tokenization. Default is \"gpt-3.5-turbo-0613\".\n",
      "- `return_tokens_per_name_and_message` _Optional, bool_ - Whether to return the number of tokens per name and per\n",
      "  message. Default is False.\n",
      "- `custom_token_count_function` _Optional, Callable_ - A custom function to count tokens. Default is None.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `int` - The number of tokens used by the text.\n",
      "- `int` - The number of tokens per message. Only returned if return_tokens_per_name_and_message is True.\n",
      "- `int` - The number of tokens per name. Only returned if return_tokens_per_name_and_message is True.\n",
      "\n",
      "#### num\\_tokens\\_from\\_messages\n",
      "\n",
      "```python\n",
      "def num_tokens_from_messages(messages: dict,\n",
      "                             model: str = \"gpt-3.5-turbo-0613\",\n",
      "                             custom_token_count_function: Callable = None,\n",
      "                             custom_prime_count: int = 3)\n",
      "```\n",
      "\n",
      "Return the number of tokens used by a list of messages.\n",
      "\n",
      "#### split\\_text\\_to\\_chunks\n",
      "\n",
      "```python\n",
      "def split_text_to_chunks(text: str,\n",
      "                         max_tokens: int = 4000,\n",
      "                         chunk_mode: str = \"multi_lines\",\n",
      "                         must_break_at_empty_line: bool = True,\n",
      "                         overlap: int = 10)\n",
      "```\n",
      "\n",
      "Split a long text into chunks of max_tokens.\n",
      "\n",
      "#### extract\\_text\\_from\\_pdf\n",
      "\n",
      "```python\n",
      "def extract_text_from_pdf(file: str) -> str\n",
      "```\n",
      "\n",
      "Extract text from PDF files\n",
      "\n",
      "#### split\\_files\\_to\\_chunks\n",
      "\n",
      "```python\n",
      "def split_files_to_chunks(files: list,\n",
      "                          max_tokens: int = 4000,\n",
      "                          chunk_mode: str = \"multi_lines\",\n",
      "                          must_break_at_empty_line: bool = True,\n",
      "                          custom_text_split_function: Callable = None)\n",
      "```\n",
      "\n",
      "Split a list of files into chunks of max_tokens.\n",
      "\n",
      "#### get\\_files\\_from\\_dir\n",
      "\n",
      "```python\n",
      "def get_files_from_dir(dir_path: Union[str, List[str]],\n",
      "                       types: list = TEXT_FORMATS,\n",
      "                       recursive: bool = True)\n",
      "```\n",
      "\n",
      "Return a list of all the files in a given directory.\n",
      "\n",
      "#### get\\_file\\_from\\_url\n",
      "\n",
      "```python\n",
      "def get_file_from_url(url: str, save_path: str = None)\n",
      "```\n",
      "\n",
      "Download a file from a URL.\n",
      "\n",
      "#### is\\_url\n",
      "\n",
      "```python\n",
      "def is_url(string: str)\n",
      "```\n",
      "\n",
      "Return True if the string is a valid URL.\n",
      "\n",
      "#### create\\_vector\\_db\\_from\\_dir\n",
      "\n",
      "```python\n",
      "def create_vector_db_from_dir(dir_path: str,\n",
      "                              max_tokens: int = 4000,\n",
      "                              client: API = None,\n",
      "                              db_path: str = \"/tmp/chromadb.db\",\n",
      "                              collection_name: str = \"all-my-documents\",\n",
      "                              get_or_create: bool = False,\n",
      "                              chunk_mode: str = \"multi_lines\",\n",
      "                              must_break_at_empty_line: bool = True,\n",
      "                              embedding_model: str = \"all-MiniLM-L6-v2\",\n",
      "                              embedding_function: Callable = None,\n",
      "                              custom_text_split_function: Callable = None)\n",
      "```\n",
      "\n",
      "Create a vector db from all the files in a given directory, the directory can also be a single file or a url to\n",
      "a single file. We support chromadb compatible APIs to create the vector db, this function is not required if\n",
      "you prepared your own vector db.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `dir_path` _str_ - the path to the directory, file or url.\n",
      "- `max_tokens` _Optional, int_ - the maximum number of tokens per chunk. Default is 4000.\n",
      "- `client` _Optional, API_ - the chromadb client. Default is None.\n",
      "- `db_path` _Optional, str_ - the path to the chromadb. Default is \"/tmp/chromadb.db\".\n",
      "- `collection_name` _Optional, str_ - the name of the collection. Default is \"all-my-documents\".\n",
      "- `get_or_create` _Optional, bool_ - Whether to get or create the collection. Default is False. If True, the collection\n",
      "  will be recreated if it already exists.\n",
      "- `chunk_mode` _Optional, str_ - the chunk mode. Default is \"multi_lines\".\n",
      "- `must_break_at_empty_line` _Optional, bool_ - Whether to break at empty line. Default is True.\n",
      "- `embedding_model` _Optional, str_ - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if\n",
      "  embedding_function is not None.\n",
      "- `embedding_function` _Optional, Callable_ - the embedding function to use. Default is None, SentenceTransformer with\n",
      "  the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding\n",
      "  functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n",
      "\n",
      "#### query\\_vector\\_db\n",
      "\n",
      "```python\n",
      "def query_vector_db(query_texts: List[str],\n",
      "                    n_results: int = 10,\n",
      "                    client: API = None,\n",
      "                    db_path: str = \"/tmp/chromadb.db\",\n",
      "                    collection_name: str = \"all-my-documents\",\n",
      "                    search_string: str = \"\",\n",
      "                    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
      "                    embedding_function: Callable = None) -> QueryResult\n",
      "```\n",
      "\n",
      "Query a vector db. We support chromadb compatible APIs, it's not required if you prepared your own vector db\n",
      "and query function.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `query_texts` _List[str]_ - the query texts.\n",
      "- `n_results` _Optional, int_ - the number of results to return. Default is 10.\n",
      "- `client` _Optional, API_ - the chromadb compatible client. Default is None, a chromadb client will be used.\n",
      "- `db_path` _Optional, str_ - the path to the vector db. Default is \"/tmp/chromadb.db\".\n",
      "- `collection_name` _Optional, str_ - the name of the collection. Default is \"all-my-documents\".\n",
      "- `search_string` _Optional, str_ - the search string. Default is \"\".\n",
      "- `embedding_model` _Optional, str_ - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if\n",
      "  embedding_function is not None.\n",
      "- `embedding_function` _Optional, Callable_ - the embedding function to use. Default is None, SentenceTransformer with\n",
      "  the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding\n",
      "  functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `QueryResult` - the query result. The format is:\n",
      "  class QueryResult(TypedDict):\n",
      "- `ids` - List[IDs]\n",
      "- `embeddings` - Optional[List[List[Embedding]]]\n",
      "- `documents` - Optional[List[List[Document]]]\n",
      "- `metadatas` - Optional[List[List[Metadata]]]\n",
      "- `distances` - Optional[List[List[float]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "FLAML (Fast and Lightweight AutoML) is an open-source Python library for hyperparameter tuning and automated machine learning. It is authored by Microsoft.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "qa_problem = \"Who is the author of FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
