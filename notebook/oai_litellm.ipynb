{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-source Model Enablement with LiteLLM\n",
    "\n",
    "To implement the usage of LiteLLM and enable AutoGen to use open-sourced models while considering the context provided, we will address the points raised and create a refactoring plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "import litellm\n",
    "from litellm import completion\n",
    "print(autogen.__version__)\n",
    "from autogen import AssistantAgent, UserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dotenv with OPENAI_API_KEY\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../.env',\n",
    "    model_api_key_map={\n",
    "        # \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        \"huggingface/mistralai/Mistral-7B-v0.1\": \"HUGGINGFACE_HUB\",\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            # \"gpt-4\",\n",
    "            \"huggingface/mistralai/Mistral-7B-v0.1\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    # is_termination_msg = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remove OPENAI_API_KEY from .env*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "# config_list_from_dotenv no OPENAI_API_KEY\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../.env',\n",
    "    model_api_key_map={\n",
    "        \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        \"huggingface/mistralai/Mistral-7B-v0.1\": \"HUGGINGFACE_HUB\",\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"huggingface/mistralai/Mistral-7B-v0.1\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    is_termination_msg = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock, save to png.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\u001b[33mcoding_runner\u001b[0m (to coding_assistant):\n",
      "\n",
      "Calculate the percentage gain YTD for Berkshire Hathaway stock and plot a chart to linechart.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m coding_assistant \u001b[39m=\u001b[39m AssistantAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoding_assistant\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     llm_config\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m coding_runner \u001b[39m=\u001b[39m UserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoding_runner\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     human_input_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNEVER\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/PR_TMP/autogen/notebook/oai_litellm.ipynb#X45sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m coding_runner\u001b[39m.\u001b[39;49minitiate_chat(coding_assistant, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCalculate the percentage gain YTD for Berkshire Hathaway stock and plot a chart to linechart.png\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:779\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 779\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    781\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:838\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    837\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 838\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:233\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[0;32m--> 233\u001b[0m         response \u001b[39m=\u001b[39m litellm\u001b[39m.\u001b[39;49mcompletion(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    234\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m         response \u001b[39m=\u001b[39m litellm\u001b[39m.\u001b[39mcompletion(request_timeout\u001b[39m=\u001b[39mrequest_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/utils.py:662\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    659\u001b[0m         liteDebuggerClient \u001b[39mand\u001b[39;00m liteDebuggerClient\u001b[39m.\u001b[39mdashboard_url \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     ):  \u001b[39m# make it easy to get to the debugger logs if you've initialized it\u001b[39;00m\n\u001b[1;32m    661\u001b[0m         e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Check the log in your dashboard - \u001b[39m\u001b[39m{\u001b[39;00mliteDebuggerClient\u001b[39m.\u001b[39mdashboard_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 662\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/utils.py:621\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[39mreturn\u001b[39;00m cached_result\n\u001b[1;32m    620\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    622\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     \u001b[39m# TODO: Add to cache for streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/timeout.py:44\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     local_timeout_duration \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mlocal_timeout_duration)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m futures\u001b[39m.\u001b[39mTimeoutError:\n\u001b[1;32m     46\u001b[0m     thread\u001b[39m.\u001b[39mstop_loop()\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/timeout.py:33\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper.<locals>.async_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39masync_func\u001b[39m():\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/main.py:1112\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, metadata, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params, acompletion)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   1110\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1111\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 1112\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   1113\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider, original_exception\u001b[39m=\u001b[39;49me, completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1114\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/utils.py:2661\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m \u001b[39m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[39;00m\n\u001b[1;32m   2660\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m-> 2661\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   2662\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2663\u001b[0m     \u001b[39mraise\u001b[39;00m original_exception\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/utils.py:2098\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThis model\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms maximum context length is\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m original_exception\u001b[39m.\u001b[39m_message:\n\u001b[1;32m   2093\u001b[0m         \u001b[39mraise\u001b[39;00m ContextWindowExceededError(\n\u001b[1;32m   2094\u001b[0m             message\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(original_exception),\n\u001b[1;32m   2095\u001b[0m             model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m   2096\u001b[0m             llm_provider\u001b[39m=\u001b[39moriginal_exception\u001b[39m.\u001b[39mllm_provider\n\u001b[1;32m   2097\u001b[0m         )\n\u001b[0;32m-> 2098\u001b[0m     \u001b[39mraise\u001b[39;00m original_exception\n\u001b[1;32m   2099\u001b[0m \u001b[39melif\u001b[39;00m model:\n\u001b[1;32m   2100\u001b[0m     error_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(original_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/main.py:392\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, metadata, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params, acompletion)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    385\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m    387\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    388\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m    389\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    390\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: litellm\u001b[39m.\u001b[39mheaders},\n\u001b[1;32m    391\u001b[0m     )\n\u001b[0;32m--> 392\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m optional_params \u001b[39mand\u001b[39;00m optional_params[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     response \u001b[39m=\u001b[39m CustomStreamWrapper(response, model, custom_llm_provider\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mopenai\u001b[39m\u001b[39m\"\u001b[39m, logging_obj\u001b[39m=\u001b[39mlogging)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/main.py:374\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, metadata, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params, acompletion)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m## COMPLETION CALL\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    375\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    376\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m    377\u001b[0m         headers\u001b[39m=\u001b[39;49mlitellm\u001b[39m.\u001b[39;49mheaders, \u001b[39m# None by default\u001b[39;49;00m\n\u001b[1;32m    378\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base, \u001b[39m# thread safe setting base, key, api_version\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m    380\u001b[0m         api_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mopenai\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    381\u001b[0m         api_version\u001b[39m=\u001b[39;49mapi_version, \u001b[39m# default None\u001b[39;49;00m\n\u001b[1;32m    382\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptional_params,\n\u001b[1;32m    383\u001b[0m     )\n\u001b[1;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    385\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m    387\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    388\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m    389\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    390\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: litellm\u001b[39m.\u001b[39mheaders},\n\u001b[1;32m    391\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[1;32m    141\u001b[0m         timeout,\n\u001b[1;32m    142\u001b[0m         stream,\n\u001b[1;32m    143\u001b[0m         headers,\n\u001b[1;32m    144\u001b[0m         request_timeout,\n\u001b[1;32m    145\u001b[0m         typed_api_type,\n\u001b[1;32m    146\u001b[0m         requestor,\n\u001b[1;32m    147\u001b[0m         url,\n\u001b[1;32m    148\u001b[0m         params,\n\u001b[0;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m MAX_TIMEOUT\n\u001b[0;32m--> 106\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[1;32m    107\u001b[0m     api_key,\n\u001b[1;32m    108\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m    109\u001b[0m     api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[1;32m    110\u001b[0m     api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[1;32m    111\u001b[0m     organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    113\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    115\u001b[0m     deployment_id,\n\u001b[1;32m    116\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     params,\n\u001b[1;32m    125\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/api_requestor.py:138\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m ):\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[1;32m    140\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[1;32m    141\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[1;32m    142\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[1;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='OAI_CONFIG_LIST',\n",
    "    filter_dict={\n",
    "            \"model\": [\"huggingface/mistralai/Mistral-7B-v0.1\"],\n",
    "        },\n",
    "    )\n",
    "print(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=30,\n",
    "    is_termination_msg = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot a chart to linechart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Names vs. Config Names Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(autogen.Completion.chat_models))\n",
    "print(autogen.Completion.chat_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(autogen.Completion.price1K))\n",
    "print(autogen.Completion.price1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(litellm.get_model_cost_map()))\n",
    "print(litellm.get_model_cost_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "from autogen.oai import Completion\n",
    "\n",
    "def map_oai_litellm_models():\n",
    "    \"\"\"\n",
    "    The purpose of this function is to find the corresponding\n",
    "    mappings of oai models as their names are different in litellm,\n",
    "    which we need to enable building agents with open sources models.\n",
    "\n",
    "    # Mapping\n",
    "    # For incommon models, a direct mapping can be created as they are named the same in both systems.\n",
    "    # For unique models, manual mapping might be needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetching model data\n",
    "    litellm_models = set(litellm.get_model_cost_map().keys())  # Assuming it's a dict\n",
    "    autogen_models = set(Completion.price1K)  # Assuming it's an iterable of model names\n",
    "    \n",
    "    # Identifying common and unique models\n",
    "    common_models = autogen_models.intersection(litellm_models)\n",
    "    unique_autogen_models = autogen_models.difference(litellm_models)\n",
    "    unique_litellm_models = litellm_models.difference(autogen_models)\n",
    "    \n",
    "    # Mapping for common models\n",
    "    common_mapping = {model: model for model in common_models}\n",
    "    \n",
    "    # Mapping for unique models\n",
    "    unique_mapping = {}\n",
    "    for auto_model in unique_autogen_models:\n",
    "        closest_match = difflib.get_close_matches(auto_model, unique_litellm_models, n=2, cutoff=0.1)\n",
    "        if closest_match:\n",
    "            unique_mapping[auto_model] = closest_match[0]\n",
    "        else:\n",
    "            # Handle unmatched models. Potentially log them for manual mapping\n",
    "            unique_mapping[auto_model] = None\n",
    "            print(f\"No match found for {auto_model}. Consider manually mapping this model.\")\n",
    "    \n",
    "    # Merge mappings\n",
    "    model_mapping = {**common_mapping, **unique_mapping}\n",
    "    \n",
    "    return model_mapping\n",
    "\n",
    "model_mapping = map_oai_litellm_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "model_mapping.pop('code-cushman-001', None)\n",
    "pp.pprint(model_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us this to replace names to suit LiteLLM.\n",
    "litellm.model_alias_map = {\n",
    "    'code-davinci-002': 'davinci-002',\n",
    "    'gpt-3.5-turbo': 'gpt-3.5-turbo',\n",
    "    'gpt-3.5-turbo-0301': 'gpt-3.5-turbo-0301',\n",
    "    'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613',\n",
    "    'gpt-3.5-turbo-16k': 'gpt-3.5-turbo-16k',\n",
    "    'gpt-3.5-turbo-16k-0613': 'gpt-3.5-turbo-16k-0613',\n",
    "    'gpt-35-turbo': 'gpt-3.5-turbo-instruct',\n",
    "    'gpt-4': 'gpt-4',\n",
    "    'gpt-4-0314': 'gpt-4-0314',\n",
    "    'gpt-4-0613': 'gpt-4-0613',\n",
    "    'gpt-4-32k': 'gpt-4-32k',\n",
    "    'gpt-4-32k-0314': 'gpt-4-32k-0314',\n",
    "    'gpt-4-32k-0613': 'gpt-4-32k-0613',\n",
    "    'text-ada-001': 'text-ada-001',\n",
    "    'text-babbage-001': 'text-babbage-001',\n",
    "    'text-curie-001': 'text-curie-001',\n",
    "    'text-davinci-002': 'davinci-002',\n",
    "    'text-davinci-003': 'text-davinci-003'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
