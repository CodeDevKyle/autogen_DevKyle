{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Round Robin Group Chat\n",
    "This notebook demonstrates how to implement a distributed round-robin group chat. In this example we will create three {py:class} `WorkerAgentRuntime` runtime object, two chat participant and a group chat manager, and each of them will connect to an instance of  `WorkerAgentRuntimeHost` which implements the required grpc server.\n",
    "You would only need to provide `OPENAI_API_KEY` variable below to be able to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "from autogen_core.application import WorkerAgentRuntime, WorkerAgentRuntimeHost\n",
    "from autogen_core.base import MessageContext\n",
    "from autogen_core.base._serialization import try_get_known_serializers_for_type\n",
    "from autogen_core.base._topic import TopicId\n",
    "from autogen_core.components import (\n",
    "    RoutedAgent,\n",
    "    message_handler,\n",
    "    type_subscription,\n",
    ")\n",
    "from autogen_core.components._default_subscription import DefaultSubscription\n",
    "from autogen_core.components._type_subscription import TypeSubscription\n",
    "from autogen_core.components.models import (\n",
    "    AssistantMessage,\n",
    "    ChatCompletionClient,\n",
    "    LLMMessage,\n",
    "    OpenAIChatCompletionClient,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class GroupChatMessage(BaseModel):\n",
    "    \"\"\"Implements a sample message sent by an LLM agent\"\"\"\n",
    "\n",
    "    body: LLMMessage\n",
    "\n",
    "\n",
    "class RequestToSpeak(BaseModel):\n",
    "    \"\"\"Message type for agents to speak\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Chat Manager\n",
    "- **group chat manager** implements a round-robing algorithm to ask agents to talk, one at a time. It sends `SpeakRequest` requests to agent topics. This agent subscrives to topic type **agent_result** to receive participants messages.\n",
    "\n",
    "# Participants\n",
    "- **data scientist**: Listens to `SpeakRequest` sent by manager and publishes a LLM API call response as `GroupChatMessage`. This agent subscribes to `topic_type=\"data_scientist\", agent_type=\"data_scientist_ag_type\"`\n",
    "- **art director**:  Listens to `SpeakRequest` sent by manager and publishes a LLM API call response as `GroupChatMessage`. This agent subscribes to `topic_type=\"art_director\", agent_type=\"art_director_ag_type\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantAgent(RoutedAgent):\n",
    "    def __init__(self, name: str, description: str, model_client: ChatCompletionClient, system_message: str) -> None:\n",
    "        super().__init__(\"A Participant\")\n",
    "        self._name = name\n",
    "        self._description = description\n",
    "        self._model_client = model_client\n",
    "        self._chat_history: List[LLMMessage] = [SystemMessage(system_message)]\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n",
    "        print(f\"received {message}\")\n",
    "        self._chat_history.append(message.body)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:\n",
    "        completion = await self._model_client.create(self._chat_history)\n",
    "        assert isinstance(completion.content, str)\n",
    "        self._chat_history.append(AssistantMessage(content=completion.content, source=self._name))\n",
    "        print(f\"\\n{'-'*80}\\n{self._name} ({id(self)}):\\n{completion.content}\")\n",
    "        await asyncio.sleep(2)\n",
    "        await self.publish_message(\n",
    "            GroupChatMessage(body=UserMessage(content=completion.content, source=self._name)),\n",
    "            TopicId(type=\"agent_result\", source=\"participant\"),\n",
    "        )\n",
    "\n",
    "\n",
    "@type_subscription(topic_type=\"agent_result\")\n",
    "class GroupChatManager(RoutedAgent):\n",
    "    def __init__(self, participant_topic_types: List[str], max_rounds: int = 3) -> None:\n",
    "        super().__init__(\"Group chat manager\")\n",
    "        self._num_rounds = 1\n",
    "        self._participant_topic_types = participant_topic_types\n",
    "        self._chat_history: List[GroupChatMessage] = []\n",
    "        self._max_rounds = max_rounds\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n",
    "        self._chat_history.append(message)\n",
    "        assert isinstance(message.body, UserMessage)\n",
    "        if self._num_rounds >= self._max_rounds:\n",
    "            print(f\"\\n\\n\\n ---> Finished running {self._num_rounds} rounds! <---\")\n",
    "            await asyncio.sleep(1)\n",
    "            return\n",
    "        speaker_topic_type = self._participant_topic_types[self._num_rounds % len(self._participant_topic_types)]\n",
    "        self._num_rounds += 1\n",
    "        print(f\"\\n{'-'*80}\\n Manager ({id(self)}): Asking {speaker_topic_type} to speak\")\n",
    "        await self.publish_message(RequestToSpeak(), TopicId(type=speaker_topic_type, source=\"manager\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed Host is now running and listening for connection at localhost:50060\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "data_scientist_agent (4642320400):\n",
      "As a data scientist, I can help analyze trends and patterns in your data to drive informed decisions. If you have specific datasets or questions in mind, feel free to share, and we can dive deeper into the analysis together!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " Manager (4642640912): Asking art_director to speak\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "art_director_agent (4642837840):\n",
      "As an art director, I believe that our visual narrative should reflect the core message of the project while also engaging the audience on an emotional level. It's crucial that each design element we choose enhances the overall aesthetic and storytelling we aim to convey.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " Manager (4642640912): Asking data_scientist to speak\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "data_scientist_agent (4642320400):\n",
      "Additionally, we can explore various statistical models or machine learning algorithms to extract insights and make predictions based on your data. Let me know what you're interested in, and we can start from there!\n",
      "\n",
      "\n",
      "\n",
      " ---> Finished running 3 rounds! <---\n"
     ]
    }
   ],
   "source": [
    "# Setting up gRPC server through `WorkerAgentRuntimeHost`\n",
    "host_address = \"localhost:50060\"\n",
    "host = WorkerAgentRuntimeHost(address=host_address)\n",
    "serializers = [\n",
    "    try_get_known_serializers_for_type(GroupChatMessage),\n",
    "    try_get_known_serializers_for_type(RequestToSpeak),\n",
    "]\n",
    "\n",
    "host.start()\n",
    "print(f\"Distributed Host is now running and listening for connection at {host_address}\")\n",
    "\n",
    "# Add data scientist runtime\n",
    "data_scientist_runtime = WorkerAgentRuntime(host_address=host_address)\n",
    "data_scientist_runtime.add_message_serializer(serializers)  # type: ignore[arg-type]\n",
    "data_scientist_runtime.start()\n",
    "await data_scientist_runtime.add_subscription(\n",
    "    TypeSubscription(topic_type=\"data_scientist\", agent_type=\"data_scientist_ag_type\")\n",
    ")\n",
    "await ParticipantAgent.register(\n",
    "    data_scientist_runtime,\n",
    "    \"data_scientist_ag_type\",\n",
    "    lambda: ParticipantAgent(\n",
    "        name=\"data_scientist_agent\",\n",
    "        description=\"A data scientist in a discussion\",\n",
    "        system_message=\"You are a data scientist in a conversation, provide 1-2 sentences in a discussion\",\n",
    "        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Add art director runtime\n",
    "art_director_runtime = WorkerAgentRuntime(host_address=host_address)\n",
    "art_director_runtime.add_message_serializer(serializers)  # type: ignore[arg-type]\n",
    "art_director_runtime.start()\n",
    "await art_director_runtime.add_subscription(\n",
    "    TypeSubscription(topic_type=\"art_director\", agent_type=\"art_director_ag_type\")\n",
    ")\n",
    "\n",
    "await ParticipantAgent.register(\n",
    "    art_director_runtime,\n",
    "    \"art_director_ag_type\",\n",
    "    lambda: ParticipantAgent(\n",
    "        name=\"art_director_agent\",\n",
    "        description=\"A art director in a discussion\",\n",
    "        system_message=\"You are a art director in a conversation, provide 1-2 sentences in a discussion\",\n",
    "        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Add group chat manager runtime\n",
    "group_chat_manager_runtime = WorkerAgentRuntime(host_address=host_address)\n",
    "group_chat_manager_runtime.add_message_serializer(serializers)  # type: ignore[arg-type]\n",
    "group_chat_manager_runtime.start()\n",
    "\n",
    "await GroupChatManager.register(\n",
    "    group_chat_manager_runtime,\n",
    "    \"group_chat_manager_ag_type\",\n",
    "    lambda: GroupChatManager(participant_topic_types=[\"data_scientist\", \"art_director\"], max_rounds=3),\n",
    ")\n",
    "\n",
    "\n",
    "# Send a message to data scientist to get started\n",
    "await group_chat_manager_runtime.publish_message(RequestToSpeak(), TopicId(type=\"data_scientist\", source=\"manager\"))\n",
    "\n",
    "await asyncio.sleep(10)  # Wait for messages to propagate\n",
    "await host.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
