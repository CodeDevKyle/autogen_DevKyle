{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "```{include} ../warning.md\n",
    "```\n",
    "\n",
    "AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages.\n",
    "All agents share the following attributes and methods:\n",
    "\n",
    "- {py:attr}`~autogen_agentchat.agents.BaseChatAgent.name`: The unique name of the agent.\n",
    "- {py:attr}`~autogen_agentchat.agents.BaseChatAgent.description`: The description of the agent in text.\n",
    "- {py:meth}`~autogen_agentchat.agents.BaseChatAgent.run`: Run with a given task and produce a {py:class}`~autogen_agentchat.base.TaskResult`.\n",
    "- {py:meth}`~autogen_agentchat.agents.BaseChatAgent.run_stream`: Run with a given task and produce an iterator of {py:class}`~autogen_agentchat.messages.AgentMessage` that ends with a {py:class}`~autogen_agentchat.base.TaskResult`.\n",
    "\n",
    "\n",
    "## Assistant Agent\n",
    "\n",
    "{py:class}`~autogen_agentchat.agents.AssistantAgent` is a built-in agent that\n",
    "is using the ReAct pattern to generate responses with ability to use tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models import OpenAIChatCompletionClient\n",
    "\n",
    "\n",
    "# Define a tool that searches the web for information.\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Find information on the web\"\"\"\n",
    "    return \"AutoGen is a programming framework for building multi-agent applications.\"\n",
    "\n",
    "\n",
    "# Create an agent that uses the OpenAI GPT-4o model.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    # api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[web_search],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the {py:meth}`~autogen_agentchat.agents.BaseChatAgent.run` \n",
    "method to run the agent with a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Find information on AutoGen'), ToolCallMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=82, completion_tokens=15), content=[FunctionCall(id='call_fNG1y1aEfDOVDMwWLGE83YXk', arguments='{\"query\":\"AutoGen\"}', name='web_search')]), ToolCallResultMessage(source='assistant', models_usage=None, content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', call_id='call_fNG1y1aEfDOVDMwWLGE83YXk')]), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=92, completion_tokens=14), content='AutoGen is a programming framework designed for creating multi-agent applications.')], stop_reason=None)\n"
     ]
    }
   ],
   "source": [
    "async def assistant_run() -> None:\n",
    "    # Run the agent with a given task.\n",
    "    result = await agent.run(task=\"Find information on AutoGen\")\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# Use asyncio.run(assistant_run()) when running in a script.\n",
    "await assistant_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to the {py:meth}`~autogen_agentchat.agents.BaseChatAgent.run` method\n",
    "returns a {py:class}`~autogen_agentchat.base.TaskResult` with a list of messages\n",
    "generated by the agent.\n",
    "\n",
    "### Stream Messages\n",
    "\n",
    "We can also stream each message as it is generated by the agent by using the\n",
    "{py:meth}`~autogen_agentchat.agents.BaseChatAgent.run_stream` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='user' models_usage=None content='Find information on AutoGen'\n",
      "source='assistant' models_usage=RequestUsage(prompt_tokens=82, completion_tokens=15) content=[FunctionCall(id='call_0M1jNwCWR6sTnCxI4FC1Y1KA', arguments='{\"query\":\"AutoGen\"}', name='web_search')]\n",
      "source='assistant' models_usage=None content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', call_id='call_0M1jNwCWR6sTnCxI4FC1Y1KA')]\n",
      "source='assistant' models_usage=RequestUsage(prompt_tokens=92, completion_tokens=16) content='AutoGen is a programming framework designed for the development of multi-agent applications.'\n",
      "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Find information on AutoGen'), ToolCallMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=82, completion_tokens=15), content=[FunctionCall(id='call_0M1jNwCWR6sTnCxI4FC1Y1KA', arguments='{\"query\":\"AutoGen\"}', name='web_search')]), ToolCallResultMessage(source='assistant', models_usage=None, content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', call_id='call_0M1jNwCWR6sTnCxI4FC1Y1KA')]), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=92, completion_tokens=16), content='AutoGen is a programming framework designed for the development of multi-agent applications.')], stop_reason=None)\n"
     ]
    }
   ],
   "source": [
    "async def assistant_run_stream() -> None:\n",
    "    # Run the agent with a given task and stream the response.\n",
    "    async for message in agent.run_stream(task=\"Find information on AutoGen\"):\n",
    "        print(message)\n",
    "\n",
    "\n",
    "# Use asyncio.run(assistant_run_stream()) when running in a script.\n",
    "await assistant_run_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {py:meth}`~autogen_agentchat.agents.BaseChatAgent.run_stream` method\n",
    "returns an asynchronous generator that yields each message generated by the agent,\n",
    "and the final task result as the last item.\n",
    "\n",
    "From the messages, you can see the assistant agent used the `web_search` tool to\n",
    "search for information and responded using the search results.\n",
    "\n",
    "### Understanding Tool Calling\n",
    "\n",
    "Large Language Models (LLMs) are typically limited to generating text or code responses. However, many complex tasks benefit from the ability to use external tools that perform specific actions, such as fetching data from APIs or databases.\n",
    "\n",
    "To address this limitation, modern LLMs can now accept a list of available tool schemas (descriptions of tools and their arguments) and generate a tool call message. This capability is known as **Tool Calling** or **Function Calling** and is becoming a popular pattern in building intelligent agent-based applications.\n",
    "\n",
    "For more information on tool calling, refer to the documentation from [OpenAI](https://platform.openai.com/docs/guides/function-calling) and [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## CodingAssistantAgent\n",
    "\n",
    "Generates responses (text and code) using an LLM upon receipt of a message. It takes a `system_message` argument that defines or sets the tone for how the agent's LLM should respond. \n",
    "\n",
    "```python\n",
    "\n",
    "writing_assistant_agent = CodingAssistantAgent(\n",
    "    name=\"writing_assistant_agent\",\n",
    "    system_message=\"You are a helpful assistant that solve tasks by generating text responses and code.\",\n",
    "    model_client=model_client,\n",
    ")\n",
    "`\n",
    "\n",
    "We can explore or test the behavior of the agent by sending a message to it using the  {py:meth}`~autogen_agentchat.agents.BaseChatAgent.on_messages`  method. \n",
    "\n",
    "```python\n",
    "result = await writing_assistant_agent.on_messages(\n",
    "    messages=[\n",
    "        TextMessage(content=\"What is the weather right now in France?\", source=\"user\"),\n",
    "    ],\n",
    "    cancellation_token=CancellationToken(),\n",
    ")\n",
    "print(result) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeExecutorAgent\n",
    "\n",
    "The {py:class}`~autogen_agentchat.agents.CodeExecutorAgent`\n",
    "preset extracts and executes code snippets found in received messages and returns the output. It is typically used within a team with another agent that generates code snippets to be executed.\n",
    "\n",
    "```{note}\n",
    "It is recommended that the {py:class}`~autogen_agentchat.agents.CodeExecutorAgent` agent\n",
    "uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running. \n",
    "Follow the installation instructions for [Docker](https://docs.docker.com/get-docker/).\n",
    "```\n",
    "\n",
    "In this example, we show how to set up a {py:class}`~autogen_agentchat.agents.CodeExecutorAgent` agent that uses the \n",
    "{py:class}`~autogen_ext.code_executors.DockerCommandLineCodeExecutor` \n",
    "to execute code snippets in a Docker container. The `work_dir` parameter indicates where all executed files are first saved locally before being executed in the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='code_executor' models_usage=None content='Hello world\\n'\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import CodeExecutorAgent\n",
    "from autogen_ext.code_executors import DockerCommandLineCodeExecutor\n",
    "\n",
    "\n",
    "async def run_code_executor_agent() -> None:\n",
    "    # Create a code executor agent that uses a Docker container to execute code.\n",
    "    code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n",
    "    await code_executor.start()\n",
    "    code_executor_agent = CodeExecutorAgent(\"code_executor\", code_executor=code_executor)\n",
    "\n",
    "    # Run the agent with a given code snippet.\n",
    "    task = \"\"\"Here is some code\n",
    "```python\n",
    "print('Hello world')\n",
    "```\n",
    "\"\"\"\n",
    "    code_execution_result = await code_executor_agent.run(task=task)\n",
    "    print(code_execution_result.messages[-1])\n",
    "\n",
    "    # Stop the code executor.\n",
    "    await code_executor.stop()\n",
    "\n",
    "\n",
    "# Use asyncio.run(run_code_executor_agent()) when running in a script.\n",
    "await run_code_executor_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows the agent executing a code snippet that prints \"Hello world\".\n",
    "The agent then returns the output of the code execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own Agents\n",
    "\n",
    "You may have agents with behaviors that do not fall into a preset. \n",
    "In such cases, you can build custom agents.\n",
    "\n",
    "All agents in AgentChat inherit from {py:class}`~autogen_agentchat.agents.BaseChatAgent` \n",
    "class and implement the following abstract methods and attributes:\n",
    "\n",
    "- {py:meth}`~autogen_agentchat.agents.BaseChatAgent.on_messages`: The abstract method that defines the behavior of the agent in response to messages. This method is called when the agent is asked to provide a response in {py:meth}`~autogen_agentchat.agents.BaseChatAgent.run`. It returns a {py:class}`~autogen_agentchat.base.Response` object.\n",
    "- {py:meth}`~autogen_agentchat.agents.BaseChatAgent.reset`: The abstract method that resets the agent to its initial state. This method is called when the agent is asked to reset itself.\n",
    "- {py:attr}`~autogen_agentchat.agents.BaseChatAgent.produced_message_types`: The list of possible message types the agent can produce in its response.\n",
    "\n",
    "Optionally, you can implement the the {py:meth}`~autogen_agentchat.agents.BaseChatAgent.on_messages_stream` method to stream messages as they are generated by the agent. If this method is not implemented, the agent\n",
    "uses the default implementation of {py:meth}`~autogen_agentchat.agents.BaseChatAgent.on_messages_stream`\n",
    "that calls the {py:meth}`~autogen_agentchat.agents.BaseChatAgent.on_messages` method and\n",
    "yields all messages in the response.\n",
    "\n",
    "\n",
    "### UserProxyAgent \n",
    "\n",
    "A common use case for building a custom agent is to create an agent that acts as a proxy for the user.\n",
    "\n",
    "In the example below we show how to implement a `UserProxyAgent` - an agent that asks the user to enter\n",
    "some text through console and then returns that message as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='user_proxy_agent' models_usage=None content='I am glad to be here.'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List, Sequence\n",
    "\n",
    "from autogen_agentchat.agents import BaseChatAgent\n",
    "from autogen_agentchat.base import Response\n",
    "from autogen_agentchat.messages import (\n",
    "    ChatMessage,\n",
    "    StopMessage,\n",
    "    TextMessage,\n",
    ")\n",
    "from autogen_core.base import CancellationToken\n",
    "\n",
    "\n",
    "class UserProxyAgent(BaseChatAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name, \"A human user.\")\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> List[type[ChatMessage]]:\n",
    "        return [TextMessage, StopMessage]\n",
    "\n",
    "    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        user_input = await asyncio.get_event_loop().run_in_executor(None, input, \"Enter your response: \")\n",
    "        if \"TERMINATE\" in user_input:\n",
    "            return Response(chat_message=StopMessage(content=\"User has terminated the conversation.\", source=self.name))\n",
    "        return Response(chat_message=TextMessage(content=user_input, source=self.name))\n",
    "\n",
    "    async def reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "async def run_user_proxy_agent() -> None:\n",
    "    user_proxy_agent = UserProxyAgent(name=\"user_proxy_agent\")\n",
    "    user_proxy_agent_result = await user_proxy_agent.run(task=\"What's your thought?\")\n",
    "    print(user_proxy_agent_result.messages[-1])\n",
    "\n",
    "\n",
    "# Use asyncio.run(run_user_proxy_agent()) when running in a script.\n",
    "await run_user_proxy_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
