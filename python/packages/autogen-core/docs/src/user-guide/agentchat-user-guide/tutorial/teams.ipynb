{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teams\n",
    "\n",
    "Teams define how groups of agents communicate to address tasks. AgentChat provides several preset team configurations to simplify building multi-agent applications.\n",
    "\n",
    "A team may consist of a single agent or multiple agents. An important configuration for each team involves defining the order in which agents send messages and determining when the team should terminate.\n",
    "\n",
    "In the following section, we will begin by defining agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from autogen_agentchat import EVENT_LOGGER_NAME\n",
    "from autogen_agentchat.logging import ConsoleLogHandler\n",
    "from autogen_agentchat.agents import CodingAssistantAgent, ToolUseAssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat, MaxMessageTermination\n",
    "from autogen_core.components.models import OpenAIChatCompletionClient\n",
    "from autogen_core.components.tools import FunctionTool\n",
    "\n",
    "\n",
    "# Set up a log handler to print logs to the console.\n",
    "logger = logging.getLogger(EVENT_LOGGER_NAME)\n",
    "logger.addHandler(ConsoleLogHandler())\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n",
    ")\n",
    "\n",
    "writing_assistant_agent = CodingAssistantAgent(\n",
    "    name=\"writing_assistant_agent\",\n",
    "    system_message=\"You are a helpful assistant that solve tasks by generating text responses and code.\",\n",
    "    model_client=model_client,\n",
    ")\n",
    "\n",
    "\n",
    "async def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is 72 degrees and Sunny.\"\n",
    "\n",
    "\n",
    "get_weather_tool = FunctionTool(get_weather, description=\"Get the weather for a city\")\n",
    "\n",
    "tool_use_agent = ToolUseAssistantAgent(\n",
    "    \"tool_use_agent\",\n",
    "    system_message=\"You are a helpful assistant that solves tasks by only using your tools.\",\n",
    "    model_client=model_client,\n",
    "    registered_tools=[get_weather_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoundRobinGroupChat\n",
    "\n",
    "A team where agents take turns sending messages (in a round robin fashion) until a termination condition is met. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_robin_team = RoundRobinGroupChat([tool_use_agent, writing_assistant_agent])\n",
    "round_robin_team_result = await round_robin_team.run(\n",
    "    \"Write a Haiku about the weather in Paris\", termination_condition=MaxMessageTermination(max_messages=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can define a team where the agents solve a problem by _writing and executing code_ in a round-robin fashion. \n",
    "\n",
    "```python \n",
    "async with DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:\n",
    "    code_executor_agent = CodeExecutorAgent(\n",
    "        \"code_executor\", code_executor=code_executor)\n",
    "    code_execution_team = RoundRobinGroupChat([writing_assistant_agent, code_executor_agent])\n",
    "    code_execution_team_result = await code_execution_team.run(\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png\", termination_condition=MaxMessageTermination(max_messages=12))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelctorGroupChat\n",
    "\n",
    "A team where a generative model (LLM) is used to select the next agent to send a message based on the current conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_team = SelectorGroupChat([tool_use_agent, writing_assistant_agent], model_client=model_client)\n",
    "\n",
    "llm_team_result = await llm_team.run(\n",
    "    \"What is the weather in paris right now? Also, \", termination_condition=MaxMessageTermination(max_messages=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "In this section, we reviewed how to define model clients, agents, and teams in AgentChat. Here are some other concepts to explore further:\n",
    "\n",
    "- Termination Conditions: Define conditions that determine when a team should stop running. In this sample, we used a `MaxMessageTermination` condition to stop the team after a certain number of messages. Explore other termination conditions supported in the AgentChat package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agnext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
